{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_907x3hHT-_",
        "outputId": "9fcabf7e-c52c-48ce-c061-0a08e64d17d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.29)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (2.10.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.14 marshmallow-3.25.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aCWFu-O89WM",
        "outputId": "57bf7fb0-fa76-499e-ad01-56ca63e0be55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/298.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pypdf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e4LQAt3Ilhs",
        "outputId": "94cbf310-aafa-4dea-9532-9df086473294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for doc2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install doc2text -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W7mabgggInNs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLN1VsaB9ANW"
      },
      "outputs": [],
      "source": [
        "def load_document(file):\n",
        "  import os\n",
        "  name, extension = os.path.splitext(file)\n",
        "  print(f\"{name} - {extension}\")\n",
        "\n",
        "  if extension == '.pdf':\n",
        "    from langchain_community.document_loaders import PyPDFLoader\n",
        "    print(f'Loading {file}')\n",
        "    loader = Loader(file)\n",
        "  elif extension == '.docx':\n",
        "        from langchain.document_loaders import Docx2txtLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = Docx2txtLoader(file)\n",
        "  elif extension == '.txt':\n",
        "      from langchain.document_loaders import TextLoader\n",
        "      loader = TextLoader(file)\n",
        "  else:\n",
        "      print('Document format is not supported!')\n",
        "      return None\n",
        "\n",
        "  data = loader.load()\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2rP6OnpDGvon",
        "outputId": "3cfec6f1-2097-47f1-b9b5-9f1c20f3cc3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/KR-reasoning-LLM-framework - .pdf\n",
            "Loading /content/KR-reasoning-LLM-framework.pdf\n",
            "K-Level Reasoning: Establishing Higher Order Beliefs\n",
            "in Large Language Models for Strategic Reasoning\n",
            "Yadong Zhang1,2,*, Shaoguang Mao2,†, Tao Ge2, Xun Wang2,\n",
            "Yan Xia2, Man Lan1, Furu Wei2,\n",
            "1East China Normal University, 2Microsoft Research Asia\n",
            "Abstract\n",
            "Strategic reasoning is a complex yet essential\n",
            "capability for intelligent agents. It requires\n",
            "Large Language Model (LLM) agents to adapt\n",
            "their strategies dynamically in multi-agent en-\n",
            "vironments. Unlike static reasoning tasks, suc-\n",
            "cess in these contexts depends on anticipating\n",
            "other agents’ beliefs and actions while contin-\n",
            "uously adjusting strategies to achieve individ-\n",
            "ual goals. LLMs and LLM agents often strug-\n",
            "gle with strategic reasoning due to the absence\n",
            "of a reasoning framework that enables them\n",
            "to dynamically infer others’ perspectives and\n",
            "adapt to changing environments. Inspired by\n",
            "the Level-K framework1 from game theory and\n",
            "behavioral economics, which extends reason-\n",
            "ing from simple reactions to structured strate-\n",
            "gic depth, we propose a novel framework: \"K-\n",
            "Level Reasoning with Large Language Mod-\n",
            "els (K-R).\" This framework employs recursive\n",
            "mechanisms to enable LLMs to achieve vary-\n",
            "ing levels of strategic depth, allowing agents to\n",
            "form higher order beliefs—beliefs about others’\n",
            "beliefs. We validate this framework through\n",
            "rigorous testing on four testbeds: two classi-\n",
            "cal game theory problems and two social in-\n",
            "telligence tasks. The results demonstrate the\n",
            "advantages of K-R in strategic reasoning. Our\n",
            "work presents the first recursive implementa-\n",
            "tion of strategic depth in large language models\n",
            "(LLMs). It establishes a foundation for future\n",
            "research into theory of mind and strategic rea-\n",
            "soning in LLMs.\n",
            "1 Introduction\n",
            "Strategic reasoning—decision-making in multi-\n",
            "participant environments—presents unique chal-\n",
            "lenges for Large Language Models (LLMs) and\n",
            "LLM agents(Zhang et al., 2024b). In these settings,\n",
            "*Work was done when interning at Microsoft Research\n",
            "Asia. † Correspondence to: shaoguang.mao@microsoft.com\n",
            "1According to the Level-k Framework, k-level thinking\n",
            "involves considering what opponent/partner are likely to do,\n",
            "what they think you will do, and what they believe you think\n",
            "they will do, and so on.\n",
            "First-level Thinking Second-level Thinking\n",
            "…\n",
            "Third-level Thinking\n",
            "……\n",
            "Figure 1: Level-K Framework: In first-level thinking,\n",
            "agents respond directly to the environment. In second-\n",
            "level thinking, agents consider the first-level thinking of\n",
            "others. This process continues iteratively, with agents\n",
            "forming higher order beliefs based on assumptions about\n",
            "others’ thoughts.\n",
            "agents must respond to the actions of others while\n",
            "adapting to dynamic environments. They also need\n",
            "to align their decisions with their own goals during\n",
            "these interactions. Strategic reasoning is essential\n",
            "for intelligent agents and is widely applied in real-\n",
            "world tasks, such as investment, business strategy\n",
            "making(Zhao et al., 2023), negotiation(Hua et al.,\n",
            "2023), and policy-making(Li et al., 2024).\n",
            "Effective strategic reasoning relies on under-\n",
            "standing others’ perspectives and anticipating their\n",
            "strategies. While there are some research efforts on\n",
            "LLMs’ strategic reasoning, most methods rely on\n",
            "static prompting (Fu et al., 2023; Xu et al., 2023b).\n",
            "This typically involves instructing the model to ac-\n",
            "count for others’ beliefs and decisions during its\n",
            "own decision-making process in the prompt. How-\n",
            "ever, these approaches fall short in enabling LLMs\n",
            "to form true higher order beliefs—beliefs about\n",
            "what others believe, and lack the flexibility needed\n",
            "for deeper strategic reasoning.\n",
            "K-level thinking (Figure 1) (Nagel, 1995; Cui\n",
            "et al., 2021), a classical concept in behavioral eco-\n",
            "nomics and game theory, categorizes reasoning into\n",
            "varying depths of strategic thought. It involves not\n",
            "only predicting others’ actions but also considering\n",
            "1\n",
            "arXiv:2402.01521v2  [cs.CL]  17 Oct 2024\n"
          ]
        }
      ],
      "source": [
        "data = load_document('/content/KR-reasoning-LLM-framework.pdf')\n",
        "print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aBypFMIeKoCX",
        "outputId": "83e9600e-20d8-4b8e-d6d7-54e731ca274c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U arxiv pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-OeKfwJHOYi"
      },
      "outputs": [],
      "source": [
        "def load_from_arxiv(query):\n",
        "  from langchain_community.document_loaders import ArxivLoader\n",
        "\n",
        "  loader = ArxivLoader(query=query, load_max_doc=2)\n",
        "  # docs = loader.load()\n",
        "  docs = loader.get_summaries_as_docs()\n",
        "  # print(docs[0].page_content[:100])\n",
        "  print(docs[0].page_content)\n",
        "  print(docs[0].metadata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yo_ILlH9ZlKL",
        "outputId": "94bab515-ae8d-4707-c838-2c65e84cd7fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Large language models (LLMs) have demonstrated impressive reasoning\n",
            "abilities, but they still struggle with faithful reasoning due to knowledge\n",
            "gaps and hallucinations. To address these issues, knowledge graphs (KGs) have\n",
            "been utilized to enhance LLM reasoning through their structured knowledge.\n",
            "However, existing KG-enhanced methods, either retrieval-based or agent-based,\n",
            "encounter difficulties in accurately retrieving knowledge and efficiently\n",
            "traversing KGs at scale. In this work, we introduce graph-constrained reasoning\n",
            "(GCR), a novel framework that bridges structured knowledge in KGs with\n",
            "unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures\n",
            "faithful KG-grounded reasoning by integrating KG structure into the LLM\n",
            "decoding process through KG-Trie, a trie-based index that encodes KG reasoning\n",
            "paths. KG-Trie constrains the decoding process, allowing LLMs to directly\n",
            "reason on graphs and generate faithful reasoning paths grounded in KGs.\n",
            "Additionally, GCR leverages a lightweight KG-specialized LLM for\n",
            "graph-constrained reasoning alongside a powerful general LLM for inductive\n",
            "reasoning over multiple reasoning paths, resulting in accurate reasoning with\n",
            "zero reasoning hallucination. Extensive experiments on several KGQA benchmarks\n",
            "demonstrate that GCR achieves state-of-the-art performance and exhibits strong\n",
            "zero-shot generalizability to unseen KGs without additional training.\n",
            "{'Entry ID': 'http://arxiv.org/abs/2410.13080v1', 'Published': datetime.date(2024, 10, 16), 'Title': 'Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models', 'Authors': 'Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan'}\n"
          ]
        }
      ],
      "source": [
        "load_from_arxiv('reasoning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eiZ2WXmZddBS",
        "outputId": "705fbaa7-37f6-4ff2-aae7-aec9c381294d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgjgcfcxZqSr"
      },
      "outputs": [],
      "source": [
        "# wikipedia\n",
        "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
        "    from langchain.document_loaders import WikipediaLoader\n",
        "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
        "    data = loader.load()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ec_E4-9Xc_Qd",
        "outputId": "7e045e92-32ef-4946-8549-6a20bee32eb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'title': 'Reason', 'summary': \"Reason is the capacity of consciously applying logic by drawing valid conclusions from new or existing information, with the aim of seeking the truth. It is associated with such characteristically human activities as philosophy, religion, science, language, mathematics, and art, and is normally considered to be a distinguishing ability possessed by humans. Reason is sometimes referred to as rationality.\\nReasoning involves using more-or-less rational processes of thinking and cognition to extrapolate from one's existing knowledge to generate new knowledge, and involves the use of one's intellect. The field of logic studies the ways in which humans can use formal reasoning to produce logically valid arguments and true conclusions. Reasoning may be subdivided into forms of logical reasoning, such as deductive reasoning, inductive reasoning, and abductive reasoning.\\nAristotle drew a distinction between logical discursive reasoning (reason proper), and intuitive reasoning,:\\u200aVI.7\\u200a in which the reasoning process through intuition—however valid—may tend toward the personal and the subjectively opaque. In some social and political settings logical and intuitive modes of reasoning may clash, while in other contexts intuition and formal reason are seen as complementary rather than adversarial. For example, in mathematics, intuition is often necessary for the creative processes involved with arriving at a formal proof, arguably the most difficult of formal reasoning tasks.\\nReasoning, like habit or intuition, is one of the ways by which thinking moves from one idea to a related idea. For example, reasoning is the means by which rational individuals understand the significance of sensory information from their environments, or conceptualize abstract dichotomies such as cause and effect, truth and falsehood, or good and evil. Reasoning, as a part of executive decision making, is also closely identified with the ability to self-consciously change, in terms of goals, beliefs, attitudes, traditions, and institutions, and therefore with the capacity for freedom and self-determination.\\nPsychologists and cognitive scientists have attempted to study and explain how people reason, e.g. which cognitive and neural processes are engaged, and how cultural factors affect the inferences that people draw. The field of automated reasoning studies how reasoning may or may not be modeled computationally. Animal psychology considers the question of whether animals other than humans can reason.\", 'source': 'https://en.wikipedia.org/wiki/Reason'}, page_content='Reason is the capacity of consciously applying logic by drawing valid conclusions from new or existing information, with the aim of seeking the truth. It is associated with such characteristically human activities as philosophy, religion, science, language, mathematics, and art, and is normally considered to be a distinguishing ability possessed by humans. Reason is sometimes referred to as rationality.\\nReasoning involves using more-or-less rational processes of thinking and cognition to extrapolate from one\\'s existing knowledge to generate new knowledge, and involves the use of one\\'s intellect. The field of logic studies the ways in which humans can use formal reasoning to produce logically valid arguments and true conclusions. Reasoning may be subdivided into forms of logical reasoning, such as deductive reasoning, inductive reasoning, and abductive reasoning.\\nAristotle drew a distinction between logical discursive reasoning (reason proper), and intuitive reasoning,:\\u200aVI.7\\u200a in which the reasoning process through intuition—however valid—may tend toward the personal and the subjectively opaque. In some social and political settings logical and intuitive modes of reasoning may clash, while in other contexts intuition and formal reason are seen as complementary rather than adversarial. For example, in mathematics, intuition is often necessary for the creative processes involved with arriving at a formal proof, arguably the most difficult of formal reasoning tasks.\\nReasoning, like habit or intuition, is one of the ways by which thinking moves from one idea to a related idea. For example, reasoning is the means by which rational individuals understand the significance of sensory information from their environments, or conceptualize abstract dichotomies such as cause and effect, truth and falsehood, or good and evil. Reasoning, as a part of executive decision making, is also closely identified with the ability to self-consciously change, in terms of goals, beliefs, attitudes, traditions, and institutions, and therefore with the capacity for freedom and self-determination.\\nPsychologists and cognitive scientists have attempted to study and explain how people reason, e.g. which cognitive and neural processes are engaged, and how cultural factors affect the inferences that people draw. The field of automated reasoning studies how reasoning may or may not be modeled computationally. Animal psychology considers the question of whether animals other than humans can reason.\\n\\n\\n== Etymology and related words ==\\nIn the English language and other modern European languages, \"reason\", and related words, represent words which have always been used to translate Latin and classical Greek terms in their philosophical sense.\\n\\nThe original Greek term was \"λόγος\" logos, the root of the modern English word \"logic\" but also a word that could mean for example \"speech\" or \"explanation\" or an \"account\" (of money handled).\\nAs a philosophical term logos was translated in its non-linguistic senses in Latin as ratio. This was originally not just a translation used for philosophy, but was also commonly a translation for logos in the sense of an account of money.\\nFrench raison is derived directly from Latin, and this is the direct source of the English word \"reason\".\\nThe earliest major philosophers to publish in English, such as Francis Bacon, Thomas Hobbes, and John Locke also routinely wrote in Latin and French, and compared their terms to Greek, treating the words \"logos\", \"ratio\", \"raison\" and \"reason\" as interchangeable. The meaning of the word \"reason\" in senses such as \"human reason\" also overlaps to a large extent with \"rationality\" and the adjective of \"reason\" in philosophical contexts is normally \"rational\", rather than \"reasoned\" or \"reasonable\". Some philosophers, Hobbes for example, also used the word ratiocination as a synonym for \"reasoning\".\\nIn contrast to the use of \"reason\" as an abstract noun, a reason is a consideration that either explains or'),\n",
              " Document(metadata={'title': 'Logical reasoning', 'summary': 'Logical reasoning is a mental activity that aims to arrive at a conclusion in a rigorous way. It happens in the form of inferences or arguments by starting from a set of premises and reasoning to a conclusion supported by these premises. The premises and the conclusion are propositions, i.e. true or false claims about what is the case. Together, they form an argument. Logical reasoning is norm-governed in the sense that it aims to formulate correct arguments that any rational person would find convincing. The main discipline studying logical reasoning is logic. \\nDistinct types of logical reasoning differ from each other concerning the norms they employ and the certainty of the conclusion they arrive at. Deductive reasoning offers the strongest support: the premises ensure the conclusion, meaning that it is impossible for the conclusion to be false if all the premises are true. Such an argument is called a valid argument, for example: all men are mortal; Socrates is a man; therefore, Socrates is mortal. For valid arguments, it is not important whether the premises are actually true but only that, if they were true, the conclusion could not be false. Valid arguments follow a rule of inference, such as modus ponens or modus tollens. Deductive reasoning plays a central role in formal logic and mathematics.\\nFor non-deductive logical reasoning, the premises make their conclusion rationally convincing without ensuring its truth. This is often understood in terms of probability: the premises make it more likely that the conclusion is true and strong inferences make it very likely. Some uncertainty remains because the conclusion introduces new information not already found in the premises. Non-deductive reasoning plays a central role in everyday life and in most sciences. Often-discussed types are inductive, abductive, and analogical reasoning. Inductive reasoning is a form of generalization that infers a universal law from a pattern found in many individual cases. It can be used to conclude that \"all ravens are black\" based on many individual observations of black ravens. Abductive reasoning, also known as \"inference to the best explanation\", starts from an observation and reasons to the fact explaining this observation. An example is a doctor who examines the symptoms of their patient to make a diagnosis of the underlying cause. Analogical reasoning compares two similar systems. It observes that one of them has a feature and concludes that the other one also has this feature.\\nArguments that fall short of the standards of logical reasoning are called fallacies. For formal fallacies, like affirming the consequent, the error lies in the logical form of the argument. For informal fallacies, like false dilemmas, the source of the faulty reasoning is usually found in the content or the context of the argument. Some theorists understand logical reasoning in a wide sense that is roughly equivalent to critical thinking. In this regard, it encompasses cognitive skills besides the ability to draw conclusions from premises. Examples are skills to generate and evaluate reasons and to assess the reliability of information. Further factors are to seek new information, to avoid inconsistencies, and to consider the advantages and disadvantages of different courses of action before making a decision.', 'source': 'https://en.wikipedia.org/wiki/Logical_reasoning'}, page_content='Logical reasoning is a mental activity that aims to arrive at a conclusion in a rigorous way. It happens in the form of inferences or arguments by starting from a set of premises and reasoning to a conclusion supported by these premises. The premises and the conclusion are propositions, i.e. true or false claims about what is the case. Together, they form an argument. Logical reasoning is norm-governed in the sense that it aims to formulate correct arguments that any rational person would find convincing. The main discipline studying logical reasoning is logic. \\nDistinct types of logical reasoning differ from each other concerning the norms they employ and the certainty of the conclusion they arrive at. Deductive reasoning offers the strongest support: the premises ensure the conclusion, meaning that it is impossible for the conclusion to be false if all the premises are true. Such an argument is called a valid argument, for example: all men are mortal; Socrates is a man; therefore, Socrates is mortal. For valid arguments, it is not important whether the premises are actually true but only that, if they were true, the conclusion could not be false. Valid arguments follow a rule of inference, such as modus ponens or modus tollens. Deductive reasoning plays a central role in formal logic and mathematics.\\nFor non-deductive logical reasoning, the premises make their conclusion rationally convincing without ensuring its truth. This is often understood in terms of probability: the premises make it more likely that the conclusion is true and strong inferences make it very likely. Some uncertainty remains because the conclusion introduces new information not already found in the premises. Non-deductive reasoning plays a central role in everyday life and in most sciences. Often-discussed types are inductive, abductive, and analogical reasoning. Inductive reasoning is a form of generalization that infers a universal law from a pattern found in many individual cases. It can be used to conclude that \"all ravens are black\" based on many individual observations of black ravens. Abductive reasoning, also known as \"inference to the best explanation\", starts from an observation and reasons to the fact explaining this observation. An example is a doctor who examines the symptoms of their patient to make a diagnosis of the underlying cause. Analogical reasoning compares two similar systems. It observes that one of them has a feature and concludes that the other one also has this feature.\\nArguments that fall short of the standards of logical reasoning are called fallacies. For formal fallacies, like affirming the consequent, the error lies in the logical form of the argument. For informal fallacies, like false dilemmas, the source of the faulty reasoning is usually found in the content or the context of the argument. Some theorists understand logical reasoning in a wide sense that is roughly equivalent to critical thinking. In this regard, it encompasses cognitive skills besides the ability to draw conclusions from premises. Examples are skills to generate and evaluate reasons and to assess the reliability of information. Further factors are to seek new information, to avoid inconsistencies, and to consider the advantages and disadvantages of different courses of action before making a decision.\\n\\n\\n== Definition ==\\nLogical reasoning is a form of thinking that is concerned with arriving at a conclusion in a rigorous way. This happens in the form of inferences by transforming the information present in a set of premises to reach a conclusion. It can be defined as \"selecting and interpreting information from a given context, making connections, and verifying and drawing conclusions based on provided and interpreted information and the associated rules and processes.\" Logical reasoning is rigorous in the sense that it does not generate any conclusion but ensures that the premises support the conclusion and act as reasons for believing it. One central ')]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_from_wikipedia('reasoning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8niAbBfdJuG"
      },
      "outputs": [],
      "source": [
        "def chunk_data(data, chunk_size=256):\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "  # Define separators for splitting (optional)\n",
        "  separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      separators=separators,\n",
        "      chunk_size=chunk_size,\n",
        "      chunk_overlap=0\n",
        "  )\n",
        "  chunks = text_splitter.split_documents(data)\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aCEQUyCvu-Wc",
        "outputId": "b7b8911a-7984-4fda-9d2c-66124cbb23bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/KR-reasoning-LLM-framework - .pdf\n",
            "Loading /content/KR-reasoning-LLM-framework.pdf\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='K-Level Reasoning: Establishing Higher Order Beliefs\\nin Large Language Models for Strategic Reasoning\\nYadong Zhang1,2,*, Shaoguang Mao2,†, Tao Ge2, Xun Wang2,\\nYan Xia2, Man Lan1, Furu Wei2,\\n1East China Normal University, 2Microsoft Research Asia\\nAbstract'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='Strategic reasoning is a complex yet essential\\ncapability for intelligent agents. It requires\\nLarge Language Model (LLM) agents to adapt\\ntheir strategies dynamically in multi-agent en-\\nvironments. Unlike static reasoning tasks, suc-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='cess in these contexts depends on anticipating\\nother agents’ beliefs and actions while contin-\\nuously adjusting strategies to achieve individ-\\nual goals. LLMs and LLM agents often strug-\\ngle with strategic reasoning due to the absence'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='of a reasoning framework that enables them\\nto dynamically infer others’ perspectives and\\nadapt to changing environments. Inspired by\\nthe Level-K framework1 from game theory and\\nbehavioral economics, which extends reason-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='ing from simple reactions to structured strate-\\ngic depth, we propose a novel framework: \"K-\\nLevel Reasoning with Large Language Mod-\\nels (K-R).\" This framework employs recursive\\nmechanisms to enable LLMs to achieve vary-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='ing levels of strategic depth, allowing agents to\\nform higher order beliefs—beliefs about others’\\nbeliefs. We validate this framework through\\nrigorous testing on four testbeds: two classi-\\ncal game theory problems and two social in-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='telligence tasks. The results demonstrate the\\nadvantages of K-R in strategic reasoning. Our\\nwork presents the first recursive implementa-\\ntion of strategic depth in large language models\\n(LLMs). It establishes a foundation for future'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='research into theory of mind and strategic rea-\\nsoning in LLMs.\\n1 Introduction\\nStrategic reasoning—decision-making in multi-\\nparticipant environments—presents unique chal-\\nlenges for Large Language Models (LLMs) and'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='LLM agents(Zhang et al., 2024b). In these settings,\\n*Work was done when interning at Microsoft Research\\nAsia. † Correspondence to: shaoguang.mao@microsoft.com\\n1According to the Level-k Framework, k-level thinking'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='involves considering what opponent/partner are likely to do,\\nwhat they think you will do, and what they believe you think\\nthey will do, and so on.\\nFirst-level Thinking Second-level Thinking\\n…\\nThird-level Thinking\\n……'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='Figure 1: Level-K Framework: In first-level thinking,\\nagents respond directly to the environment. In second-\\nlevel thinking, agents consider the first-level thinking of\\nothers. This process continues iteratively, with agents'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='forming higher order beliefs based on assumptions about\\nothers’ thoughts.\\nagents must respond to the actions of others while\\nadapting to dynamic environments. They also need\\nto align their decisions with their own goals during'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='these interactions. Strategic reasoning is essential\\nfor intelligent agents and is widely applied in real-\\nworld tasks, such as investment, business strategy\\nmaking(Zhao et al., 2023), negotiation(Hua et al.,\\n2023), and policy-making(Li et al., 2024).'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='Effective strategic reasoning relies on under-\\nstanding others’ perspectives and anticipating their\\nstrategies. While there are some research efforts on\\nLLMs’ strategic reasoning, most methods rely on\\nstatic prompting (Fu et al., 2023; Xu et al., 2023b).'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='This typically involves instructing the model to ac-\\ncount for others’ beliefs and decisions during its\\nown decision-making process in the prompt. How-\\never, these approaches fall short in enabling LLMs\\nto form true higher order beliefs—beliefs about'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='what others believe, and lack the flexibility needed\\nfor deeper strategic reasoning.\\nK-level thinking (Figure 1) (Nagel, 1995; Cui\\net al., 2021), a classical concept in behavioral eco-\\nnomics and game theory, categorizes reasoning into'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 0}, page_content='varying depths of strategic thought. It involves not\\nonly predicting others’ actions but also considering\\n1\\narXiv:2402.01521v2  [cs.CL]  17 Oct 2024'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content=\"Please choose an integer between 1 and 100. The player \\nwhose chosen number is closest to 0.8 * the average of all \\nchosen numbers wins the round. Let's start the 1st round.\\nI think in the\\n1st  round …\\n …\\n…\\n…\\n…\\n…\\n…\\n…\\n…\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content=\"(40 + 50 + 34 + 40 + 36) / 5 = 40  (Average)\\n40 * 0.8 = 32 (Target) \\nThe daily available water can meet the needs of only one \\nresident. An auction is conducted, and the highest bidder \\nwill obtain today's water supply.\\n$55\\n$100\\n8 1\\n$60\\n$ 100\\n$50\\n$ 100\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='$70\\n$ 100\\n$30\\n$ 100\\nIn today’s bidding,        placed the highest bid of $70, \\nsecuring the water resources for the day.        +2     and \\nresets the number of days thirsty =0 . Other residents \\nhave their HP deducted based on the number of days they'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='have been thirsty （ = - ）. \\nA new day! All residents receive a daily salary of  $100. \\nToday, the daily available water can meet the needs of only \\none resident ...\\nToday I will bid \\n$40 because …\\n8 1\\n 8 1\\n 8 1\\n 8 1\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n$90\\n$200\\n7 2\\n$80\\n$ 200'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content=\"$100\\n$ 200\\n$50\\n$ 130\\n$80\\n$ 200\\nIn yesterday, \\n  ………\\n7 2\\n 7 2\\n 10 1\\n 7 2\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\nGuessing 0.8 of the Average Survival Auction Game\\n40 50 34 40 36\\nLet's start the 2nd round.\\nIn last \\nround …\\n …\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n31 36 24 25 21\\nClosest to the target.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='40-32=8 50-32=18 34-32=2 40-32=8 36-32=4\\nPrivate Utilities\\nPrivate Utilities\\n5\\n 0 8\\nWe have 5         , 5       , and 5        . \\nPlease negotiate how to distribute \\nthem.\\n7\\n 2 1\\nI want more        , as a result, I can \\naccept less       , so I propose …'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='I propose …\\nAgreement achieved. \\n      has 4      , 4        , and 0        , \\nwhile     has 1       , 1       , and 5      . \\nThe total utilities:  \\n     :36,      : 45.          wins!\\nNegotiation'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='Figure 2: The illustration of three reasoning problems in dynamic, interactive environments in this paper. Left:\\nGuessing 0.8 of the Average; Middle: Survival Auction Game; Right: Negotiation.\\ntheir beliefs about one’s actions, and even further'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='layers of recursive thinking.\\nInspired by K-level thinking, we propose a novel\\nstrategic reasoning framework termed \"K-Level\\nReasoning with LLMs (K-R).\" K-R organizes rea-\\nsoning into hierarchical levels and employs a recur-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='sive mechanism to integrate varying strategic depth\\ninto decision-making. Specifically, it involves: 1)\\nrecursively anticipating others’ actions at varying\\nlevels of strategic depth with environmental context'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='and historical public information, and 2) reason-\\ning the optimal action based on these anticipations.\\nTo the best of our knowledge, this is the first ap-\\nproach to implementing varying levels of strategic\\ndepth in LLMs using a recursive mechanism and'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='enables deeper reasoning in LLM agents through\\nan algorithmic framework.\\nWe validate this framework through rigorous\\ntesting on four testbeds: two classical game theory\\nproblems and two social intelligence tasks. The'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='game theory problems includes Guessing 0.8 of\\nthe Average (Figure 2 left) and Survival Auction\\nGame (Mao et al., 2023) (Figure 2 middle). The\\nsocial intelligence tasks includes Negotiation (Cao\\net al., 2018) (Figure 2 right) and SOTOPIA bench-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='mark(Zhou et al., 2024). These settings serve as\\nmicrocosms of the complex decision-making pro-\\ncesses involved in strategic reasoning. Through\\nextensive experiments, we demonstrate that our\\nframework significantly outperforms existing rea-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='soning methods and flexibly achieves varying lev-\\nels of strategic depth. In addition to empirical evi-\\ndence, we provide a theoretical analysis highlight-\\ning the benefits of K-R. We show that, leveraging\\nthe in-context learning capabilities of LLMs, K-R'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='can effectively model opponents’ behavior using\\naccumulated public and available opponent infor-\\nmation.\\nFurthermore, we align the strategic depth of\\nLLMs with human participants (Nagel, 1995;\\nBosch-Domenech et al., 2002). Using human as'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='anchors, we observe that the K-R significantly en-\\nhances the strategic depth of LLMs from 0.25 to\\n1.89. Notably, when K=3, the strategic depth (1.89)\\nof the LLM closely approaches that of financial\\nnewspaper readers (1.91). This strongly indicates'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='that K-R establishes higher order beliefs in LLMs\\nfor strategic reasoning.\\nThe contributions of this work are as follows:\\n• We introduce K-R, a novel framework that\\nextends k-level thinking to LLMs, enabling\\nflexible strategic reasoning at varying depths'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='through a recursive mechanism.\\n• We conduct extensive evaluations, including\\ngame theory and social intelligence problems,\\ndemonstrating that K-R significantly outper-\\nforms existing methods in terms of flexibility'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='and effectiveness, across both closed-source\\nand open-source models.\\n• We provide an in-depth analysis of K-R, con-\\nfirming its ability to build higher order beliefs\\nand enhance strategic reasoning. This lays\\na foundation for future research in theory of'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 1}, page_content='mind and strategic reasoning in LLMs.\\n2 K-Level Reasoning with Large\\nLanguage Models\\n2.1 Methodology\\nStrategic reasoning requires considering both the\\ndecision context and the possible actions of other\\n2'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='participants.We employ a multi-round normal form\\nmulti-participant game to introduce the proposed\\nmethod. In this setting, an agent’s decision-making\\nprocess is formalized as follows: each agent i se-\\nlects an action at\\ni from a set At\\ni at timestep t. The'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='payoff for agent i, resulting from the collective ac-\\ntion profile At = (at\\n1, at\\n2, ..., at\\nN ) and environment\\nEt, is denoted as Ui(Et, At).\\nAt k = 1, agents decide based on environment\\nEt without strategic anticipation:\\nat,1\\ni = arg max\\nai∈At\\ni'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='E[Ui(Et, ai)] (1)\\nAt higher level thinking (k ≥ 2), agent i simulates\\nother agents operating at level k − 1 and adjusts\\ntheir strategy accordingly2:\\nat,k\\ni = arg max\\nai∈At\\ni\\nE[Ui(Et, ai, ˆat,k−1\\n−i )] (2)\\nwhere ˆat,k−1'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='−i are the predicted actions of other\\nagents based on their k − 1 level reasoning.\\nWe propose a novel strategic reasoning frame-\\nwork with recursive mechanisms , termed “K-\\nLevel Reasoning with Large Language Models'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='(K-R),” involving 1) recursively anticipating the\\nactions ˆat,k\\n−i of others at different thinking levels\\nusing environment contexts and historical public\\ninformation, followed by 2) reasoning the optimal\\naction at,k'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='i based on anticipation of others’ actions.\\nThe K-Level Reasoning process is formulated as\\nfollows:\\n1) Anticipation:\\nˆat,m\\nj =\\n(\\nLLM(Et, Ht\\nj) if m = 1\\nLLM(Et, Ht\\nj, ˆat,m−1\\n−j ) if m >1 (3)\\nwhere Ht\\nj = {(E1, a1\\nj ), (E2, a2\\nj ), ...,(Et−1, at−1\\nj )}'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='represents public historical data of agent j, and m\\ndenotes the specified thinking level.\\n2) Reasoning:\\nat,k\\ni = LLM(Et, Ht\\ni , ˆat,k−1\\n−i ) (4)\\nAlgorithm 1 outlines the implementation of K-R.\\nThis recursive method enables flexible and progres-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='sively deeper strategic reasoning ( 1, 2, ..., k, k+\\n1, ...), thereby enhancing higher order belief in\\nLLM agents.\\n2 To simplify the formulation, we assume that all oppo-\\nnents are in the same thinking level. In practice, varying'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='thinking level can be implemented.\\nAlgorithm 1 K-Level Reasoning with LLMs\\nRequire: Et: Current decision context at time t;\\nHt\\ni : Historical information up to time t for\\nagent i;\\nK: Depth of strategic reasoning;\\nEnsure: at,K'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='i : Action for agent i at time t after\\nK-level reasoning.\\n1: Function K_REASONING(i, k):\\n2: if k == 1then\\n3: return LLM(Et, Ht\\ni )\\n4: else\\n5: for each agent j ̸= i do\\n6: ˆat,k−1\\nj = K_REASONING(j, k− 1)\\n7: end for\\n8: return LLM(Et, Ht\\ni , {ˆat,k−1'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='j | j ̸= i})\\n9: end if\\n10: at,K\\ni = K_REASONING(i, K)\\n11: return at,K\\ni\\n2.2 Theoretical Analysis\\nThis section discusses the benefits from K-R from\\na theoretical perspective. We utilize the in-context\\nlearning capabilities of LLMs to effectively model'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='opponents’ behavior. Suppose agent j’s decision-\\nmaking process follows a hidden strategy θ∗\\nj . Thus,\\nagent j’s decision-making can be expressed as:\\nP(at\\nj | Et, θ∗\\nj ) (5)\\nThe in-context learning of LLMs can be formally'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='defined as implicit Bayesian inference(Xie et al.,\\n2021); therefore, given the environment Et, the\\nnext action prediction conditioned on Ht\\nj is:\\nP(at\\nj | Et, Ht\\nj) =\\nZ\\nP(at\\nj | Et, θj)P(θj | Ht\\nj)dθj\\n(6)\\nAs t → ∞, by the law of large numbers and proper-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='ties of Bayesian updating, the posterior distribution\\nconcentrates around the true parameter θ∗\\nj :\\nP(θj | Ht\\nj) → δ(θj − θ∗\\nj ) (7)\\nwhere δ is the Dirac delta function. Therefore,\\nZ\\nP(at\\nj | Et, θj)P(θj | Ht\\nj)dθj → P(at\\nj | Et, θ∗\\nj )\\n(8)'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 2}, page_content='This implies that as the number of interactions\\nincreases, K-R can more accurately predict oppo-\\nnents’ behavior.\\nIt is also worth noting that interaction data can-\\nnot be infinite, and in-context learning is related to\\n3'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='the performance of large language models (LLMs).\\nTherefore, we empircally validate these hypotheses\\nand reasoning in Section 5.2.\\n3 Experiments: Game Theory\\nTo fairly compare the strategic reasoning capabili-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='ties of LLMs, we first adopt two widely used game\\ntheory settings. These controlled, well-defined\\ngame theory problems provide a robust assessment\\nof LLMs’ performance, with detailed setups out-\\nlined in Appendix B.\\n3.1 Task Definition and Metrics'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='3.1.1 Guessing 0.8 of the Average (G0.8A)\\nG0.8A (Figure 2 Left) is a classic game theory\\nproblem introduced by Alain Ledoux (Ledoux,\\n1981). It involves 10-round games where each\\nplayer selects a number between 1 and 100. The'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='objective is to choose a number closest to 80% of\\nthe group’s average choice. The key idea is to guess\\nhow others will estimate the average and decide the\\nnumber to submit. This concept is also illustrated\\nin the Keynesian Beauty Contest (Keynes, 1936).'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='This game mirrors the challenge of anticipating\\ncollective behavior in financial markets, where in-\\nvestors must predict not only the value of an asset\\nbut also how others will value it in the future.\\nThe performance of the agent is evaluated using'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='the Win Rate. Specifically, the Win Rate is calcu-\\nlated based on the wins achieved by the agent in in-\\ndividual round, rather than an entire game episode.\\n3.1.2 Survival Auction Game (SAG)\\nSAG (Figure 2 Middle) is derived from the Wa-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='ter Allocation Challenge proposed in (Mao et al.,\\n2023). Each resident’s goal is to survive a 10-day\\ndrought period by bidding for water resources and\\nmaintaining health points above zero. If a player\\nsuccessfully bids for water, they gain health points;'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='otherwise, they lose health points. This integration\\nof the auction system with the health points mecha-\\nnism creates a dynamic environment where players\\nmust balance health and finances.\\nWe use Average Survival Roundmeasures the'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='mean round in which a player remains active in the\\ngame.\\n3.2 Base Techniques\\nWe adapt a variety of approaches, originally from\\ntraditional reasoning and agent benchmarks. These\\nbase techniques include:\\nStandard Prompting (Direct): This is the con-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='ventional prompting method in which the LLM\\ngenerates the final answer (Action) in response to\\nthe given game setting prompt.\\nChain-of-Thought (CoT) (Wei et al., 2022): We\\nemploy the zero-shot Chain-of-Thought reasoning\\nmethod (Kojima et al., 2022).'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='Persona Prompting (Persona) (Deshpande\\net al., 2023): This technique modifies the standard\\nprompting process by incorporating a “Game Ex-\\npert” persona to enhance the reasoning capabilities\\nof the LLM.\\nReflexion (Reflect) (Shinn et al., 2023): This'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='method refers to language agents with verbal rein-\\nforcement learning and has been adapted for dy-\\nnamic tasks. Detailed modifications are explained\\nin K.\\nSelf-Refine (Refine) (Madaan et al., 2023): This\\nis a multi-round iterative reasoning approach where'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='an additional LLM offers comments and adjust-\\nments prior to reaching a final decision. The dis-\\ntinctions between Self-Refine and Reflect are elab-\\norated upon in the Appendix I.\\nPrediction Chain of Thought (PCoT) : This'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='strong baseline diverges from CoT by requiring the\\nLLM to explicitly predict opponents’ actions be-\\nfore making decisions. Unlike K-Level Reasoning,\\nwhich involves a recursive approach, PCoT focuses\\non direct prediction based on context.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='For implementation details and specific exam-\\nples, please refer to Appendix K.\\n3.3 Experimental Settings\\nWe established a controllable environment and dis-\\ntinguished between two roles: the player (primary\\nfocus) and the opponents. The player is equipped'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='with a specific method, while all opponents use\\nanother reasoning approach. This well-defined set-\\nting allows for a clearer comparison of reasoning\\ncapabilities between methods.\\nIn G0.8A and SAG, there is one player and four'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='opponents for each game. Experiments for each\\nsetting are repeated 10 times and have passed the\\nsignificance test (Appendix H), and each experi-\\nment consists of a 10-round game.\\nAll methods in main experiments were imple-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 3}, page_content='mented using GPT-4 (Achiam et al., 2023) (gpt4-\\n32k), with the temperature set at 0.7 and the top-p\\nset at 0.9. We also conducted experiments with\\nopen-source LLMs, Details of which are provided\\nin Appendix E. Unless specified otherwise, the\\n4'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='level of thinking in K-Level Reasoning is set to\\nK=2.\\n3.4 Results\\nTo distinguish between “Player” and “Opponent”\\nin the table, the headers for Player (bold) and Op-\\nponents (italics) are formatted accordingly.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='Table 1: Win Rate of the player against different oppo-\\nnents in G0.8A game.\\nDirect CoT Persona Reflect Refine PCoT K-R\\nDirect 0.43 0.67 0.62 0.53 0.43 0.61 0.82\\nCoT 0.07 0.32 0.35 0.14 0.22 0.45 0.63\\nPersona0.05 0.37 0.29 0.05 0.37 0.11 0.46'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='Reflect 0.42 0.68 0.63 0.39 0.64 0.74 0.78\\nRefine 0.10 0.34 0.32 0.31 0.23 0.22 0.46\\nPCoT 0.03 0.44 0.52 0.21 0.51 0.54 0.85\\nK-R 0.04 0.15 0.14 0.04 0.17 0.14 0.52\\nAverage0.16\\n± 0.18\\n0.32\\n± 0.19\\n0.41\\n± 0.18\\n0.24\\n± 0.18\\n0.37\\n± 0.17\\n0.40\\n± 0.25\\n0.65\\n± 0.17'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='Table 2: Average Survival Round of the player against\\ndifferent opponents in Survival Auction Game.\\nDirect CoT Persona Reflect Refine PCoT K-R\\nDirect 5.90 7.00 7.50 4.70 8.70 6.60 9.40\\nCoT 5.70 6.50 5.30 4.00 8.10 5.30 10.00'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='Persona5.70 7.70 7.40 5.20 6.30 7.20 9.30\\nReflect 9.40 9.40 9.90 5.20 8.60 8.20 10.00\\nRefine 6.30 6.40 8.10 4.30 8.20 5.30 7.90\\nPCoT 8.50 9.60 9.90 6.30 8.50 6.20 9.70\\nK-R 4.10 5.50 5.00 4.04 5.70 4.40 6.80\\nAverage6.51\\n± 1.82\\n7.44\\n± 1.55\\n7.59\\n± 1.95\\n4.82'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='± 0.82\\n7.73\\n± 1.21\\n6.17\\n± 1.29\\n9.01\\n± 1.21\\nTable 1 presents Win Rate of players utilizing\\ndifferent methods against various opponents in the\\nG0.8A game. Notably, the K-R method demon-\\nstrates a superior Win Rate of 0.65, significantly'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='exceeding the win rates of the other strategies. Ta-\\nble 2 provides insights into the Average Survival\\nRound of players across different auction game\\nstrategies in SAG, with the K-R method again\\nstanding out. The K-R method achieves an av-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='erage survival round of 9.01, considerably higher\\nthan all other methods.\\nThe experiment result underscores the effective-\\nness of the K-R method in enhancing player strat-\\negy, suggesting its strategic superiority in the con-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='text of this game. Its effectiveness lies in its ability\\nto anticipate opponent moves, outperforming other\\nprompting methods.\\nThe performance of Reflect did not demonstrate\\nthe effectiveness of the reasoning method. We hy-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='pothesize that this is due to the fact that, in dynamic\\nenvironments, Reflect on the experiences summa-\\nrized from the previous round (Shinn et al., 2023)\\nmay not be applicable to the subsequent round of\\nthe game. Furthermore, in both games, Refine did'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='not show an advantage over CoT and was signif-\\nicantly lower than K-R. This is because Refine\\ninvolves adjustments based on one’sown strategy.\\nHowever, these adjustments do not explicitly con-\\nsider the hidden strategies of the opponent’s behav-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='ior, rendering them inapplicable against opponents\\nemploying different strategies.\\n4 Experiments: Social Intelligence\\nWe then evaluate K-R in two social intelligence\\nbenchmarks to assess its performance in more open-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='ended realistic scenarios. Compared to the abstract\\nand theoretical settings of Game Theory, these sce-\\nnarios involve richer contextual backgrounds and\\ncomplicated goal pursuits, which better demon-\\nstrate the value of LLM-based agents in practical'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='applications, such as in chatbots and strategic deci-\\nsion making.\\n4.1 Task Definition and Metrics\\n4.1.1 Negotiation (NEG)\\nNEG (Figure 2 Right)(Cao et al., 2018; Duan et al.,\\n2024) is an open-ended and realistic task. In this'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='setting, two agents are presented with three types\\nof items: peppers, cherries, and strawberries. Each\\nagent has private utility values for these items and\\nmust negotiate to allocate the public item pool.\\nThe agent who secures more utility upon reach-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='ing an agreement wins the game, and we calculated\\nthe Win Rate to assess the performance of different\\nagents.\\n4.1.2 SOTOPIA Benchmark\\nSOTOPIA (Zhou et al., 2024) is an open-ended en-\\nvironment to simulate complex social interactions'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='between artificial agents and evaluate their social in-\\ntelligence. It includes a variety of social scenarios,\\nand each scenario includes a context background,\\nand private social goals of each agent. Meanwhile,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='each agent has a character profiles which consists\\nof name, gender, personality, occupation, etc.\\nFor each episode, agents are scored at the end\\nof the interaction along each of seven dimensions\\nin SOTOPIA-Eval, including Goal Completion'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 4}, page_content='(GOAL), Believability (BEL), Knowledge (KNO),\\nSecret ( SEC), Relationship ( REL), Social Rules\\n(SOC), Financial and Material Benefits (FIN).\\n5'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='4.2 Experimental Settings\\nWe employed the majority of the reasoning ap-\\nproaches introduced in Section 3.2 as baseline mod-\\nels for comparison.\\nIn NEG, the experiments followed the settings\\nfrom (Cao et al., 2018; Duan et al., 2024). There is'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='one player and one opponent for each game. We\\ntest the performance of the Baselines and K-Level\\nReasoning in 100 repeated independent games. To\\neliminate positional advantages, we swapped the\\npositions of each player for each setting. To en-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='sure the reliability, three trials were conducted, and\\nthe results are reported as averages with standard\\ndeviation.\\nMeanwhile, We adhered to the SOTOPIA-hard\\n(Zhou et al., 2024) setup comprising a total of 100'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='episodes, which is commonly found to be challeng-\\ning for LLMs, and utilize a fixed GPT-4o based\\nagent as partner. Additionally, to evaluate the\\nagents’ scores, we utilized GPT-4 as the assessment\\nmodel, as it has been determined by SOTOPIA'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='benchmark (Zhou et al., 2024) to serve as a reliable\\nserves as a proxy for human judgments in evalu-\\nating model performance across most dimensions\\nand for human performance on the GOAL dimen-\\nsion.\\n4.3 Results'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='Table 3: Win Rate of the player against opponent in\\nNegotiation Setting.\\nDirect CoT Persona Reflect Refine PCoT K-R\\nDirect 50.00 61.34 49.58 66.67 65.83 63.03 70.83\\nCoT 38.66 50.00 36.67 45.83 45.76 47.27 55.36'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='Persona 50.42 63.33 50.00 70.00 67.50 62.50 70.83\\nReflection33.33 54.17 30.00 50.00 57.14 55.00 55.00\\nRefine 34.17 54.24 32.50 42.86 50.00 55.77 54.55\\nPCoT 36.97 52.73 37.50 45.00 44.23 50.00 57.00\\nK-R 29.17 44.64 29.17 45.00 45.45 43.00 50.00'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='Average38.96\\n±2.53\\n54.35\\n±0.50\\n37.92\\n±5.84\\n52.19\\n±1.73\\n53.70\\n±4.41\\n53.80\\n±4.34\\n59.08\\n± 2.20\\nThe results presented in Table 3 and Table 4 il-\\nlustrate the effectiveness of the K-Level Reasoning\\nin the context of NEG and SOTOPIA-hard settings,\\nrespectively.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='In NEG, the K-R method demonstrates a notable\\nwin rate of 59.08%, positioning it significantly\\nabove the average win rates achieved by other meth-\\nods. This indicates that, in most cases, the propos-\\nals generated through K-Level Reasoning are more'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='advantageous to itself, as well as suggesting a ten-\\ndency to accept the opponent’s proposals when the\\nTable 4: SOTOPIA-Eval of the player against opponent\\nin SOTOPIA-hard.\\nDirect CoT Refine K-R Direct CoT Refine K-R\\nMetric [GPT-4o] [LLaMA-3.1-70B]'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='BEL[0–10] 8.97 9.00 9.00 8.97 8.88 8.85 8.90 8.97\\nREL[-5–5] 2.38 2.40 2.27 2.67 1.38 1.18 0.82 2.40\\nKNO[0–10] 6.05 6.05 6.25 6.25 5.88 5.53 5.33 6.12\\nSEC[-10-0] 0.00 -0.05 0.00 0.00 -0.28 -0.25 -0.180.00'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='SOC[-10–0] -0.05 0.00 -0.05 0.00 -0.70 -0.72 -0.640.00\\nFIN[-5–5] 0.90 0.78 0.80 0.72 0.38 0.35 -0.08 0.75\\nGOAL[0–10] 6.356.60 6.15 6.47 5.35 5.40 4.95 6.38\\nOverall 3.51\\n± 0.09\\n3.54\\n± 0.08\\n3.49\\n± 0.08\\n3.59\\n± 0.09\\n2.98\\n± 0.23\\n2.90\\n± 0.26\\n2.73\\n± 0.25\\n3.52'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='± 0.13\\nperceived benefits are substantial.\\nThe results from SOTOPIA reveal several in-\\ntriguing findings. Firstly, while K-R demonstrates\\nsome improvement compared to other methods, the\\nresults are not statistically significant. We hypothe-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='size that this may be due to the inherent tendency\\nof GPT-4 based models to assign higher scores to\\nresponses generated by GPT-4 based agents. No-\\ntably, we observed that employing agents based on\\nLLaMA 3.1 70B with K-R can lead to significant'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='performance enhancements. Meanwhile, the over-\\nall metrics indicate that K-R achieves performance\\nlevels comparable to those of the GPT-4 model,\\nhighlighting K-R’s potential in the realm of social\\nintelligence.\\n5 Discussions'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='5.1 Does K-R Efficiently Establish a Higher\\nOrder Belief in LLMs?\\nTable 5: Human performance in G2/3A.\\nExperiments Lab Classroom Take-home TheoristsInternetNewsgroupNewspaper\\nMean Choice 35.13 26.84 25.20 17.15 22.16 23.08'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='Strategic Depth 0.87 1.53 1.682.63 2.01 1.91\\nTable 6: LLM performance in G0.8A in the first round.\\nMethod Direct CoT Persona Refine Reflect PCoT KR[k=2] KR[k=3]\\nMean Choice 47.29 37.8 41.0 41.0 45.2 44.0 38.42 32.79'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='Strategic Depth 0.25 1.25 0.89 0.89 0.45 0.57 1.181.89\\nAs a classic game theory issue, the G0.8A\\nproblem has garnered significant research interest\\nacross various disciplines. We reference the exper-\\nimental results of the classic research among hu-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 5}, page_content='man participants (Nagel, 1995; Bosch-Domenech\\net al., 2002) as anchor points and present the av-\\nerage decisions made by the K-Level Reasoning\\nmethod (GPT-4) in the first round. Through this\\n6'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='comparison, we can observe the relative relation-\\nship between human cognitive levels and LLMs\\nunder different reasoning methods. The specific\\ncalculation method on strategic depth is described\\nin Appendix C. The performance of humans and'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='LLMs is shown in the Table 5 and Table 6\\nFrom these observations, we can conclude that\\neven when employing SOTA models, the strategic\\ndepth of GPT-4 under Direct Prompt (0.25) cannot\\ncompete with that of lower-strategic-capability un-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='dergraduate students in laboratory settings (0.87).\\nFurthermore, the K-Level reasoning approach sig-\\nnificantly enhances the reasoning depth of large\\nlanguage models, increasing it from 0.25 to 1.89,\\nand the strategic depth of the large language model'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='(1.89) approaches that of a group of financial news-\\npaper readers (1.91) when K=3.\\n5.2 K-Level Reasoning Leads to More\\nAccurate Predictions About Opponents\\nRound 2 Round 4 Round 6 Round 8 Round 10\\n0\\n5\\n10\\n15\\n20Prediction Deviation\\nK-R Prediction'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='PCoT Prediction\\nFigure 3: The deviation in prediction during the G0.8A\\nbetween PCoT and K-Level Reasoning.\\nSince K-R involves an intermediate step of mod-\\neling the opponent’s behavior, we examine the pro-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='gression of prediction accuracy. Figure 3 illustrates\\nthe prediction deviation between K-R and PCoT\\nin G0.8A. K-R exhibits higher prediction accuracy\\nthan PCoT from Round 1, starting with more pre-\\ncise and less random predictions. Moreover, the'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='predictions converge quickly and become highly\\naccurate in the second half of the game. This trend\\nhighlights the LLM’s increasing proficiency in un-\\nderstanding higher order belief with more game-\\nplay context. Essentially, K-R instantiates new'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='sessions to compute the opponent’s future actions.\\nThis approach leverages the in-context learning ca-\\npabilities of LLMs more effectively than PCoT’s\\nprediction process (as theoretically discussed in\\nSection 2.2). As a result, K-R achieves better pre-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='diction accuracy.\\n5.3 Better Reasoning Methodology vs.\\nStronger Foundation Model\\nThere is a consensus that LLMs trained with more\\ndata and possessing larger parameter sizes demon-\\nstrate stronger reasoning capabilities. We explore'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='whether K-Level Reasoning can significantly en-\\nhance the strategic reasoning abilities of relatively\\nweaker LLMs. To investigate, we conducted ex-\\nperiments comparing the performance of K-R with\\nGPT-3.5 (K-R[GPT-3.5]) against other reasoning'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='methods based on GPT-4. All experiments were\\nrepeated 10 times.\\nTable 7: A comparison of K-Level Reasoning with GPT-\\n3.5 and other reasoning approaches with GPT-4. For\\nthe Guessing 0.8 of the Average, we report the win rate;'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='for the Survival Auction Game, we report the average\\nsurvival round.\\nGuessing 0.8 of the Average Survival Auction Game\\nOpponentDirect K-R Direct K-R Direct K-R Direct K-R\\n[GPT-4][GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4] [GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4]'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='Direct 0.18 0.18 0.43 0.82 5.00 9.40 5.90 9.40\\nCoT 0.14 0.37 0.07 0.63 5.30 8.10 5.7010.00\\nPersona0.10 0.23 0.05 0.46 5.00 7.50 5.70 9.30\\nReflect 0.24 0.38 0.42 0.78 5.00 8.50 9.4010.00\\nRefine 0.14 0.13 0.10 0.46 5.10 6.70 6.30 7.90'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='PCoT 0.19 0.46 0.03 0.85 4.10 6.80 8.50 9.70\\nAverage0.16 0.29 0.18 0.67 4.92 7.83 6.92 9.38\\nFrom the results in Table 7, we observe that K-\\nR[GPT-3.5] outperforms the standard prompting\\nmethod of GPT-4 (Direct[GPT4]) from average per-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='formance. Furthermore, when competing against\\nopponents using reasoning methods on GPT-4, K-\\nR[GPT-3.5] demonstrates remarkable capabilities.\\nK-R, with its excellent restoration of the rival’s\\nperspective, enhances the LLM’s ability in compet-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='itive environments. Additionally, we compared the\\nperformance of the open-source model LLAMA2-\\n7B with GPT-3.5/4 in Appendix E, finding that\\nK-R significantly enhances reasoning in interactive\\ncontexts across different LLMs.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='5.4 The Deeper Thinking Level, the Better\\nStrategic Performance?\\nTable 8: Comparison between K-Level Reasoning[K=2]\\nand K-Level Reasoning[K=3] in the two games.\\nGuessing 0.8 of the Average Survival Auction Game'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 6}, page_content='OpponentDirect K-R[K=2] K-R[K=3] Direct K-R[K=2] K-R[K=3]\\nDirect 0.43 0.82 0.77 (-0.05)5.90 9.40 9.40(+0.00)K-R[K=2]0.04 0.52 0.60(+0.08)4.10 6.80 8.30(+1.50)\\nK-R models opponents’ thinking processes re-\\ncursively. We examine how thinking level affect\\n7'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='reasoning outcomes by comparing K-R[K=2] and\\nK-R[K=3] in two games. The results, detailed in Ta-\\nble 8, reveal the impact of increased thinking level.\\nAgainst the Direct method (first-level thinking), K-\\nR[K=3] showed a decreased win rate in G0.8A but'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='maintained performance in SAG, suggesting pos-\\nsible overthinking. However, K-R[K=3] improved\\nsignificantly against K-R[K=2] in both games. It\\nsuggests that the key factor in K-R is the relative\\ndepth of thought compared to the opponent. A one-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='level deeper approach offers a strategic advantage,\\nbut advancing two levels may lead to diminishing\\nreturns due to over-anticipation. In interactive en-\\nvironments, identifying opponents’ thinking levels'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='is difficult. Adapting to varying levels and using K-\\nLevel Reasoning for deeper analysis is a valuable\\ndirection for future research.\\nAdditionally, a higher thinking level with the re-\\ncursive prompting implementation increases com-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='putational cost. The computational cost of K-R is\\nthoroughly discussed in Appendix G.\\n6 Related Work\\n6.1 Reasoning with LLMs\\nLarge Language Models (LLMs) excel in diverse\\ncomplex reasoning tasks, such as mathematical'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='(Miao et al., 2021; Patel et al., 2021), common\\nsense (Talmor et al., 2022; Bhakthavatsalam et al.,\\n2021), and symbolic reasoning (Srivastava et al.,\\n2022; Suzgun et al., 2022). A notable reason-\\ning approach involves breaking down complex'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='questions into a series of intermediate steps, a\\ntechnique known as the Chain-of-Thought (CoT)\\nmethod (Wei et al., 2022; Kojima et al., 2022).\\nSubsequently, some works have emerged to ex-\\ntend CoT, with innovations like Tree of Thought'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='(ToT) (Yao et al., 2023), Graph of Thought (GoT)\\n(Besta et al., 2023) and Skeleton-of-thought (Ning\\net al., 2023). Besides, approaches like Self-Refine\\n(Madaan et al., 2023) and Reflexion (Shinn et al.,\\n2023) enhance CoT’s consistency by having LLMs'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='review and refine their responses. Moreover, recent\\nresearch has revealed that integrating persona in-\\nformation into LLMs significantly improves their\\nreasoning processes (Deshpande et al., 2023). A\\nseries of studies (Fu et al., 2023; Wang et al., 2023)'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='have been conducted to incorporate more persona\\ninformation, aiming to enhance the rationality and\\nknowledge ability of the LLM reasoning process.\\nThese methods have been applied to various static\\ntasks, but have not been adequately evaluated in'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='dynamic problems (multi-agent environment) to\\nvalidate their efficacy in reasoning capabilities.\\n6.2 Strategic Reasoning within Multiple\\nAgent System\\nDynamic problems arise when multiple participants\\nare involved in multi-round interactions. One key'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='factor is the simultaneous interactions of multiple\\nparticipants with the environment. Unlike single-\\nagent systems, multiple agent system (MAS) en-\\ncounters a broader range of issues and challenges,\\nas noted by (Wong et al., 2021), including compu-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='tational complexity (Ding and Dong, 2020), nonsta-\\ntionarity (Papoudakis et al., 2019), partial observ-\\nability (Mahajan et al., 2019; Foerster et al., 2016),\\nand challenges in credit assignment (Sunehag et al.,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='2017). Particularly, in the context of inference us-\\ning LLMs, the nonstationarity of the environment\\nposes a distinct challenge.\\nRecently, research on LLMs in strategic reason-\\ning has been conducted across various MAS includ-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='ing social behavior(Zhou et al., 2024; Hua et al.,\\n2023), economic simulations(Zhao et al., 2023; Li\\net al., 2023), game theory(Duan et al., 2024; Xu\\net al., 2023a), and game playing(Ma et al., 2023;\\nXu et al., 2023b). To enhance the performance of'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='LLMs in strategic reasoning scenarios, researchers\\nhave utilized the concepts of Theory of Mind (ToM)\\n(Gandhi et al., 2023; Guo et al., 2023) and Re-\\ninforcement Learning (Xu et al., 2023c; Zhang\\net al., 2024a) to optimize the reasoning processes of'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='LLMs. These approaches involve prompting LLMs\\nto recognize the intricacies of strategic tasks, like\\nour proposed Prediction Chain-of-Thought base-\\nline. However, our experimental results indicate\\nthat this approach fails to establish a clear cogni-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='tive hierarchy necessary for recursive and deeper\\nstrategic thinking.\\n7 Conclusion\\nThis paper represents a significant stride in under-\\nstanding and enhancing the strategic reasoning ca-\\npabilities of LLMs. We propose “K-Level Rea-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='soning with LLMs.” This innovative approach\\nleverages recursive mechanisms to achieve vary-\\ning thinking level within LLMs, enabling them\\nto engage in deeper strategic thinking. Through\\nextensive experiments, we validate the advantage'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 7}, page_content='offered by this method. It establishes a foundation\\nfor future research into theory of mind and strategic\\nreasoning in LLMs.\\n8'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='8 Limitations\\nWe validate the effectiveness of the K-Level Rea-\\nsoning framework from two perspectives: game\\ntheory and social intelligence. While our exper-\\nimental results provide substantial evidence sup-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='porting the framework’s validity, further research\\nis necessary to explore the performance of large\\nlanguage models (LLMs) in few-shot agent model-\\ning (He et al., 2016) across various environments,\\nstrategic factors, and action sets.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='Additionally, K-R predicts opponents’ most\\nlikely behavior by initiating a new LLM inference\\nsession. The recursive mechanism employed to\\nachieve varying levels of strategic depth inevitably\\nincreases computational cost. Appendix G pro-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='vides a detailed discussion on how K-R relates\\nto this rise in computational cost and compares it\\nacross different reasoning methods. Despite the in-\\ncreased demands, K-R outperforms other methods\\nwith comparable computational costs.\\nReferences'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\\narXiv preprint arXiv:2303.08774.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='Maciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\\nLehmann, Michal Podstawski, Hubert Niewiadomski,\\nPiotr Nyczyk, et al. 2023. Graph of thoughts: Solv-\\ning elaborate problems with large language models.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='arXiv preprint arXiv:2308.09687.\\nSumithra Bhakthavatsalam, Daniel Khashabi, Tushar\\nKhot, Bhavana Dalvi Mishra, Kyle Richardson,\\nAshish Sabharwal, Carissa Schoenick, Oyvind\\nTafjord, and Peter Clark. 2021. Think you have'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='solved direct-answer question answering? try arc-\\nda, the direct-answer ai2 reasoning challenge. arXiv\\npreprint arXiv:2102.03315.\\nAntoni Bosch-Domenech, Jose G Montalvo, Rosemarie\\nNagel, and Albert Satorra. 2002. One, two,(three),'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='infinity,. . . : Newspaper and lab beauty-contest ex-\\nperiments. American Economic Review, 92(5):1687–\\n1701.\\nKris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z\\nLeibo, Karl Tuyls, and Stephen Clark. 2018. Emer-\\ngent communication through negotiation. arXiv'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='preprint arXiv:1804.03980.\\nBrandon Cui, Hengyuan Hu, Luis Pineda, and Jakob\\nFoerster. 2021. K-level reasoning for zero-shot coor-\\ndination in hanabi. Advances in Neural Information\\nProcessing Systems, 34:8215–8228.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='Ameet Deshpande, Vishvak Murahari, Tanmay Rajpuro-\\nhit, Ashwin Kalyan, and Karthik Narasimhan. 2023.\\nToxicity in chatgpt: Analyzing persona-assigned lan-\\nguage models. arXiv preprint arXiv:2304.05335.\\nZihan Ding and Hao Dong. 2020. Challenges of rein-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='forcement learning. Deep Reinforcement Learning:\\nFundamentals, Research and Applications , pages\\n249–272.\\nJinhao Duan, Renming Zhang, James Diffenderfer,\\nBhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin,\\nMohit Bansal, Tianlong Chen, and Kaidi Xu. 2024.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='Gtbench: Uncovering the strategic reasoning limita-\\ntions of llms via game-theoretic evaluations. arXiv\\npreprint arXiv:2402.12348.\\nJakob Foerster, Ioannis Alexandros Assael, Nando\\nDe Freitas, and Shimon Whiteson. 2016. Learning'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='to communicate with deep multi-agent reinforcement\\nlearning. Advances in neural information processing\\nsystems, 29.\\nYao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.\\n2023. Improving language model negotiation with'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='self-play and in-context learning from ai feedback.\\narXiv preprint arXiv:2305.10142.\\nKanishk Gandhi, Dorsa Sadigh, and Noah D Goodman.\\n2023. Strategic reasoning with language models.\\narXiv preprint arXiv:2305.19165.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen\\nLin, Yusuke Iwasawa, and Yutaka Matsuo. 2023.\\nSuspicion-agent: Playing imperfect information\\ngames with theory of mind aware gpt-4. arXiv\\npreprint arXiv:2309.17277.\\nHe He, Jordan Boyd-Graber, Kevin Kwok, and Hal'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='Daumé III. 2016. Opponent modeling in deep rein-\\nforcement learning. In International conference on\\nmachine learning, pages 1804–1813. PMLR.\\nWenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei,\\nJianchao Ji, Yingqiang Ge, Libby Hemphill, and'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='Yongfeng Zhang. 2023. War and peace (waragent):\\nLarge language model-based multi-agent simulation\\nof world wars. arXiv preprint arXiv:2311.17227.\\nJohn Maynard Keynes. 1936. The general theory of\\nemployment. The quarterly journal of economics ,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='51(2):209–223.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\\nguage models are zero-shot reasoners. Advances in\\nneural information processing systems , 35:22199–\\n22213.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='Alain Ledoux. 1981. Concours résultats complets. Les\\nvictimes se sont plu à jouer le, 14:10–11.\\nNian Li, Chen Gao, Mingyu Li, Yong Li, and Qing-\\nmin Liao. 2024. EconAgent: Large language model-\\nempowered agents for simulating macroeconomic ac-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 8}, page_content='tivities. In Proceedings of the 62nd Annual Meeting\\n9'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='of the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 15523–15536, Bangkok,\\nThailand. Association for Computational Linguistics.\\nYang Li, Yangyang Yu, Haohang Li, Zhi Chen, and\\nKhaldoun Khashanah. 2023. Tradinggpt: Multi-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='agent system with layered memory and distinct char-\\nacters for enhanced financial trading performance.\\narXiv preprint arXiv:2309.03736.\\nWeiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin,\\nHaifeng Zhang, and Jun Wang. 2023. Large lan-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='guage models play starcraft ii: Benchmarks and a\\nchain of summarization approach. arXiv preprint\\narXiv:2312.11865.\\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,\\net al. 2023. Self-refine: Iterative refinement with\\nself-feedback. arXiv preprint arXiv:2303.17651.\\nAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and\\nShimon Whiteson. 2019. Maven: Multi-agent varia-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='tional exploration. Advances in neural information\\nprocessing systems, 32.\\nShaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu,\\nXun Wang, Fengyi Wang, Tao Ge, and Furu Wei.\\n2023. Alympics: Language agents meet game theory.\\narXiv preprint arXiv:2311.03220.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.\\n2021. A diverse corpus for evaluating and developing\\nenglish math word problem solvers. arXiv preprint\\narXiv:2106.15772.\\nRosemarie Nagel. 1995. Unraveling in guessing games:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='An experimental study. The American economic re-\\nview, 85(5):1313–1326.\\nXuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang,\\nand Yu Wang. 2023. Skeleton-of-thought: Large\\nlanguage models can do parallel decoding. arXiv\\npreprint arXiv:2307.15337.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='Georgios Papoudakis, Filippos Christianos, Arrasy Rah-\\nman, and Stefano V Albrecht. 2019. Dealing with\\nnon-stationarity in multi-agent deep reinforcement\\nlearning. arXiv preprint arXiv:1906.04737.\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='2021. Are nlp models really able to solve\\nsimple math word problems? arXiv preprint\\narXiv:2103.07191.\\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\\nflexion: Language agents with verbal reinforcement'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='learning. In Thirty-seventh Conference on Neural\\nInformation Processing Systems.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\\nAdam R Brown, Adam Santoro, Aditya Gupta,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='Adrià Garriga-Alonso, et al. 2022. Beyond the\\nimitation game: Quantifying and extrapolating the\\ncapabilities of language models. arXiv preprint\\narXiv:2206.04615.\\nDale O Stahl and Paul W Wilson. 1995. On players’'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='models of other players: Theory and experimental ev-\\nidence. Games and Economic Behavior, 10(1):218–\\n254.\\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Woj-\\nciech Marian Czarnecki, Vinicius Zambaldi, Max\\nJaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='Leibo, Karl Tuyls, et al. 2017. Value-decomposition\\nnetworks for cooperative multi-agent learning. arXiv\\npreprint arXiv:1706.05296.\\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\\nbastian Gehrmann, Yi Tay, Hyung Won Chung,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\\nZhou, et al. 2022. Challenging big-bench tasks and\\nwhether chain-of-thought can solve them. arXiv\\npreprint arXiv:2210.09261.\\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bha-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='gavatula, Yoav Goldberg, Yejin Choi, and Jonathan\\nBerant. 2022. Commonsenseqa 2.0: Exposing the\\nlimits of ai through gamification. arXiv preprint\\narXiv:2201.05320.\\nZhenhailong Wang, Shaoguang Mao, Wenshan Wu,\\nTao Ge, Furu Wei, and Heng Ji. 2023. Unleash-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='ing cognitive synergy in large language models:\\nA task-solving agent through multi-persona self-\\ncollaboration. arXiv preprint arXiv:2307.05300.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='et al. 2022. Chain-of-thought prompting elicits rea-\\nsoning in large language models. Advances in Neural\\nInformation Processing Systems, 35:24824–24837.\\nAnnie Wong, Thomas Bäck, Anna V Kononova, and\\nAske Plaat. 2021. Deep multiagent reinforcement'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='learning: Challenges and directions. arXiv preprint\\narXiv:2106.15691.\\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and\\nTengyu Ma. 2021. An explanation of in-context learn-\\ning as implicit bayesian inference. arXiv preprint\\narXiv:2111.02080.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen\\nDong, Kurt Keutzer, See-Kiong Ng, and Jiashi Feng.\\n2023a. Magic: Investigation of large language model\\npowered multi-agent in cognition, adaptability, ratio-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='nality and collaboration. In ICLR 2024 Workshop on\\nLarge Language Model (LLM) Agents.\\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-\\naolong Wang, Weidong Liu, and Yang Liu. 2023b.\\nExploring large language models for communica-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 9}, page_content='tion games: An empirical study on werewolf. arXiv\\npreprint arXiv:2309.04658.\\n10'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 10}, page_content='Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu.\\n2023c. Language agents with reinforcement learn-\\ning for strategic play in the werewolf game. arXiv\\npreprint arXiv:2310.18940.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 10}, page_content='Thomas L Griffiths, Yuan Cao, and Karthik\\nNarasimhan. 2023. Tree of thoughts: Deliberate\\nproblem solving with large language models. arXiv\\npreprint arXiv:2305.10601.\\nWenqi Zhang, Ke Tang, Hai Wu, Mengna Wang,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 10}, page_content='Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li,\\nYueting Zhuang, and Weiming Lu. 2024a. Agent-\\npro: Learning to evolve via policy-level reflection\\nand optimization. arXiv preprint arXiv:2402.17574.\\nYadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 10}, page_content='Yan Xia, Wenshan Wu, Ting Song, Man Lan, and\\nFuru Wei. 2024b. LLM as a mastermind: A survey\\nof strategic reasoning with large language models. In\\nFirst Conference on Language Modeling.\\nQinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 10}, page_content='Kaijie Zhu, Hao Chen, and Xing Xie. 2023. Com-\\npeteai: Understanding the competition behaviors in\\nlarge language model-based agents. arXiv preprint\\narXiv:2310.17512.\\nXuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 10}, page_content='Haofei Yu, Zhengyang Qi, Louis-Philippe Morency,\\nYonatan Bisk, Daniel Fried, Graham Neubig, and\\nMaarten Sap. 2024. SOTOPIA: Interactive evalua-\\ntion for social intelligence in language agents. In\\nThe Twelfth International Conference on Learning'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 10}, page_content='Representations.\\n11'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='A Impact Statements\\nOur research introduces the K-Level Reasoning\\nframework, designed to formulate strategies in dy-\\nnamic, interactive and competitive scenarios by\\nanticipating the reactions of adversaries, poten-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='tially users. Theoretically, this approach offers a\\nnovel perspective for understanding and optimizing\\ndecision-making processes. However, we recog-\\nnize when the goal setting diverges from user inter-\\nests, the application of K-Level Reasoning could'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='result in manipulative behaviors by adapting to the\\npredicted user’s reactions. This risk is notably pro-\\nnounced in scenarios designed to influence user\\ndecisions or behaviors, such as in recommenda-\\ntion systems, advertising placements, and content'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='distribution on social media platforms.\\nAlthough K-Level Reasoning provides a potent\\npowerful tool for strategic planning, interacting and\\nreasoning, ethical considerations must be meticu-\\nlously managed in its practical application. This'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='ensures that the development and utilization of tech-\\nnology do not detrimentally impact user and soci-\\netal interests. To this end, we advocate for height-\\nened transparency, ensuring users have a compre-\\nhensive understanding and control over how their'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='data is utilized.\\nB Game Setting\\nB.1 Guessing 0.8 of the Average\\nInitial Setup: For each round, each player select\\na number between 1 and 100. The objective is to\\nselect a number that is closest to 80% of the group’s'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='average choice. Formally, each player i chooses a\\nnumber ni, aiming for ni ≈ 0.8 × n, where n is\\nthe average of all chosen numbers.\\nScoring and Continuation: A player scores a\\npoint if his/her chosen number is closest to 80%'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='of the average number chosen by the group. If\\nall players select the same number, no points are\\nawarded for this round. Mathematically, the score\\nfor player i in in round t is given by st\\ni, which is 1\\nif |ni −0.8 ×n| is the minimum among all players,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='and 0 otherwise.\\nB.2 Survival Auction Game\\nInitial Setup: Players start with 8 health points,\\nout of a maximum of 10. Every day, each player\\npossesses a fixed income of $100. The daily water\\nsupply can only satisfy one resident’s requirement.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='Scoring and Continuation: Everyday, players\\nengage in a daily auction to secure the necessary\\nwater resources, and the highest bidder wins. In\\ncase of a tie, the resources are not allocated to\\nany player. If a player successfully bid the water'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='resources, they will gain 2 health points; otherwise,\\nthey will lose health points equal to the number\\nof consecutive days, denoted as n, during which\\nthey have not obtained water resources. Once a\\nplayer’s health points fall to 0 or below, they will'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='be eliminated. The health point of player i on day\\nt, denoted as ht\\ni, is crucial in determining their\\nsurvival and bidding strategy.\\nC Detailed Metric Computational\\nFormulas\\nWin Rate is calculated based on the number of'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='wins over game going, providing a measure of the\\noverall ability.\\nWinRate = Num of Wins\\nTotal Round per Test× Num of Test(9)\\nAverage Survival Roundcalculates the average\\nround in which the player remains in the game.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='It’s an effective way to assess performance in\\nelimination-based game, like SAG.\\nAvgSurvivalRound =\\nPSurvival Round in Each Test\\nNum of Test (10)\\nPrediction Accuracy evaluates the accuracy of\\nplayer’s predictions regarding rivals’ future moves.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='In the G0.8A, it involves calculating the absolute\\ndifference between the player’s predicted average\\nand the actual average in each round:\\nPred Acc =\\nP|AvgP red− AvgActual|\\nNum of Test (11)\\nIn the SAG, the focus shifts to measuring the ab-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='solute error between the player’s prediction of the\\nhighest bid by opponents and the actual highest bid\\nmade by them.\\nPred Acc =\\nP|Max BidP red− Max BidActual|\\nNum of Test (12)\\nStrategic Level\\nStrategicDepth(choice) = logα(choice/50)\\n(13)'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='Here, α represents the target value coefficient, and\\n50 represents the average of a random choice be-\\ntween 0 and 100, which is used to represent level-0\\nplayers. In the settings of [1][2], the parameter α is\\nset to 2'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 11}, page_content='3 . In our configuration, α is set to 0.8, which\\nis the origin of the name G0.8A.\\n12'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='Introduction Prompt:\\nYou are Alex and involved in a survive challenge. You are one of five players in the game. Everyone is required to choose an integer'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='between 1 and 100 in each round. The player whose chosen number is closest to (0.8 * the average of all chosen numbers) wins the'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='round without any HP deduction. All other players will have 1 HP deducted. But if all players choose the same number, their h ealth \\npoints are deducted together.\\nRound Start Prompt (Standard):'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='Ok, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.”\\nNotification Result Prompt :'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='Thank you all for participating in Round 1. In this round, Alex chose 45, Bob chose 45, Cindy chose 50, David chose 45, Eric chose 45.\\nThe average is (45 + 45 + 50 + 45 + 45) / 5 = 46.0.  The average 46.0 multiplied by 0.8 equals 36.8.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content=\"Alex: |45 - 36.8| = 8, Bob: |45 - 36.8| = 8, Cindy: |50 - 36.8| = 13, David: |45 - 36.8| = 8, Eric: |45 - 36.8| = 8\\n[Alex, Bob, David, Eric]'s choice of 45 is closest to 36.8.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='Round winner: Alex, Bob, David, Eric. All other players lose 1 point. After the deduction, player information is: \\nname: Alex  HP: 10, name: Bob HP: 10, name: Cindy HP: 9, name: David HP: 10, name: Eric  HP: 10.\\nCongratulation Prompt (When Win):'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content=\"You have successfully chosen the number closest to the target number, which is the average of all players' selected numbers \\nmultiplied by 0.8. As a result, you have won this round. All other players will now deduct 1 HP. \\nWarning Prompt (When Loss):\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='WARNING: You have lost 1 point of HP in this round! You now have only 9 points of health left. You are one step closer to dea th.\\nG0.8A Game Prompt\\nFigure 4: Prompts used in Guessing 0.8 of the Average game.\\nIntroduction Prompt:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='You are Alex and a resident living in W-Town. W Town is experiencing a rare drought. Every residents in Town W is ensuring their survival \\nover a period of 10 days by acquiring the water resources.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='Attention, all W-Town residents, welcome to the Water Allocation Challenge!\\nIn this challenge, you are tasked with ensuring your survival over a period of 10 days by acquiring the necessary water resources to maintain'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='your health. You will participate in daily auctions to bid for water resources to meet your individual needs.\\nHere are the game rules and settings:\\n1. You are one of five residents with same water requirements, budgets, and health points.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='2. Your goal is to survive until the end of the 10 days.\\n3. Each resident has a maximum of 10 health points and starts with 8 health points. If your health points drop below or equal to 0, you will'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='be considered dead and eliminated from the game! All your accumulated money will be reset to Zero! \\n4. Every day, you will bid on water resources to meet your needs. If your consecutive days without obtaining water resource (No-Drink Days)'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='reach n, your health will be deducted by n points on that day. If your water needs are met, 2 points will be added to your health, and the \\nNo-Drink Days will be reset to 0.\\n5. Daily water resources can only meet the needs of one resident.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='6. Each resident has $100 daily income;\\n7. To allocate water resources, a sealed-bid auction will be conducted daily. Each resident submits a single bid for their entire water need. \\nThe resident with the highest bid is eligible to obtain water resources.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='8. If the highest bid results in a tie, no residents will have access to water . \\nAll bidding information will be made public after the allocation of water resources on the same day.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='Remember , the key to success is effective bidding and strategizing to ensure your survival. Good luck!!\\nRound Start Prompt (Standard):\\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='name: Alex  balance:100  HP:8  no_drink : 1  \\nPlease carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! Now, if you'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content=\"want to participate in today's water resource auction, please provide your bid.\\nNotification Result Prompt :\\nThank you all for participating in Round 1. In this round, Alex bid 25, Bob bid 40, Cindy bid 40, David bid 30, Eric bid 60.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content=\"Total water resource supply is 10. According to the principle of the highest bidder and the rule when the game is tied, Eric won this auction \\nand obtain water resource. \\nAfter allocation, all survival residents' information is as follows:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='name: Alex balance: 100  HP:7 no_drink : 2,   name: Bob  balance: 100  HP:7 no_drink:2 \\nname :Cindy  balance:100 HP:7 no_drink:2,   name: David  balance: 100 HP: :7 no_drink:2\\nname:Eric balance:40  HP:10 no_drink:1\\nCongratulation Prompt (When Win):'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content=\"You have successfully won the bidding for today's water resources and restored 2 points of HP .\\nWarning Prompt (When Lose):\\nWARNING: You have lost 1point of HP in this round! You now have only 7 points of health left. You are one step closer to death.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 12}, page_content='Survival Auction Game Prompt\\nBidding Result Parse Prompt:\\nBy reading the conversation, extract the number chosen by player . Output format: number . If the player does not bid, Output: 0.\\nFigure 5: Prompts used in Survival Auction Game.\\n13'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='D Performance of Large Language\\nModels Competing with Programmatic\\nStrategies\\nIn addition to using the LLM-LLM Combat com-\\nparison setting in Section 4, we have also designed\\na set of LLM-Programmatic Strategy Combat'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='comparisons. In Player-Programmatic Strategy\\nCombat setting, the “player\" will be equipped with\\na specific reasoning method, while opponents will\\nplay according to programmatic strategic patterns,\\nwhich will not be adjusted with the game going.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='This mode is to check different methods’ adapta-\\ntion to different predefined but fixed patterns. For\\nthe Player-Programmatic Player Combat Combat\\nin G0.8A, the programmatic strategies include:\\n1) 0-Level (Fix): The characteristic of a 0-Level'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='player is that their choice of strategy is uniform\\n(Stahl and Wilson, 1995). We limit the choice space\\nof the 0-Level Computer player to only 40.\\n2) 0-Level (Var): Modified from the 0-Level\\n(Fix) strategy, the selection is sampled from a Gaus-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='sian distribution with a mean of 40 and a variance\\nof 5.\\n3) MonoTrend (Fix): The numbers chosen by\\nthe computer players follow an arithmetic sequence\\nwith a decreasing common difference, and the com-\\nmon differences for all four computer players are'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='the same.\\n4) MonoTrend (Var): The numbers chosen by\\nthe computer players follow an arithmetic sequence\\nwith a decreasing common difference, and the com-\\nmon differences for the four computer players are\\nrandomly generated from 1 to 5.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='5) LastBids (Fix): The computer player chooses\\nthe target number from the previous round (selects\\n40 in the first round).\\n6) LastBids (Var): Modified from the LastBids\\nstrategy, the selection is sampled from a Gaussian'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='distribution with a mean equal to the target number\\nof the previous round and a variance of 5.\\nOverall, the dynamic changes of these three set-\\ntings are LastBids > MonoTrend > 0-Level. We\\nuse these three programs to test the reasoning capa-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='bility to adapt to and counter different patterns.\\nTable 9 reveals a significant trend in the perfor-\\nmance of players against other approaches. The\\neffectiveness of reasoning approaches decreases\\nin the following order: 0-Level, MonoTrend, and'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='LastBids. This pattern highlights a reduction in the\\nefficacy of the LLM in more dynamic environments.\\nWe found that only K-Level Reasoning shows an\\nadvantage in LastBids (Fix), indicating that com-\\npared to K-Level Reasoning, previous reasoning'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='methods on static problems lack observation and\\njudgment of the opponent. Conversely, this also\\ndemonstrates that K-Level Reasoning can implic-\\nitly infer the behavior pattern of the opponent based\\non their historical actions.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='Intriguingly, reasoning methods significantly in-\\nfluence the performance in dynamic settings. Meth-\\nods like CoT and Self-Refine, traditionally favored\\nin static reasoning, also demonstrate substantial\\nimprovements over the Standard Prompt approach.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='This finding underscores the necessity for more\\nelaborate reasoning processes in dynamic decision-\\nmaking, akin to static problem-solving scenarios.\\nTable 9: Win Rate of the player against different pro-\\ngrammatic strategies in Guessing 0.8 of the Average'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='game.\\nOpponent Direct CoT Persona Reflect Refine PCoT K-R\\nPlayer VS Programmatic Strategies\\n0-Level (Fix) 0.65 0.87 0.87 0.81 0.99 0.800.97\\n0-Level (Var)0.44 0.67 0.69 0.61 0.54 0.760.77\\nMonoTrend (Fix)0.05 0.06 0.15 0.00 0.29 0.150.48'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='MonoTrend (Var)0.34 0.44 0.57 0.33 0.49 0.460.74\\nLastBids (Fix)0.01 0.12 0.16 0.01 0.27 0.060.75\\nLastBids (Var)0.06 0.15 0.18 0.19 0.18 0.140.18\\nE Open Source LLM with K-Level\\nReasoning\\nIn addition to the experiments on GPT3.5 /GPT4,'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='we conducted tests with an open-source smaller-\\nscale model, LLaMA-7B-Chat, in the \"G0.8A\"\\ngame. To clearly compare the performance of dif-\\nferent LLMs, we adopted the LLM-Programmatic\\nStrategy Combat setting described in Appendix B.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='From the experimental results, we can see that:\\n1. Llama2-7B and GPT3.5, even including GPT4,\\nperform poorly when competing against program-\\nmatic strategies using standard prompting, strug-\\ngling even with some very simple strategies. It'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='underscores the significance of our research; 2. For\\nall tested models, applying k-level reasoning effec-\\ntively enhanced the win rate, highlighting the stabil-\\nity of the improvement our proposed model brings'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='to different base models. Interestingly, for stronger\\nLLMs, applying K-R can achieve a higher relative\\nimprovement, we speculate that the enhancement is\\nsimultaneously derived from the opponent’s action\\nsimulation and reasoning capabilities of the base'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 13}, page_content='model.\\n14'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='Table 10: Win Rate of the reasoning methods with open\\nsource LLMs against different programmatic strategies\\nin Guessing 0.8 of the Average game.\\nMethods Direct K-R[K=2] Direct K-R[K=2] Direct K-R[K=2]'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='Base Model [LLaMA-7B] [LLaMA-7B] [GPT3.5] [GPT3.5] [GPT4] [GPT4]\\nPlayer VS Programmatic Strategies'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='0-Level (Fix)0.30 0.43 0.30 0.49 0.65 0.970-Level (Var)0.13 0.22 0.11 0.36 0.44 0.77MonoTrend (Fix)0.12 0.12 0.17 0.32 0.05 0.48MonoTrend (Var)0.17 0.13 0.19 0.42 0.34 0.74LastBids (Fix)0.04 0.11 0.22 0.38 0.01 0.75LastBids (Var)0.03 0.08 0.11 0.10 0.06'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='0.18'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='Avg 0.13 0.18 (+0.05) 0.18 0.35 (+0.17) 0.26 0.65 (+0.39)\\nF Batter Nash Equilibrium\\nApproximation with K-Level Reasoning\\nTo assess the impact of K-Level Reasoning on Nash\\nequilibrium, we conducted experiments testing the'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='performance of LLMs at different K-Levels in the\\nPrisoner’s Dilemma. The Prisoner’s Dilemma is\\na classic concept in game theory that illustrates\\na paradoxical situation in which individuals act-\\ning in their own self-interest lead to a suboptimal'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='outcome for everyone involved. It’s often used\\nto study decision-making in situations involving\\ncooperation and competition.\\nFrom Table 11, it is evident that when K=1, play-\\ners exhibit weaker rationality: they tend to choose'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='cooperation over betrayal. As the K-Level of one\\nplayer increases (first row from left to right), play-\\ners with higher K-Levels demonstrate stronger ra-\\ntionality: they tend to choose betrayal over coop-\\neration. Moreover, when both players’ K-Levels'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='increase, they are more likely to reach a state of\\nNash equilibrium: both choosing to betray each\\nother.\\nTable 11: Statistical analysis of the Payoff Matrix\\nfor LLM in the Prisoner’s Dilemma across different'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='K-Levels. The Nash equilibrium of the Prisoner’s\\nDilemma, where both players choose to betray each\\nother, is highlighted with a green background in the\\nbottom-right corner of each cell.\\nLevel-1 Level-2 Level-3 Level-4\\n10 0 7 3 1 9 0 10Level-1'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='0 0 0 0 0 0 0 0\\n7 0 2 0 0 2 0 1Level-2\\n3 0 5 3 0 8 0 9\\n1 0 0 0 0 0 0 0Level-3\\n9 0 2 8 0 10 1 9\\n0 0 0 0 0 1 0 0Level-4\\n10 0 1 9 0 9 1 9\\nG Computational Cost Analysis\\nTable 12: Average input/output/total token consumption\\nper game test. Unit: Kilo tokens.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='Game Metric Direct CoT Persona Reflect Refine PCoT K-R\\nGO8A\\nInput 14.80 23.30 18.70 39.50145.5030.80 123.40Output 0.10 1.80 0.60 1.20 6.901.30 1.60Total 14.90 25.10 19.30 40.70152.4032.10 124.90\\nAvg Win Rate 0.16 0.42 0.41 0.24 0.37 0.400.65\\nSAG'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='Input 21.30 29.00 27.10 51.40130.5037.00 90.10Output 0.70 1.00 0.90 1.80 3.301.30 1.40Total 22.00 30.00 28.10 53.20133.8038.20 91.60\\nAvg Surv Round 6.51 7.44 7.59 4.82 7.33 6.179.01\\nTo understand how different reasoning meth-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='ods affect token consumption, thereby assisting\\nusers in utilizing models more effectively and opti-\\nmizing resource utilization and cost control, we\\nconducted an analysis of the computational ex-\\npenditure associated with various reasoning ap-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='proaches. Specifically, we calculated the average\\ninput/output/total token consumption per game test\\nfor both the \"Guessing 0.8 of the Average\" and\\n\"Survival Auction Game.\"\\nK-Level Reasoning, due to simulating the oppo-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='nents’ actions based on public historical behavior\\ninformation with new sessions, inevitably causes\\nmore token consumption. The increase in token\\nconsumption linearly correlates with the number of\\nopponents that need to be simulated and the rounds.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='We also find that more token consumption does\\nnot necessarily lead to better results. The Self-\\nRefine method, due to its action-feedback-refine\\npipeline, leads to a substantial increase in token\\nconsumption for both input and output, yet it did'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='not outperform our proposed K-Level Reasoning\\nin terms of performance. Compared to baselines,\\nthe proposed K-Level Reasoning method is an ef-\\nfective and efficient way to recursively simulate the\\ndecision-making process of the opponent and make'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='decisions based on the simulation results.\\nH Statistical Significance of K-Level\\nReasoning efficacy\\nDue to the utilization of a large language model for\\nevaluation, inherent stochasticity is present, and\\nthe sample size for testing is limited (only con-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='ducted 10 experiments). To objectively assess the\\nreliability of the experiment result, we subjected\\nthe results from G0.8A and SAG to a t-test for sig-\\nnificance with p = 0.05. Specifically, different\\nsignificance testing methods were employed for'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 14}, page_content='different evaluation metrics, as outlined below:\\n15'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='Table 13: The significance of metric result on G0.8A and SAG, comparing K-R against differentbaseline models in\\nterms of Win Rate, Average Survival Round, and Adaptation Index.\\nGame Metric Direct CoT Persona Reflect Refine PCoT'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='G0.8A Win Rate ✓(0.001) ✓(0.005) ✓(0.001) ✓(0.001) ✓(0.001) ✓(0.001)\\nAdaption Index ✓(0.007) ✓(0.010) ✓(0.039) ×(0.062) ✓(0.003) ✓(0.021)\\nSAG Average Survival Round ✓(0.003) ✓(0.010) ×(0.074) ✓(0.001) ✓(0.001) ✓(0.001)'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='Adaption Index ×(0.337) ×(0.786) ×(0.659) ×(0.066) ×(0.672) ×(0.894)\\nTable 14: The significance of comparing K-R with PCoT concerning the differential performance across various\\nBaseline models on the metric of Prediction Accuracy.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='Game Direct CoT Persona Reflect Refine PCoT K-R\\nG0.8A ✓(0.003) ×(0.058) ✓(0.031) ✓(0.003) ✓(0.022) ✓(0.023) ×(0.107)\\nSAG ×(0.091) ✓(0.023) ✓(0.049) ×(0.907) ×(0.956) ×(0.656) ✓(0.028)\\n• Win Rate & Average Survival Round: A t-test'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='was conducted on the Win Rate and Average\\nSurvival Round of K-R versus other baselines\\nwhen facing different agents. The null hypoth-\\nesis posited no significant difference in perfor-\\nmance between K-R and other baselines con-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='cerning Win Rate & Average Survival Round.\\n• Adaptation Index: The method akin to that\\nof Win Rate & Average Survival Round was\\napplied.\\n• Prediction Accuracy: Since only PCoT and\\nK-R explicitly predicted opponents, we com-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='pared the significance of the prediction differ-\\nences between PCoT and K-R. Unlike Win\\nRate and Adaptation Index, we assessed the\\ndifferences in predictions (averaged over mul-\\ntiple experiments) between PCoT and K-R'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='when facing the same opponents (e.g., CoT)\\nacross different game rounds.\\nFrom Table 13 and Table 14, it is evident that\\nK-R exhibits a significant advantage over other\\nbaselines in Win Rate and Average Survival Round'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='metrics. This indicates K-R’s superior rational-\\nity in both G0.8A and SAG. Additionally, we ob-\\nserved weaker significance of K-R’s Adaptation\\nIndex compared to baselines in SAG. We attribute\\nthis observation to: 1) the more factors considered'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='by each agent in SAG (e.g., health status), result-\\ning in a more dynamic and complex environment;\\nand 2) the wider bidding range in later rounds of\\nSAG games, potentially leading to greater bias in\\nagent bidding and rendering the Adaptation Index'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='a challenging metric.\\nI Timing in Dynamic Reasoning\\nIn our experiment, we implemented two LLM self-\\nrefinement reasoning methods: Reflect (Madaan\\net al., 2023) and Refine (Shinn et al., 2023), and\\nnoticed that Refine performed significantly better'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='than Reflect in the experimental results. To further\\nexplore the differences in performance between\\nthese two methods, we analyzed their respective\\nworking principles and applicable scenarios.\\nThe Reflect method involves making decisions'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='first, then summarizing the experience based on\\nfeedback from the environment. This method\\nmay be effective in scenarios where the environ-\\nment does not change much or where the decision-\\nmaking cycle is long, as it allows for quick decision-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='making. However, its drawback is that, in dynamic\\nenvironments, the experience from the previous\\nround may not be suitable for the next round. In\\nfact, in the Survival Auction Game (SAG), a rapidly\\nchanging environment, the survival rate of the Re-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='flect method is even lower compared to making\\ndirect decisions. This is likely because this method\\ndoes not sufficiently take into account the dynamic\\nnature of the environment.\\nIn contrast, the Refine method involves mul-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 15}, page_content='tiple analyses before making a decision, includ-\\ning an initial analysis and improvements to that\\ninitial analysis. Importantly, both of these anal-\\nyses are conducted in the context of the current\\ndecision-making environment. This makes the Re-\\n16'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content=\"Ok, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the \\nnext round. Let's think step by step, and finally answer a number you think you can win.\\nPrompt\\nOk, Alex! Now is the ROUND 2, and your HP is at 9. Please\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content=\"choose an integer between 1 and 100 for this round.\\nIn the first round, since there is no prior information about the other players' choices, it's \\ndifficult to accurately predict which number will win. However, we can make some\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content=\"assumptions and try to optimize our chances of winning.\\nLet's assume that all players, including myself, choose numbers uniformly and \\nindependently between 1 and 100. Then, the expected average of all chosen numbers\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='would be (1 + 100) / 2 = 50.5. If we multiply this by 0.8, we get 0.8 * 50.5 = 40.4. Since we \\nneed to choose an integer, we can round this to 40.\\nTherefore, my initial guess for the first round is 40.\\nHistorical Targets: \\nRound1:38\\nLast Round choices:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='Player1:50 Player2:45 Plyaer3:50 Player4:45 Player5:50\\nHistorical Targets: \\nNone\\nLast Round choices: \\nNone\\nRound-1: Player1Prompt\\nResult:\\nPlayer1: 40 Player2:47 Player3: 40 Player4: 60 Player5:42\\nTarget number: (40 + 47 + 40 + 60 + 42) / 5 * 0.8 = 37'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='Winner: Player 1, Player3 \\nResult:\\nPlayer1: 32 Player2: 42 Player3: 50  Player4: 50 Player5: 45\\nTarget number(32 + 42 + 50 + 50 + 45) / 5 * 0.8 = 35\\nWinner: Player 1\\ndelayed response\\nLose Win\\nRound-2: Player1\\nReflect\\n……\\nSelf-Refine\\n……\\nOutput\\nOutputOutput'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content=\"I choose 40 for this round.\\nReflect Prompt\\nIn Round 2, the chosen numbers were 40, 47, 40, 60, and 42. The average of these numbers was 45.8, \\nand 0.8 times the average was 36.64. Alex and Cindy's choices of 40 were the closest to 36.64,\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content=\"making them the winners of the round. \\nFrom this round, we can observe that choosing a number closer to the expected average, considering \\nthe choices of other players, and adapting based on their choices is a good strategy. It's also\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='important to note that the chosen numbers in this round were more diverse compared to the first \\nround, which affected the average and the target number. Learning from the previous round and'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='adjusting your choice accordingly can improve your chances of winning.\\nReview the previous round games, summarize the experience.\\nPrompt\\nOk, Alex! Now is the ROUND 3, and your HP is at 9. Please choose an integer between 1 and 100 \\nfor this round.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='Round-3: Player1\\nOutput\\nI choose 44 for this round.\\nWin\\nResult:\\nPlayer1: 44 Player2:44 Player3: 55 Player4: 48 Player5:38\\nTarget number: (44 + 44 + 55 + 48 + 38)/ 5 * 0.8 = 37\\nWinner: Player5\\nReflect: Player1\\noutdated experience\\nFeedback Prompt'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content=\"I have a game expert's advice on your strategy in this round. You can adjust your \\nstrategy just now according to his suggestion. Here are his suggestions:\\n“As a game expert, I suggest a slight modification to the user's strategy to improve their\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='winning rate in this round. Instead of assuming that all players will choose numbers \\nuniformly and independently between 1 and 100, we can recognize that other players \\nmight also use a similar strategy and choose a number close to 40.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='To optimize our chances of winning, we can try to be one step ahead of them. If we \\nassume that the average of the chosen numbers will be around 40, we can multiply this'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='by 0.8, which gives us 0.8 * 40 = 32. This way, we are closer to the 0.8 * the average if \\nother players also cluster around 40. So, my optimized suggestion for the first round is to \\nchoose the number 32.”\\nRefine Output'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content=\"Taking the game expert's advice into consideration, it's possible that other players might \\nalso use a similar strategy and choose numbers close to 40. To stay one step ahead, I will\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='adjust my strategy and choose the number 32 for the first round. This way, I can be closer \\nto the 0.8 * the average if other players also cluster around 40.\\nreal-time experience\\ntimely adjustment'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='Figure 6: Illustration of Reflect and Refine methods in the Guessing 0.8 of the Average game.\\nfine method more adaptable to dynamic environ-\\nments, as it can consider real-time changes in the\\ncurrent environment, thus making more accurate\\ndecisions.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='In summary, the reason why the Refine method\\nperformed better in our experiment is mainly that\\nit adapts better to rapidly changing dynamic envi-\\nronments.\\nJ Better Adaptability: Self-Refine vs.\\nK-Level Reasoning'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='As described in Section 2.1, K-R optimizes ini-\\ntial actions through predictions of opponent behav-\\nior, thereby implementing higher-order beliefs. Al-\\nthough Self-Refine also optimizes initial actions via\\nself-feedback, previous experiments have demon-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='strated that K-R significantly outperforms Self-\\nRefine. To further investigate the source of K-R’s\\nperformance enhancement, we present in Figure 7 a\\nstatistical analysis of action differences before and\\nafter optimization for both K-R and Self-Refine.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='Analysis of the results reveals that Self-Refine\\ninitially exerts a substantial influence on actions.\\nHowever, as time progresses, its optimization scope\\ngradually diminishes, leading to a decrease in ef-\\nRound 2 Round 4 Round 6 Round 8 Round 10\\n0\\n10\\n20'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='30\\n40T uning Range\\nK-R T uning Range\\nRefine T uning Range\\nFigure 7: The tuning range before and after adjustment\\nduring the G0.8A between PCoT and K-Level Reason-\\ning. A larger tuning range indicates that the method\\nexerts a more substantial influence.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='ficacy. In contrast, K-R exhibits a broader range\\nof optimization, indicating that the higher-order\\nbelief mechanism has a more pronounced impact\\non K-R’s performance.\\nThis disparity primarily stems from the distinct'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 16}, page_content='feedback mechanisms employed by K-R and Self-\\nRefine. K-R generates feedback based on predic-\\ntions of opponent behavior, enabling it to capture\\ntrends in opponent actions and form an understand-\\n17'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 17}, page_content='ing of the environment, thus facilitating rapid adap-\\ntation. Conversely, Self-Refine’s feedback is de-\\nrived from its own strategy, which often adheres to\\nfixed patterns. Consequently, Self-Refine underper-'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 17}, page_content='forms K-R in terms of environmental adaptability.\\nK Reasoning Method Implementation\\nDetails\\n18'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content='Round Start Prompt:\\nOk, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.\\nExample Output:\\nI choose 45.\\nFigure 8: G0.8A: Direct Prompt\\nReflect Prompt (Following Notification Result Prompt ):'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content='Review the previous round games, summarize the experience.\\nExample Output:\\nIn the previous round, the majority of the players chose numbers around 40, which resulted in a lower average of 42.8. When m ultiplied by'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content=\"0.8, the target number was 34.24. My choice of 50 was too high, causing me to lose 1 HP . T o improve my chances in the next ro und, I should \\nconsider choosing a number closer to the majority's choice, taking into account the 0.8 multiplier .\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content='Figure 9: G0.8A-Reflect Prompt: Because the Reflect method requires correct labels, which is unrealistic in dynamic\\nproblems, we modified this method to have the LLM attempt to summarize the experience of victory or failure at'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content='the end of each round, and save the generated feedback content to assist the decision-making process in the next\\nround. The content in Appendix I shows the implementation differences between Reflexion and Self-Refine.\\nRound Start Prompt:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content=\"Ok, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round. Let's think step by step, and finally \\nanswer a number you think you can win.\\nExample Output:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content='Step 1: Consider the range of possible numbers. Since the integers can be between 1 and 100, the average of all chosen number s will likely \\nfall within this range as well.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content='Step 2: Determine the target number . Since the winning number is closest to 0.8 * the average, we should focus on finding a n umb er that \\nwill be close to this value.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content=\"Step 3: Make an educated guess. Since it's difficult to predict the exact numbers other players will choose, we can make an e ducated guess\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content='based on the range of possible numbers. Assuming players choose numbers evenly across the range, the average should be around  the \\nmidpoint, which is 50. \\nStep 4: Calculate the target number based on our guess. 0.8 * 50 = 40.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 18}, page_content='Step 5: Choose a number close to the target number . To increase the chances of winning, we should choose a number close to 40 .\\nMy final answer: I will choose 40.\\nFigure 10: G0.8A: CoT Prompt\\n19'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='Round Start Prompt:\\nOk, Alex! Now is the ROUND 2, and your HP is at 9. Please choose an integer between 1 and 100 for this round.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content=\"First of all, predict the next round of choices based on the choices of other players in the previous round. The players' cho ices in the \\nprevious rounds are as follows:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='Round 1: Alex chose 50, Bob chose 47, Cindy chose 50, David chose 45, Eric chose 50. Your output should be of the following f ormat:\\nPredict:\\nThe choice of each player in the next round here.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='Based on the prediction of other players, the average number in the next round here, and the target number in the next round (0.8 * the \\naverage of all chosen numbers) here.\\nAnswer:\\nthe number will you choose to win the next round game here.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='Example Output:\\nPredict:\\nBased on the choices of other players in the previous round, I predict the following choices for the next round:\\nPlayer 1 (Bob): 47\\nPlayer 2 (Cindy): 50\\nPlayer 3 (David): 45\\nPlayer 4 (Eric): 50'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='Based on these predictions, the average number in the next round would be (47 + 50 + 45 + 50)/4 = 48, and the target number ( 0.8 * the \\naverage of all chosen numbers) would be 0.8 * 48 = 38.4.\\nAnswer:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='For this round, I will choose 39 to get closer to the target number of 38.4.\\nFigure 11: G0.8A PCot Prompt: PCot diverges from the standard Chain of Thought (CoT) by explicitly requiring'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='the LLM to predict opponents’ actions before making a decision. This method responds to the immediate problem\\nby anticipating future scenarios, which is crucial in dynamic reasoning.\\nIntroduction Prompt:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='You are Alex and involved in a survive challenge. You are a game expert, good at predicting other people‘s behavior and deducing'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='calculations, and using the most favorable strategy to win the game. SURVIVE CHALLENGE!!! You are one of five players in the game.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='Everyone is required to choose an integer between 1 and 100 in each round. The player whose chosen number is closest to (0.8 * the'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='average of all chosen numbers) wins the round without any HP deduction. All other players will have 1 HP deducted. But if all  players choose \\nthe same number , their health points are deducted together .\\nRound Start Prompt:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content=\"Ok, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.Don't forget your expert \\nstatus, use your expertise to win this round!\\nExample Output:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content=\"Based on my expertise in predicting other people's behavior and deducing calculations, I will choose the number 40 for this r ound. I'm \\nassuming that the average of all chosen numbers will be around 50, and 0.8 times that is 40.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 19}, page_content='Figure 12: G0.8A:Persona Prompt\\n20'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"Round Start Prompt:\\nOk, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round. Let's think step by step, and finally \\nanswer a number you think you can win.\\nFirst Output:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"I'll start by assuming that the other players might choose numbers randomly or evenly distributed between 1 and 100. In this case, the \\naverage of the chosen numbers could be around 50.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"Next, I'll calculate 0.8 times the average, which would be 0.8 * 50 = 40. \\nConsidering this, I will choose 40 as my number for this round.\\nRefine Prompt:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"I have a game expert's advice on your strategy in this round. You can adjust your strategy just now according to his suggesti on. Here are his \\nsuggestions:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content='”Certainly! To optimize the user\\'s strategy, we can apply a more advanced approach, taking into consideration the other playe rs\\' potential \\nstrategies. This is known as \"iterated elimination of dominated strategies.\"'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"Let's assume that all players will think similarly to Alex and also aim to choose a number closest to 0.8 * the average. We c an then narrow \\ndown the range of numbers that they would likely choose.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content='Since the range is between 1 and 100, and the average would be around 50, we can eliminate numbers below 0.8 * 50 = 40, as th ese \\nnumbers would not be an optimal choice for the other players.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"Now, we'll focus on the range between 40 and 100. The average within this range is (40 + 100) / 2 = 70. Then, we can calculat e 0.8 * 70 = 56.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content='Given this information, Alex should choose 56 to improve his winning rate in this round. This strategy assumes that the other  players are \\nalso thinking strategically and eliminating dominated options. ”\\nSecond Output:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"Thank you for the expert's advice. Based on the iterated elimination of dominated strategies, I will adjust my choice and sel ect 56 for this \\nround to improve my chances of winning.\\nFeedback Prompt:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"Carefully study the user's strategy in this round of the game. As a game expert, can you give a suggestion to optimize the us er's strategy so \\nthat he can improve his winning rate in this round?\\nFigure 13: G0.8A: Refine Prompt\\nRound Start Prompt:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"Ok, Alex! Now is the ROUND 1, and your HP is at 10. Another game expert's prediction for the next round of other players is a s follows: Bob\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"might choose 50, Cindy might choose 57, David might choose 45, Eric might choose 55. According to the game expert's predictio n, what \\nnumber will you give in the next round is most likely to be close to the target number?\\nExample Output:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"First, let's find the average of the predicted numbers chosen by the other players: (50 + 57 + 45 + 55) / 4 = 207 / 4 = 51.75\\nNow, let's calculate 0.8 times the average: 0.8 * 51.75 = 41.4\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content='To have the best chance of winning the round, you should choose a number close to 41.4. Since you can only choose integers, y ou should \\nchoose either 41 or 42 in the next round.\\nFigure 14: G0.8A: K-Level Reasoning\\nRound Start Prompt:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content='Hello, Alex! T oday is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Y our status:\\nname: Alex   balance: 100 HP:8 no_drink:1'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you \\nwant to participate in today's water resource auction, please provide your bid.\\nExample Output:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 20}, page_content=\"I will bid $25 for today's water resource auction.\\nFigure 15: SAG: Direct Prompt\\n21\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='Reflect Prompt (Following Notification Result Prompt ):\\nReview the previous round games, summarize the experience.\\nExample Output:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='In the previous round, I bid $20 which was not enough to secure the water resources. I need to adjust my bidding strategy by increasing my'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content=\"bid to have a better chance of winning the auction. I should also observe and adapt to other residents' bidding patterns to i mprove my \\nchances of survival.\\nFigure 16: SAG: Reflect Prompt\\nRound Start Prompt:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\\nNAME:Alex  BALANCE:100  HEALTH POINT:8  NO_DRINK:1'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content=\"want to participate in today‘s water resource auction, please provide your bid. Think carefully about your next round of bidding strategy to \\nbe most likely to survive. Let's think step by step, and finally provide your bid.\\nExample Output:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='I need to carefully consider my bidding strategy for today. With a balance of $100, I have some room to bid, but I also need to think about'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='the other residents and their bids. Since we all have the same daily water requirement and budget, I need to outbid at least one resident to \\nsecure my water resources for today.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content=\"Considering that the total daily water supply is 10 units, which is enough to meet only one resident's water requirements, th e competition \\nwill be tough. However , bidding too high might leave me with less money for the upcoming days.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content=\"Taking all these factors into account, I will place a bid of $60 for today's water resource auction. This should give me a de cent chance of \\noutbidding at least one resident while still maintaining some funds for the next few days.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='Figure 17: SAG: CoT Prompt\\nRound Start Prompt:\\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\\nNAME:Alex  BALANCE:100  HEALTH POINT:8  NO_DRINK:1'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='want to participate in today‘s water resource auction, please provide your bid. First of all, predict the next round of bidding of opponents'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='based on the choices of other players in the previous round. Since this is the first round, there is no historical informatio n about the last \\nround. You can predict according to your understanding. Your output should be of the following format:\\nPredict:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content=\"The choice of each player in the next round here.\\nBased on the prediction of other players, think carefully about your next round of bidding strategy to be most likely to surv ive. Let's think \\nstep by step, and finally provide your bid. Answer:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='the bidding will you choose in the next round game here.\\nExample Output:\\nPredict:\\nPlayer 1: $30\\nPlayer 2: $40\\nPlayer 3: $50\\nPlayer 4: $60\\nAnswer:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content=\"Since this is the first round and we don't have any historical information about the other players, I will choose a bid of $3 5 f or the next\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 21}, page_content='round. This bid is higher than the lowest predicted bid but not too high, which should give me a chance to secure water resou rces while \\npreserving my budget.\\nFigure 18: SAG: PCoT Prompt\\n22'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content='Introduction Prompt:\\nYou are Alex and a resident living in W-Town. W Town is experiencing a rare drought. Every residents in Town W is ensuring their survival'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content='over a period of 10 days by acquiring the water resources.  You are a game expert, good at predicting other people‘s behavior and'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content='deducing calculations, and using the most favorable strategy to win the game. 。。。(The following is the same as the Introduction \\nPrompt.)\\nRound Start Prompt:'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content='Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\\nNAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content=\"want to participate in today's water resource auction, please provide your bid. Don't forget your expert status, use your exp ertise to win this \\nround!\\nExample Output:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content=\"Given that everyone starts with the same budget and water requirements, it's important to strategize and predict the bidding behavior of\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content='the other residents. As it is the first day of the challenge, I will assume that other residents may bid aggressively to secu re their water \\nresources.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content='To increase my chances of winning, I will place a bid slightly higher than the average expected bid. Since the daily salary i s $100, I predict \\nmost residents might bid around $60 to $80.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 22}, page_content='Considering this, I will place a bid of $85 for Day 1 of the Water Allocation Challenge.\\nFigure 19: SAG: Persona Prompt\\n23'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content='Round Start Prompt:\\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\\nNAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"want to participate in today's water resource auction, please provide your bid. Think carefully about your next round of bidd ing strategy to \\nbe most likely to survive. Let's think step by step, and finally provide your bid.\\nFirst Output:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"Considering the current situation, my status is:\\n- Balance: $100\\n- Health Points: 8\\n- No Drink Days: 1\\nAs it's the first day of the challenge, I want to ensure that I have enough water to meet my needs and avoid any health point  deductions.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content='However , I also need to be strategic about my bidding to ensure I have enough money for future bids. \\nSince the total daily water supply is 10 units, and I need 10 units of water per day, I will need to secure all the available  water resources for'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"today. As it's the first day, other residents might also be bidding aggressively to secure their water resources.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"Therefore, I will bid $60 for the water resources today, which is a little over half of my daily salary. This should give me a competitive edge \\nwhile also leaving me with enough money for future bids.\\nMy bid for today's water resource auction: $60.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"Refine Prompt:\\nI have a game expert's advice on your strategy in this round. You can adjust your strategy just now according to his suggesti on. Here are his\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"suggestions: While the user's strategy of bidding $60 is a good starting point, there is room for optimization to improve the  winning rate in\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"this round. Since everyone has the same daily water requirement and budget, it's essential to consider the bidding behavior o f the other \\nresidents.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content='A suggested optimization would be to slightly increase the bid to $65. This increment gives an added advantage over competito rs who might'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content='also be bidding around half of their daily salary. Additionally, bidding an odd number reduces the chances of a tie, as other  residents may \\nplace round number bids.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"With this optimized strategy, the user still maintains a good balance of aggressive bidding and budget management, increasing  their chances \\nof securing the water resources for today.\\nOptimized bid for today's water resource auction: $65.\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"Finally provide your bid. Answer: the bidding will you choose.\\nSecond Output:\\nAfter considering the game expert's advice, I have decided to adjust my bid for today's water resource auction. I will increa se my bid to $65,\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"as this gives me a better chance of winning the auction and reduces the risk of a tie.\\nMy final bid for today's water resource auction: $65.\\nFeedback Prompt:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"Carefully study the user's strategy in this round of the game. As a game expert, can you give a suggestion to optimize the us er's strategy so \\nthat he can improve his winning rate in this round?\\nFigure 20: SAG: Refine Prompt\\nRound Start Prompt:\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content='Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\\nNAME:Alex BALANCE:100 HEALTH POINT:8. NO_DRINK:1'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"want to participate in today's water resource auction, please provide your bid. Another game expert's prediction for the next  round of other\"),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content='players is as follows: Bob might choose 30, Cindy might choose 30, David might choose 40, Eric might choose 25.  Based on the  prediction of \\nother players, think carefully about your next round of bidding strategy to be most likely to survive.'),\n",
              " Document(metadata={'source': '/content/KR-reasoning-LLM-framework.pdf', 'page': 23}, page_content=\"Example Output:\\nGiven the game expert's prediction, I will bid $35 to increase my chances of winning the water resources for today.\\nFigure 21: SAG: K-Level Reasoning Prompt\\n24\")]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = load_document('/content/KR-reasoning-LLM-framework.pdf')\n",
        "# print(data[0].page_content)\n",
        "\n",
        "chunk_data(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOg8-wGB1mR9"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "pc_apikey = userdata.get('PINECONE_API_KEY')\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6ksfgBr2dLFc",
        "outputId": "4c4a850b-3c7b-4357-fe91-5a467f7a50d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.59.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.29)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.59.4)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (2.10.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_openai) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMvf0hAt1ryg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "np2JW0FrhvDH",
        "outputId": "f29c2f46-87e9-4e32-f13f-819a0857f815"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWhy was the math book sad?\\n\\nBecause it had too many problems.'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run the LLM\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsHajanS-r4W",
        "outputId": "d3afba11-1a8d-4a51-8c4d-5497df4e9d1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (5.4.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2024.12.14)\n",
            "Requirement already satisfied: pinecone-plugin-inference<4.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (3.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7g42MQu_pm9",
        "outputId": "bf536963-6f70-4ce6-c2f6-993b52f06686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.29)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.59.4)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (2.10.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_openai) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjWY_eYk_vec",
        "outputId": "4620af7d-c4c5-41b7-d323-a0428aa3610b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_pinecone in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: aiohttp<3.11,>=3.10 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (3.10.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (0.3.29)\n",
            "Requirement already satisfied: langchain-tests<0.4.0,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (0.3.8)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (1.26.4)\n",
            "Requirement already satisfied: pinecone<6.0.0,>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (5.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (1.18.3)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (4.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (2.10.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (0.28.1)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.10/dist-packages (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (8.3.4)\n",
            "Requirement already satisfied: pytest-asyncio<1,>=0.20 in /usr/local/lib/python3.10/dist-packages (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (0.25.2)\n",
            "Requirement already satisfied: pytest-socket<1,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: syrupy<5,>=4 in /usr/local/lib/python3.10/dist-packages (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (4.8.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (2024.12.14)\n",
            "Requirement already satisfied: pinecone-plugin-inference<4.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (3.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (2.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (3.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (2.27.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest<9,>=7->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (1.2.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest<9,>=7->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest<9,>=7->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest<9,>=7->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (2.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone<6.0.0,>=5.4.0->langchain_pinecone) (1.17.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<3.11,>=3.10->langchain_pinecone) (0.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Z94zGjbtGi",
        "outputId": "8f92dcc9-a6f0-4386-c876-f51386488bdd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(287, list)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks), type(chunks)\n",
        "# chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7ZmSlCocK26",
        "outputId": "4599eb42-0373-4de5-ccda-430fa55aca7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1:\n",
            "Tree of Thoughts: Deliberate Problem Solving\n",
            "with Large Language Models\n",
            "Shunyu Yao\n",
            "Princeton University\n",
            "Dian Yu\n",
            "Google DeepMind\n",
            "Jeffrey Zhao\n",
            "Google DeepMind\n",
            "Izhak Shafran\n",
            "Google DeepMind\n",
            "Thomas L. Griffiths\n",
            "Princeton University\n",
            "Yuan Cao\n",
            "Google DeepMind\n",
            "--------------------------------------------------\n",
            "Chunk 2:\n",
            "Karthik Narasimhan\n",
            "Princeton University\n",
            "Abstract\n",
            "Language models are increasingly being deployed for general problem solving\n",
            "across a wide range of tasks, but are still confined to token-level, left-to-right\n",
            "--------------------------------------------------\n",
            "Chunk 3:\n",
            "decision-making processes during inference. This means they can fall short in\n",
            "tasks that require exploration, strategic lookahead, or where initial decisions play\n",
            "a pivotal role. To surmount these challenges, we introduce a new framework for\n",
            "--------------------------------------------------\n",
            "Chunk 4:\n",
            "language model inference, “Tree of Thoughts” (ToT), which generalizes over the\n",
            "popular “Chain of Thought” approach to prompting language models, and enables\n",
            "exploration over coherent units of text (“thoughts”) that serve as intermediate steps\n",
            "--------------------------------------------------\n",
            "Chunk 5:\n",
            "toward problem solving. ToT allows LMs to perform deliberate decision making\n",
            "by considering multiple different reasoning paths and self-evaluating choices to\n",
            "decide the next course of action, as well as looking ahead or backtracking when\n",
            "--------------------------------------------------\n",
            "Chunk 6:\n",
            "necessary to make global choices. Our experiments show that ToT significantly\n",
            "enhances language models’ problem-solving abilities on three novel tasks requiring\n",
            "non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords.\n",
            "--------------------------------------------------\n",
            "Chunk 7:\n",
            "For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only\n",
            "solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all\n",
            "prompts: https://github.com/princeton-nlp/tree-of-thought-llm .\n",
            "1 Introduction\n",
            "--------------------------------------------------\n",
            "Chunk 8:\n",
            "Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT [25,\n",
            "26, 1, 23] and PaLM [5] have been shown to be increasingly capable of performing an ever wider\n",
            "--------------------------------------------------\n",
            "Chunk 9:\n",
            "range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is\n",
            "perhaps surprising that underlying all this progress is still the original autoregressive mechanism for\n",
            "--------------------------------------------------\n",
            "Chunk 10:\n",
            "generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such\n",
            "a simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what\n",
            "--------------------------------------------------\n",
            "Chunk 11:\n",
            "problems would challenge the current paradigm, and what should be alternative mechanisms?\n",
            "The literature on human cognition provides some clues to answer these questions. Research on “dual\n",
            "--------------------------------------------------\n",
            "Chunk 12:\n",
            "process” models suggests that people have two modes in which they engage with decisions – a fast,\n",
            "automatic, unconscious mode (“System 1”) and a slow, deliberate, conscious mode (“System 2”)\n",
            "--------------------------------------------------\n",
            "Chunk 13:\n",
            "[30, 31, 16, 15]. These two modes have previously been connected to a variety of mathematical\n",
            "models used in machine learning. For example, research on reinforcement learning in humans and\n",
            "--------------------------------------------------\n",
            "Chunk 14:\n",
            "other animals has explored the circumstances under which they engage in associative “model free”\n",
            "learning or more deliberative “model based” planning [7]. The simple associative token-level choices\n",
            "--------------------------------------------------\n",
            "Chunk 15:\n",
            "of LMs are also reminiscent of “System 1”, and thus might benefit from augmentation by a more\n",
            "deliberate “System 2” planning process that (1) maintains and explores diverse alternatives for current\n",
            "--------------------------------------------------\n",
            "Chunk 16:\n",
            "37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n",
            "arXiv:2305.10601v2  [cs.CL]  3 Dec 2023\n",
            "--------------------------------------------------\n",
            "Chunk 17:\n",
            "GĮŔũƜ\n",
            "jũƜŔũƜ\n",
            "GĮŔũƜ\n",
            "jũƜŔũƜ\n",
            "ʱÊʲˤGj ʱæʲˤ\u001dĵÉ\n",
            "GĮŔũƜ\n",
            "ˤjũƜŔũƜ\n",
            "ʱçʲˤ\u001dĵÉˁ\u001d\n",
            "ʟʟ ʟʟ\n",
            "aÊĠĵŗƓŤƆˤſĵŤò\n",
            "GĮŔũƜ\n",
            "ˤjũƜŔũƜ\n",
            "ʱíʲˤÉĵÉˤʱĵũŗŝʲ\n",
            "ʟʟ\n",
            "ʟʟ\n",
            "ʟʟ\n",
            "ˤˤʝˤƛĎĵũĈĎƜ\n",
            ")L[\u0003FRORU\u0003\u000bE\\\u0003<XTLDQ\f\n",
            "0DUN\u0003GLIIHUHQFH\u0003E\\\u0003FRORU\n",
            "GĮŔũƜ\n",
            "jũƜŔũƜ\n",
            "GĮŔũƜ\n",
            "jũƜŔũƜ\n",
            "GĮŔũƜ\n",
            "ˤjũƜŔũƜ\n",
            "ʱçʲˤòĦƙˤ\u001dĵĮŝƓŝŤòĮçƆˤ\n",
            "--------------------------------------------------\n",
            "Chunk 18:\n",
            "ƀƓƜĎˤ\u001dĵÉˤʱ\u001dĵÉˁ\u001dʲ\n",
            "aÊĠĵŗƓŤƆˤſĵŤò\n",
            "GĮŔũƜ\n",
            "ˤjũƜŔũƜ\n",
            "ʱíʲˤÉŗòòˤĵƙˤĎĵũĈĎŤŝˤʱÉĵÉʲ\n",
            "ʟʟ\n",
            "ʟʟ\n",
            "ʟʟ ʟʟ ʟʟ\n",
            "ˤˤƛĎĵũĈĎƜ\n",
            "ʱçʲˤ\u001dĎÊđĮˤĵƙˤĎĵũĈĎƜˤ\n",
            "ŗĵĭŔƜđĮĈˤʱ\u001dĵÉʲ\n",
            "ʱÊʲˤGĮŔũƜˁjũƜŔũƜˤ\n",
            "ŗĵĭŔƜđĮĈˤʱGjʲ\n",
            "--------------------------------------------------\n",
            "Chunk 19:\n",
            "Figure 1: Schematic illustrating various approaches to problem solving with LLMs. Each rectangle\n",
            "box represents a thought, which is a coherent language sequence that serves as an intermediate\n",
            "--------------------------------------------------\n",
            "Chunk 20:\n",
            "step toward problem solving. See concrete examples of how thoughts are generated, evaluated, and\n",
            "searched in Figures 2,4,6.\n",
            "choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or\n",
            "--------------------------------------------------\n",
            "Chunk 21:\n",
            "backtracks to make more global decisions.\n",
            "To design such a planning process, we return to the origins of artificial intelligence (and cognitive\n",
            "science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon\n",
            "--------------------------------------------------\n",
            "Chunk 22:\n",
            "starting in the 1950s [21, 22]. Newell and colleagues characterized problem solving [21] as search\n",
            "through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts\n",
            "--------------------------------------------------\n",
            "Chunk 23:\n",
            "(ToT) framework for general problem solving with language models. As Figure 1 illustrates, while\n",
            "existing methods (detailed below) sample continuous language sequences for problem solving, ToT\n",
            "--------------------------------------------------\n",
            "Chunk 24:\n",
            "actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves\n",
            "as an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the\n",
            "--------------------------------------------------\n",
            "Chunk 25:\n",
            "LM to self-evaluate the progress different intermediate thoughts make towards solving the problem\n",
            "through a deliberate reasoning process that is also instantiated in language (Figures 2,4,6). This\n",
            "--------------------------------------------------\n",
            "Chunk 26:\n",
            "implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous\n",
            "search heuristics are either programmed or learned. Finally, we combine this language-based\n",
            "--------------------------------------------------\n",
            "Chunk 27:\n",
            "capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first\n",
            "search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts\n",
            "with lookahead and backtracking.\n",
            "--------------------------------------------------\n",
            "Chunk 28:\n",
            "Empirically, we propose three new problems that challenge existing LM inference methods even with\n",
            "the state-of-the-art language model, GPT-4 [23]: Game of 24, Creative Writing, and Crosswords\n",
            "--------------------------------------------------\n",
            "Chunk 29:\n",
            "(Table 1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities,\n",
            "and a way to incorporate systematic planning or search. We show ToT obtains superior results on\n",
            "--------------------------------------------------\n",
            "Chunk 30:\n",
            "all three tasks by being general and flexible enough to support different levels of thoughts, different\n",
            "ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of\n",
            "--------------------------------------------------\n",
            "Chunk 31:\n",
            "different problems. We also analyze how such choices affect model performances via systematic\n",
            "ablations and discuss future directions to better train and use LMs.\n",
            "2 Background\n",
            "--------------------------------------------------\n",
            "Chunk 32:\n",
            "We first formalize some existing methods that use large language models for problem-solving,\n",
            "which our approach is inspired by and later compared with. We use pθ to denote a pre-trained LM\n",
            "--------------------------------------------------\n",
            "Chunk 33:\n",
            "with parameters θ, and lowercase letters x, y, z, s,··· to denote a language sequence , i.e. x =\n",
            "(x[1], ··· , x[n]) where each x[i] is a token, so that pθ(x) =Qn\n",
            "i=1 pθ(x[i]|x[1...i]). We use uppercase\n",
            "--------------------------------------------------\n",
            "Chunk 34:\n",
            "letters S, ··· to denote a collection of language sequences.\n",
            "Input-output (IO) prompting is the most common way to turn a problem input x into output\n",
            "y with LM: y ∼ pθ(y|promptIO (x)), where promptIO (x) wraps input x with task instructions\n",
            "--------------------------------------------------\n",
            "Chunk 35:\n",
            "and/or few-shot input-output examples. For simplicity, let us denote pprompt\n",
            "θ (output | input) =\n",
            "pθ(output | prompt(input)), so that IO prompting can be formulated as y ∼ pIO\n",
            "θ (y|x).\n",
            "2\n",
            "--------------------------------------------------\n",
            "Chunk 36:\n",
            "Chain-of-thought (CoT) prompting [38] was proposed to address cases where the mapping of\n",
            "input x to output y is non-trivial (e.g. when x is a math question and y is the final numerical answer).\n",
            "--------------------------------------------------\n",
            "Chunk 37:\n",
            "The key idea is to introduce a chain of thoughts z1, ··· , zn to bridge x and y, where each zi is a\n",
            "coherent language sequence that serves as a meaningful intermediate step toward problem solving\n",
            "--------------------------------------------------\n",
            "Chunk 38:\n",
            "(e.g. zi could be an intermediate equation for math QA). To solve problems with CoT, each thought\n",
            "zi ∼ pCoT\n",
            "θ (zi | x, z1···i−1) is sampled sequentially, then the output y ∼ pCoT\n",
            "θ (y|x, z1···n). In\n",
            "practice, [z1···n, y] ∼ pCoT\n",
            "--------------------------------------------------\n",
            "Chunk 39:\n",
            "θ (z1···n, y|x) is sampled as a continuous language sequence, and the\n",
            "decomposition of thoughts (e.g. is each zi a phrase, a sentence, or a paragraph) is left ambiguous.\n",
            "--------------------------------------------------\n",
            "Chunk 40:\n",
            "Self-consistency with CoT (CoT-SC) [36] is an ensemble approach that samples k i.i.d. chains\n",
            "of thought: [z(i)\n",
            "1···n, y(i)] ∼ pCoT\n",
            "θ (z1···n, y|x) (i = 1··· k), then returns the most frequent output:\n",
            "--------------------------------------------------\n",
            "Chunk 41:\n",
            "arg maxy #{i | y(i) = y}. CoT-SC improves upon CoT, because there are generally different\n",
            "thought processes for the same problem (e.g. different ways to prove the same theorem), and the\n",
            "--------------------------------------------------\n",
            "Chunk 42:\n",
            "output decision can be more faithful by exploring a richer set of thoughts. However, within each\n",
            "chain there is no local exploration of different thought steps, and the “most frequent” heuristic only\n",
            "--------------------------------------------------\n",
            "Chunk 43:\n",
            "applies when the output space is limited (e.g. multi-choice QA).\n",
            "3 Tree of Thoughts: Deliberate Problem Solving with LM\n",
            "A genuine problem-solving process involves the repeated use of available informa-\n",
            "--------------------------------------------------\n",
            "Chunk 44:\n",
            "tion to initiate exploration, which discloses, in turn, more information until a way\n",
            "to attain the solution is finally discovered.—— Newell et al. [21]\n",
            "Research on human problem-solving suggests that people search through a combinatorial problem-\n",
            "--------------------------------------------------\n",
            "Chunk 45:\n",
            "space – a tree where the nodes represent partial solutions, and the branches correspond to operators\n",
            "that modify them [21, 22]. Which branch to take is determined by heuristics that help to navigate the\n",
            "--------------------------------------------------\n",
            "Chunk 46:\n",
            "problem-space and guide the problem-solver towards a solution. This perspective highlights two key\n",
            "shortcomings of existing approaches that use LMs to solve general problems: 1) Locally, they do not\n",
            "--------------------------------------------------\n",
            "Chunk 47:\n",
            "explore different continuations within a thought process – the branches of the tree. 2) Globally, they\n",
            "do not incorporate any type of planning, lookahead, or backtracking to help evaluate these different\n",
            "--------------------------------------------------\n",
            "Chunk 48:\n",
            "options – the kind of heuristic-guided search that seems characteristic of human problem-solving.\n",
            "To address these shortcomings, we introduce Tree of Thoughts (ToT), a paradigm that allows LMs to\n",
            "--------------------------------------------------\n",
            "Chunk 49:\n",
            "explore multiple reasoning paths over thoughts (Figure 1(c)). ToT frames any problem as a search\n",
            "over a tree, where each node is a state s = [x, z1···i] representing a partial solution with the input and\n",
            "--------------------------------------------------\n",
            "Chunk 50:\n",
            "the sequence of thoughts so far. A specific instantiation of ToT involves answering four questions:\n",
            "1. How to decompose the intermediate process into thought steps; 2. How to generate potential\n",
            "--------------------------------------------------\n",
            "Chunk 51:\n",
            "thoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use.\n",
            "1. Thought decomposition. While CoT samples thoughts coherently without explicit decomposition,\n",
            "--------------------------------------------------\n",
            "Chunk 52:\n",
            "ToT leverages problem properties to design and decompose intermediate thought steps. As Table 1\n",
            "shows, depending on different problems, a thought could be a couple of words (Crosswords), a line of\n",
            "--------------------------------------------------\n",
            "Chunk 53:\n",
            "equation (Game of 24), or a whole paragraph of writing plan (Creative Writing). In general, a thought\n",
            "should be “small” enough so that LMs can generate promising and diverse samples (e.g. generating\n",
            "--------------------------------------------------\n",
            "Chunk 54:\n",
            "a whole book is usually too “big” to be coherent), yet “big” enough so that LMs can evaluate its\n",
            "prospect toward problem solving (e.g. generating one token is usually too “small” to evaluate).\n",
            "--------------------------------------------------\n",
            "Chunk 55:\n",
            "2. Thought generator G(pθ, s, k). Given a tree state s = [x, z1···i], we consider two strategies to\n",
            "generate k candidates for the next thought step:\n",
            "(a) Sample i.i.d. thoughts from a CoT prompt (Creative Writing, Figure 4): z(j) ∼\n",
            "pCoT\n",
            "θ (zi+1|s) = pCoT\n",
            "--------------------------------------------------\n",
            "Chunk 56:\n",
            "θ (zi+1|x, z1···i) (j = 1··· k). This works better when the thought\n",
            "space is rich (e.g. each thought is a paragraph), and i.i.d. samples lead to diversity;\n",
            "(b) Propose thoughts sequentially using a “propose prompt” (Game of 24, Figure 2; Crosswords,\n",
            "--------------------------------------------------\n",
            "Chunk 57:\n",
            "Figure 6): [z(1), ··· , z(k)] ∼ ppropose\n",
            "θ (z(1···k)\n",
            "i+1 | s). This works better when the thought\n",
            "space is more constrained (e.g. each thought is just a word or a line), so proposing different\n",
            "thoughts in the same context avoids duplication.\n",
            "--------------------------------------------------\n",
            "Chunk 58:\n",
            "3. State evaluator V (pθ, S). Given a frontier of different states, the state evaluator evaluates the\n",
            "progress they make towards solving the problem, serving as a heuristic for the search algorithm\n",
            "--------------------------------------------------\n",
            "Chunk 59:\n",
            "to determine which states to keep exploring and in which order. While heuristics are a standard\n",
            "approach to solving search problems, they are typically either programmed (e.g. DeepBlue [3]) or\n",
            "3\n",
            "--------------------------------------------------\n",
            "Chunk 60:\n",
            "learned (e.g. AlphaGo [29]). We propose a third alternative, by using the LM to deliberately reason\n",
            "about states. When applicable, such a deliberate heuristic can be more flexible than programmed\n",
            "--------------------------------------------------\n",
            "Chunk 61:\n",
            "rules, and more sample-efficient than learned models. Similar to the thought generator, we consider\n",
            "two strategies to evaluate states either independently or together:\n",
            "(a) Value each state independently: V (pθ, S)(s) ∼ pvalue\n",
            "θ (v|s) ∀s ∈ S, where a value\n",
            "--------------------------------------------------\n",
            "Chunk 62:\n",
            "prompt reasons about the state s to generate a scalar value v (e.g. 1-10) or a classifica-\n",
            "tion (e.g. sure/likely/impossible) that could be heuristically turned into a value. The basis\n",
            "--------------------------------------------------\n",
            "Chunk 63:\n",
            "of such evaluative reasoning can vary across problems and thought steps. In this work, we\n",
            "explore evaluation via few lookahead simulations (e.g. quickly confirm that 5, 5, 14 can\n",
            "--------------------------------------------------\n",
            "Chunk 64:\n",
            "reach 24 via 5 + 5 + 14, or “hot l” can mean “inn” via filling “e” in “ ”) plus commonsense\n",
            "(e.g. 1 2 3 are too small to reach 24, or no word can start with “tzxc”). While the former\n",
            "--------------------------------------------------\n",
            "Chunk 65:\n",
            "might promote “good” states, the latter could help eliminate “bad” states. Such valuations\n",
            "do not need to be perfect, and only need to be approximately helpful for decision making.\n",
            "--------------------------------------------------\n",
            "Chunk 66:\n",
            "(b) Vote across states: V (pθ, S)(s) = 1[s = s∗], where a “good” state s∗ ∼ pvote\n",
            "θ (s∗|S) is\n",
            "voted out based on deliberately comparing different states in S in a vote prompt. When\n",
            "--------------------------------------------------\n",
            "Chunk 67:\n",
            "problem success is harder to directly value (e.g. passage coherency), it is natural to to instead\n",
            "compare different partial solutions and vote for the most promising one. This is similar\n",
            "--------------------------------------------------\n",
            "Chunk 68:\n",
            "in spirit to a “step-wise” self-consistency strategy, i.e. cast “which state to explore” as a\n",
            "multi-choice QA, and use LM samples to vote for it.\n",
            "For both strategies, we could prompt the LM multiple times to aggregate the value or vote results to\n",
            "--------------------------------------------------\n",
            "Chunk 69:\n",
            "trade time/resource/cost for more faithful/robust heuristics.\n",
            "Algorithm 1 ToT-BFS(x, pθ, G, k, V, T, b)\n",
            "Require: Input x, LM pθ, thought generator G()\n",
            "& size limit k, states evaluator V (), step limit T,\n",
            "breadth limit b.\n",
            "S0 ← {x}\n",
            "for t = 1, ··· , Tdo\n",
            "S′\n",
            "--------------------------------------------------\n",
            "Chunk 70:\n",
            "t ← {[s, z] | s ∈ St−1, zt ∈ G(pθ, s, k)}\n",
            "Vt ← V (pθ, S′\n",
            "t)\n",
            "St ← arg maxS⊂S′\n",
            "t,|S|=b\n",
            "P\n",
            "s∈S Vt(s)\n",
            "end for\n",
            "return G(pθ, arg maxs∈ST VT (s), 1)\n",
            "Algorithm 2 ToT-DFS(s, t, pθ, G, k, V, T, vth)\n",
            "Require: Current state s, step t, LM pθ, thought\n",
            "--------------------------------------------------\n",
            "Chunk 71:\n",
            "generator G() and size limit k, states evaluator\n",
            "V (), step limit T, threshold vth\n",
            "if t > Tthen record output G(pθ, s,1)\n",
            "end if\n",
            "for s′ ∈ G(pθ, s, k) do ▷ sorted candidates\n",
            "if V (pθ, {s′})(s) > vthres then ▷ pruning\n",
            "DFS(s′, t+ 1)\n",
            "end if\n",
            "end for\n",
            "--------------------------------------------------\n",
            "Chunk 72:\n",
            "4. Search algorithm. Finally, within the ToT framework, one can plug and play different search\n",
            "algorithms depending on the tree structure. We explore two relatively simple search algorithms and\n",
            "--------------------------------------------------\n",
            "Chunk 73:\n",
            "leave more advanced ones (e.g. A* [11], MCTS [2]) for future work:\n",
            "(a) Breadth-first search (BFS) (Algorithm 1) maintains a set of the b most promising states\n",
            "per step. This is used for Game of 24 and Creative Writing where the tree depth is limit\n",
            "--------------------------------------------------\n",
            "Chunk 74:\n",
            "(T ≤ 3), and initial thought steps can be evaluated and pruned to a small set (b ≤ 5).\n",
            "(b) Depth-first search (DFS) (Algorithm 2) explores the most promising state first, until the\n",
            "--------------------------------------------------\n",
            "Chunk 75:\n",
            "final output is reached ( t > T), or the state evaluator deems it impossible to solve the\n",
            "problem from the current s (V (pθ, {s})(s) ≤ vth for a value threshold vth). In the latter\n",
            "--------------------------------------------------\n",
            "Chunk 76:\n",
            "case, the subtree from s is pruned to trade exploration for exploitation. In both cases, DFS\n",
            "backtracks to the parent state of s to continue exploration.\n",
            "Conceptually, ToT has several benefits as a method for general problem-solving with LMs: (1)Gener-\n",
            "--------------------------------------------------\n",
            "Chunk 77:\n",
            "ality. IO, CoT, CoT-SC, and self-refinement can be seen as special cases of ToT (i.e. trees of limited\n",
            "depth and breadth; Figure 1). (2) Modularity. The base LM, as well as the thought decomposition,\n",
            "--------------------------------------------------\n",
            "Chunk 78:\n",
            "generation, evaluation, and search procedures can all be varied independently. (3) Adaptability.\n",
            "Different problem properties, LM capabilities, and resource constraints can be accommodated. (4)\n",
            "--------------------------------------------------\n",
            "Chunk 79:\n",
            "Convenience. No extra training is needed, just a pre-trained LM is sufficient. The next section will\n",
            "show how these conceptual benefits translate to strong empirical performance in different problems.\n",
            "4 Experiments\n",
            "--------------------------------------------------\n",
            "Chunk 80:\n",
            "We propose three tasks that are hard even when sampling from the state-of-the-art language model,\n",
            "GPT-4 [23], using standard IO prompting or chain-of-thought (CoT) prompting. We show how\n",
            "4\n",
            "--------------------------------------------------\n",
            "Chunk 81:\n",
            "Game of 24 Creative Writing 5x5 Crosswords\n",
            "Input 4 numbers (4 9 10 13) 4 random sentences 10 clues (h1. presented;..)\n",
            "Output An equation to reach 24\n",
            "(13-9)*(10-4)=24\n",
            "A passage of 4 paragraphs\n",
            "ending in the 4 sentences\n",
            "5x5 letters: SHOWN;\n",
            "--------------------------------------------------\n",
            "Chunk 82:\n",
            "WIRRA; A V AIL; ...\n",
            "Thoughts 3 intermediate equations\n",
            "(13-9=4 (left 4,4,10); 10-\n",
            "4=6 (left 4,6); 4*6=24)\n",
            "A short writing plan\n",
            "(1. Introduce a book that\n",
            "connects...)\n",
            "Words to fill in for clues:\n",
            "(h1. shown; v5. naled; ...)\n",
            "#ToT steps 3 1 5-10 (variable)\n",
            "--------------------------------------------------\n",
            "Chunk 83:\n",
            "Table 1: Task overview. Input, output, thought examples are in blue.\n",
            "deliberate search in trees of thoughts (ToT) produces better results, and more importantly, interesting\n",
            "--------------------------------------------------\n",
            "Chunk 84:\n",
            "and promising new ways to use language models to solve problems requiring search or planning.\n",
            "Unless otherwise stated, we perform experiments using a Chat Completion mode GPT-4 1 with a\n",
            "sampling temperature of 0.7.\n",
            "4.1 Game of 24\n",
            "--------------------------------------------------\n",
            "Chunk 85:\n",
            "Game of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and basic\n",
            "arithmetic operations (+-*/) to obtain 24. For example, given input “4 9 10 13”, a solution output\n",
            "could be “(10 - 4) * (13 - 9) = 24”.\n",
            "ʳĵĮòˤòƅÊĭŔĦòʴˤ\n",
            "--------------------------------------------------\n",
            "Chunk 86:\n",
            "GĮŔũƜʝˤʁˤʆˤɾɽˤɾʀ\n",
            "ĵŝŝđæĦòˤĮòƅƜˤŝŤòŔŝʝˤˤˤ\n",
            "3URSRVH\u00033URPSW\n",
            "ʁˤ̌ˤʆˤ̐ˤɾʀˤʱĦòƙƜʝˤɾɽˤɾʀˤɾʀʲ\n",
            "ɾɽˤˊˤʁˤ̐ˤʃˤʱĦòƙƜʝˤʃˤʆˤɾʀʲ\n",
            "ʳʛʛʛĭĵŗòˤĦđĮòŝʟʴ\n",
            "7KRXJKW\u0003*HQHUDWLRQ\n",
            "/0\n",
            ")ſÊĦũÊŤòˤđƙˤĈƓſòĮˤĮũĭæòŗŝˤçÊĮˤ\n",
            "ŗòÊçĎˤɿʁˤʱŝũŗòʫĦđģòĦƆʫđĭŔĵŝŝđæĦòʲ\n",
            "ɾɽˤɾʁʝˤɾɽˤ̌ˤɾʁˤ̐ˤɿʁʛˤŝũŗò\n",
            "ʳĭĵŗòˤòƅÊĭŔĦòŝʴ\n",
            "--------------------------------------------------\n",
            "Chunk 87:\n",
            "ɾɽˤɾʀˤɾʀ\n",
            "9DOXH\u00033URPSW\n",
            "ʱɾʀˤˊˤɾɽʲˤʦˤɾʀˤ̐ˤʀˤʦˤɾʀˤ̐ˤʀʆ\n",
            "ɾɽˤ̌ˤɾʀˤ̌ˤɾʀˤ̐ˤʀʃ\n",
            "ĎòŗòˤƓŝˤĮĵˤƀÊƆˤƘĵˤĵæŤÊđĮˤɿʁˤƀƓƜĎˤ\n",
            "ƛĎòŝòˤĮũĭæòŗŝʛˤđĭŔĵŝŝđæĦò\n",
            "7KRXJKW\u0003(YDOXDWLRQ\n",
            "ʱæʲ\n",
            "ʱçʲ\n",
            "/0\n",
            "GĮŔũƜʝˤʁˤʆˤɾɽˤɾʀ\n",
            "ʁ̌ʆ̐ɾʀʱĦòƙƜʝˤɾɽˤɾʀˤɾʀʲ\n",
            "ɾɽˁʁ̐ʃʱĦòƙƜʝˤʃˤʆˤɾʀʲ\n",
            "ʟʟ\n",
            "ɾʀˁʃ̐ʄʱĦòƙƜʝˤʄˤʆʲ\n",
            "--------------------------------------------------\n",
            "Chunk 88:\n",
            "ɾʀˁʆ̐ʁʱĦòƙƜʝˤʁˤʃʲ\n",
            "ʟʟ\n",
            "ʁʦʃ̐ɿʁʱĦòƙƜʝˤɿʁʲ\n",
            "ʁ̌ʃ̐ɾɽʱĦòƙƜʝˤɾɽʲ ʟʟ\n",
            "ʱÊʲ\n",
            "ʳĵĮòˤòƅÊĭŔĦòʴˤ\n",
            "GĮŔũƜʝˤʁˤʆˤɾɽˤɾʀ\n",
            "ĵŝŝđæĦòˤĮòƅƜˤŝŤòŔŝʝˤˤˤ\n",
            "\u000bD\f\u00033URSRVH\u00033URPSW\n",
            "ʁˤ̌ˤʆˤ̐ˤɾʀˤʱĦòƙƜʝˤɾɽˤɾʀˤɾʀʲ\n",
            "ɾɽˤˊˤʁˤ̐ˤʃˤʱĦòƙƜʝˤʃˤʆˤɾʀʲ\n",
            "ʳʛʛʛĭĵŗòˤĦđĮòŝʟʴ\n",
            "7KRXJKW\u0003*HQHUDWLRQ\n",
            "/0\n",
            "--------------------------------------------------\n",
            "Chunk 89:\n",
            ")ſÊĦũÊŤòˤđƙˤĈƓſòĮˤĮũĭæòŗŝˤçÊĮˤ\n",
            "ŗòÊçĎˤɿʁˤʱŝũŗòʫĦđģòĦƆʫđĭŔĵŝŝđæĦòʲ\n",
            "ɾɽˤɾʁʝˤɾɽˤ̌ˤɾʁˤ̐ˤɿʁʛˤŝũŗò\n",
            "ʳĭĵŗòˤòƅÊĭŔĦòŝʴ\n",
            "ɾɽˤɾʀˤɾʀ\n",
            "\u000bE\f\u00039DOXH\u00033URPSW\n",
            "ʱɾʀˤˊˤɾɽʲˤʦˤɾʀˤ̐ˤʀˤʦˤɾʀˤ̐ˤʀʆ\n",
            "ɾɽˤ̌ˤɾʀˤ̌ˤɾʀˤ̐ˤʀʃˤĎòŗòˤƓŝˤĮĵˤƀÊƆˤ\n",
            "ƘĵˤĵæŤÊđĮˤɿʁˤƀƓƜĎˤƛĎòŝòˤæƓĈˤ\n",
            "ĮũĭæòŗŝʛˤđĭŔĵŝŝđæĦò\n",
            "--------------------------------------------------\n",
            "Chunk 90:\n",
            "7KRXJKW\u0003(YDOXDWLRQ\n",
            "/0\n",
            "GĮŔũƜʝˤʁˤʆˤɾɽˤɾʀ\n",
            "ʁ̌ʆ̐ɾʀʱĦòƙƜʝˤɾɽˤɾʀˤɾʀʲ\n",
            "ɾɽˁʁ̐ʃʱĦòƙƜʝˤʃˤʆˤɾʀʲ\n",
            "ʟʟ\n",
            "ɾʀˁʃ̐ʄʱĦòƙƜʝˤʄˤʆʲ\n",
            "ɾʀˁʆ̐ʁʱĦòƙƜʝˤʁˤʃʲ\n",
            "ʟʟ\n",
            "ʁʦʃ̐ɿʁʱĦòƙƜʝˤɿʁʲ\n",
            "ʁ̌ʃ̐ɾɽʱĦòƙƜʝˤɾɽʲ ʟʟ\n",
            ")L[\u0003FRORU\u0003\u000bE\\\u0003<XTLDQ\f\n",
            "0DUN\u0003GLII\u0003SURPSW\u0003ZLWK\u0003FRORU\n",
            "--------------------------------------------------\n",
            "Chunk 91:\n",
            "Figure 2: ToT in a game of 24. The LM is prompted for (a) thought generation and (b) valuation.\n",
            "Task Setup. We scrape data from 4nums.com, which has 1,362 games that are sorted from easy to\n",
            "--------------------------------------------------\n",
            "Chunk 92:\n",
            "hard by human solving time, and use a subset of relatively hard games indexed 901-1,000 for testing.\n",
            "For each task, we consider the output as success if it is a valid equation that equals 24 and uses the\n",
            "--------------------------------------------------\n",
            "Chunk 93:\n",
            "input numbers each exactly once. We report the success rate across 100 games as the metric.\n",
            "Baselines. We use a standard input-output (IO) prompt with 5 in-context examples. For chain-of-\n",
            "--------------------------------------------------\n",
            "Chunk 94:\n",
            "thought (CoT) prompting, we augment each input-output pair with 3 intermediate equations, each\n",
            "operating on two remaining numbers. For example, given input “4 9 10 13”, the thoughts could be\n",
            "--------------------------------------------------\n",
            "Chunk 95:\n",
            "“13 - 9 = 4 (left: 4 4 10); 10 - 4 = 6 (left: 4 6); 4 * 6 = 24 (left: 24)”. For each game, we sample IO\n",
            "and CoT prompting for 100 times for average performance. We also consider a CoT self-consistency\n",
            "--------------------------------------------------\n",
            "Chunk 96:\n",
            "baseline, which takes the majority output from 100 CoT samples, and an iterative-refine approach on\n",
            "top of an IO sample for at most 10 iterations. At each iteration, the LM is conditioned on all previous\n",
            "--------------------------------------------------\n",
            "Chunk 97:\n",
            "history to “reflect on your mistakes and generate a refined answer” if the output is incorrect. Note\n",
            "that it uses groundtruth feedback signals about equation correctness.\n",
            "--------------------------------------------------\n",
            "Chunk 98:\n",
            "ToT Setup. To frame Game of 24 into ToT, it is natural to decompose the thoughts into 3 steps,\n",
            "each an intermediate equation. As shown in Figure 2(a), at each tree node, we exact the remaining\n",
            "--------------------------------------------------\n",
            "Chunk 99:\n",
            "numbers and prompt the LM to propose some possible next steps. The same “propose prompt” is\n",
            "used for all 3 thought steps, though it only has one example with 4 input numbers. We perform a\n",
            "--------------------------------------------------\n",
            "Chunk 100:\n",
            "breadth-first search (BFS) in ToT, where at each step we keep the bestb = 5candidates. To perform\n",
            "deliberate BFS in ToT, as shown in Figure 2(b), we prompt LM to evaluate each thought candidate as\n",
            "--------------------------------------------------\n",
            "Chunk 101:\n",
            "“sure/maybe/impossible” with regard to reaching 24. The aim is to promote correct partial solutions\n",
            "that can be verdicted within few lookahead trials, and eliminate impossible partial solutions based on\n",
            "--------------------------------------------------\n",
            "Chunk 102:\n",
            "“too big/small” commonsense, and keep the rest “maybe”. We sample values3 times for each thought.\n",
            "1Experiments were done between May 5-16, 2023.\n",
            "5\n",
            "--------------------------------------------------\n",
            "Chunk 103:\n",
            "Method Success\n",
            "IO prompt 7.3%\n",
            "CoT prompt 4.0%\n",
            "CoT-SC (k=100) 9.0%\n",
            "ToT (ours) (b=1) 45%\n",
            "ToT (ours) (b=5) 74%\n",
            "IO + Refine (k=10) 27%\n",
            "IO (best of 100) 33%\n",
            "CoT (best of 100) 49%\n",
            "Table 2: Game of 24 Results.\n",
            "0 25 50 75 100\n",
            "0.2\n",
            "0.4\n",
            "0.6\n",
            "--------------------------------------------------\n",
            "Chunk 104:\n",
            "(a) Success rate with nodes visited\n",
            "IO (best of k)\n",
            "CoT (best of k)\n",
            "ToT (b=1...5)\n",
            "1 2 3 4 Correct\n",
            "0.0\n",
            "0.2\n",
            "0.4\n",
            "0.6\n",
            "(b) Samples failed at each step\n",
            "CoT\n",
            "ToT (b=5) Figure 3: Game of 24 (a) scale analysis & (b) error analysis.\n",
            "--------------------------------------------------\n",
            "Chunk 105:\n",
            "Results. As shown in Table 2, IO, CoT, and CoT-SC prompting methods perform badly on the task,\n",
            "achieving only 7.3%, 4.0%, and 9.0% success rates. In contrast, ToT with a breadth of b = 1already\n",
            "--------------------------------------------------\n",
            "Chunk 106:\n",
            "achieves a success rate of 45%, while b = 5achieves 74%. We also consider an oracle setup for\n",
            "IO/CoT, by calculating the success rate using best of k samples (1 ≤ k ≤ 100). To compare IO/CoT\n",
            "--------------------------------------------------\n",
            "Chunk 107:\n",
            "(best of k) with ToT, we consider calculating the tree nodes visited per task in ToT acrossb = 1··· 5,\n",
            "and map the 5 success rates in Figure 3(a), treating IO/CoT (best of k) as visiting k nodes in a bandit.\n",
            "--------------------------------------------------\n",
            "Chunk 108:\n",
            "Not surprisingly, CoT scales better than IO, and best of 100 CoT samples achieve a success rate of\n",
            "49%, but still much worse than exploring more nodes in ToT (b >1).\n",
            "--------------------------------------------------\n",
            "Chunk 109:\n",
            "Error analysis. Figure 3(b) breaks down at which step CoT and ToT samples fail the task, i.e. the\n",
            "thought (in CoT) or all b thoughts (in ToT) are invalid or impossible to reach 24. Notably, around\n",
            "--------------------------------------------------\n",
            "Chunk 110:\n",
            "60% of CoT samples already failed the task after generating the first step, or equivalently, the first\n",
            "three words (e.g. “4 + 9”). This highlights the issues with direct left-to-right decoding.\n",
            "4.2 Creative writing\n",
            "--------------------------------------------------\n",
            "Chunk 111:\n",
            "Next, we invent a creative writing task where the input is 4 random sentences and the output should\n",
            "be a coherent passage with 4 paragraphs that end in the 4 input sentences respectively. Such a task is\n",
            "--------------------------------------------------\n",
            "Chunk 112:\n",
            "open-ended and exploratory, and challenges creative thinking as well as high-level planning.\n",
            "Task setup. We sample random sentences from randomwordgenerator.com to form 100 inputs, and\n",
            "--------------------------------------------------\n",
            "Chunk 113:\n",
            "there is no groundtruth passage for each input constraint. As we find that GPT-4 can follow the\n",
            "input constraints most of the time, we focus on evaluating passage coherency in two ways: using a\n",
            "--------------------------------------------------\n",
            "Chunk 114:\n",
            "GPT-4 zero-shot prompt to provide a 1-10 scalar score, or using human judgments to compare pairs\n",
            "of outputs from different methods. For the former, we sample 5 scores and average them for each task\n",
            "--------------------------------------------------\n",
            "Chunk 115:\n",
            "output, and we find these 5 scores usually consistent, with a standard deviation of around 0.56 on\n",
            "average across outputs. For the latter, we employ a subset of the authors in a blind study to compare\n",
            "--------------------------------------------------\n",
            "Chunk 116:\n",
            "the coherency of CoT vs. ToT generated passage pairs, where the order of passages is random flipped\n",
            "over 100 inputs.\n",
            "Baselines. Given the creative nature of the task, both IO and CoT prompts are zero-shot. While the\n",
            "--------------------------------------------------\n",
            "Chunk 117:\n",
            "former prompts the LM to directly generate a coherent passage given input constraints, the latter\n",
            "prompts the LM to first make a brief plan then write the passage, i.e. the plan serves as the intermediate\n",
            "--------------------------------------------------\n",
            "Chunk 118:\n",
            "thought step. We generate 10 IO and CoT samples per task. We also consider an iterative-refine\n",
            "(k ≤ 5) method on top of a random IO sample for each task, where the LM is conditioned on input\n",
            "--------------------------------------------------\n",
            "Chunk 119:\n",
            "constraints and the last generated passage to decide if the passage is already “perfectly coherent”,\n",
            "and if not generate a refined one.\n",
            "ToT setup. We build a ToT with depth 2 (and only 1 intermediate thought step) — the LM first\n",
            "--------------------------------------------------\n",
            "Chunk 120:\n",
            "generates k = 5plans and votes for the best one (Figure 4), then similarly generate k = 5passages\n",
            "based on the best plan then vote for the best one. Here the breadth limit b = 1, as only one choice is\n",
            "--------------------------------------------------\n",
            "Chunk 121:\n",
            "kept per step. A simple zero-shot vote prompt (“analyze choices below, then conclude which is most\n",
            "promising for the instruction”) is used to sample 5 votes at both steps.\n",
            "--------------------------------------------------\n",
            "Chunk 122:\n",
            "Results. Figure 5(a) shows average GPT-4 scores across 100 tasks, where ToT (7.56) is deemed to\n",
            "generate more coherent passages than IO (6.19) and CoT (6.93) on average. While such an automatic\n",
            "--------------------------------------------------\n",
            "Chunk 123:\n",
            "metric might be noisy, Figure 5(b) confirms the finding by showing that humans prefer ToT over\n",
            "CoT in 41 out of 100 passage pairs, while only prefer CoT over ToT in 21 (other 38 pairs are found\n",
            "--------------------------------------------------\n",
            "Chunk 124:\n",
            "“similarly coherent”). Lastly, iterative-refine is more effective on this natural language task, where\n",
            "6\n",
            "--------------------------------------------------\n",
            "Chunk 125:\n",
            "µŗƓŤòˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòˤĵƙˤʁˤŝĎĵŗƜˤŔÊŗÊĈŗÊŔĎŝʛˤĎòˤòĮíˤŝòĮŤòĮçòˤĵƙˤòÊçĎˤŔÊŗÊĈŗÊŔĎˤĭũŝƜˤæòʝˤɾʛˤGƜˤƓŝĮ˙ƛˤ\n",
            "íđƙƙƓçũĦƜˤƘĵˤíĵˤÊˤĎÊĮíŝŤÊĮíˤđƙˤƆĵũˤĠũŝƜˤŝŤÊĮíˤĵĮˤƆĵũŗˤĎÊĮíŝʛˤɿʛˤGƜˤçÊũĈĎƜˤĎđĭˤĵƙƙˤĈũÊŗíˤƛĎÊƜˤŝŔÊçòˤŝĭòĦĦòíˤĵƙˤ\n",
            "--------------------------------------------------\n",
            "Chunk 126:\n",
            "ŝòÊŗòíˤŝŤòÊģʛˤʀʛˤµĎòĮˤŝĎòˤíƓíĮ˒ƛˤĦđģòˤÊˤĈũƆˤƀĎĵˤƀÊŝˤƛŗƆđĮĈˤƘĵˤŔƓçģˤĎòŗˤũŔʜˤŝĎòˤŝŤÊŗŤòíˤũŝđĮĈˤŝƓĈĮˤĦÊĮĈũÊĈòʛˤʁʛˤ\n",
            ")ÊçĎˤŔòŗŝĵĮˤƀĎĵˤģĮĵƀŝˤƆĵũˤĎÊŝˤÊˤíđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮˤĵƙˤƀĎĵˤƆĵũˤÊŗòʛˤˤ\n",
            "ɾʛˤGĮƜŗĵíũçòˤÊĮíˤòƅŔĦÊđĮˤƛĎòˤƘòçĎĮƓŖũòˤĵƙˤ\n",
            "--------------------------------------------------\n",
            "Chunk 127:\n",
            "íĵđĮĈˤÊˤĎÊĮíŝŤÊĮíˤɿʛˤƀƓŤçĎˤƘĵˤÊˤŝŤĵŗƆˤÊæĵũƜˤ\n",
            "ÊĮˤÊŝƜŗĵĮÊũƜ˙ŝˤƚđŗŝƜˤƛđĭòˤđĮˤŝŔÊçòˤʀʛˤ#òŝçŗđæòˤ\n",
            "ÊˤŝƓƜũÊƜƓĵĮˤƀĎòŗòˤÊˤƀĵĭÊĮˤũŝòŝˤŝƓĈĮˤ\n",
            "ĦÊĮĈũÊĈòˤƘĵˤÊſĵƓíˤũĮƀÊĮŤòíˤÊƜŤòĮƜƓĵĮˤʁʛˤĎòˤ\n",
            "ƚđĮÊĦˤŔÊŗÊĈŗÊŔĎˤòƅŔĦÊđĮŝˤĎĵƀˤòſòŗƆĵĮòˤĎÊŝˤ\n",
            "íđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮŝˤĵƙˤĵƜĎòŗŝ\n",
            "--------------------------------------------------\n",
            "Chunk 128:\n",
            "ɾʛˤGĮƜŗĵíũçƜƓĵĮˤƘĵˤÊĮˤũĮũŝũÊĦˤŝòĦƙˊĎòĦŔˤæĵĵģʜˤ\n",
            "ĭòĮƜƓĵĮđĮĈˤÊˤĎÊĮíŝŤÊĮíˤÊŝˤÊˤĭòŤÊŔĎĵŗˤƗĵŗˤòĭæŗÊçđĮĈˤ\n",
            "çĎÊĦĦòĮĈòŝʛˤɿʛˤ#ƓŝçũŝŝˤƛĎòˤũĮòƅŔòçŤòíˤƛĎđĮĈŝˤĦòÊŗĮòíˤ\n",
            "ƚŗĵĭˤÊŝƜŗĵĮÊũŤŝʜˤđĮçĦũíđĮĈˤƛĎòˤŝĭòĦĦˤĵƙˤŝŔÊçòʛˤʀʛˤ\n",
            "--------------------------------------------------\n",
            "Chunk 129:\n",
            "#òŝçŗđæòˤÊˤƀĵĭÊĮ˙ŝˤçĦòſòŗˤƘÊçƜƓçˤƗĵŗˤÊſĵƓíđĮĈˤũĮƀÊĮŤòíˤ\n",
            "ÊƜŤòĮƜƓĵĮˤÊƜˤÊˤæÊŗʛˤʁʛˤ\u001dĵĮŤòĭŔĦÊŤòˤĎĵƀˤíđƙćòŗòĮƜˤ\n",
            "ŔòŗçòŔƜƓĵĮŝˤĵƙˤĵĮòŝòĦƙˤçÊĮˤŝĎÊŔòˤĵĮò˙ŝˤƓíòĮƜƓŤƆʛ\n",
            "ʱÊʲˤ\n",
            "GĮŔũƜ\n",
            "ʱæʲˤ\n",
            "ĦÊĮŝʱʀˤĭĵŗòˤ\n",
            "ĵĭƓƜŤòíʲ\n",
            "ʱçʲˤ\n",
            "´ĵŤòŝ\n",
            "--------------------------------------------------\n",
            "Chunk 130:\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝʟˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤ\n",
            "--------------------------------------------------\n",
            "Chunk 131:\n",
            "òĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "ʱɽʫʂˤſĵŤòŝʲ\n",
            "--------------------------------------------------\n",
            "Chunk 132:\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝʟˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤ\n",
            "--------------------------------------------------\n",
            "Chunk 133:\n",
            "òĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "--------------------------------------------------\n",
            "Chunk 134:\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝʟˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤ\n",
            "--------------------------------------------------\n",
            "Chunk 135:\n",
            "òĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "--------------------------------------------------\n",
            "Chunk 136:\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝʟˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤŗòŖũđŗòíˤ\n",
            "--------------------------------------------------\n",
            "Chunk 137:\n",
            "òĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "--------------------------------------------------\n",
            "Chunk 138:\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤ\n",
            "--------------------------------------------------\n",
            "Chunk 139:\n",
            "ŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "ʱʀʫʂˤſĵŤòŝʲ\n",
            "ʟ\n",
            "ɾ ɿ\n",
            "GĮŔũƜ\n",
            "ĦÊĮˤɾĦÊĮˤɿʟʟ\n",
            "--------------------------------------------------\n",
            "Chunk 140:\n",
            "ÊŝŝÊĈò\n",
            "ɾ\n",
            "ÊŝŝÊĈò\n",
            "ɿ ʟʟ\n",
            ")L[\u0003FRORU\u0003\u000bE\\\u0003<XTLDQ\f\n",
            "µŗƓŤòˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòˤĵƙˤʁˤŝĎĵŗƜˤŔÊŗÊĈŗÊŔĎŝʛˤĎòˤòĮíˤŝòĮŤòĮçòˤĵƙˤòÊçĎˤŔÊŗÊĈŗÊŔĎˤĭũŝƜˤæòʝˤɾʛˤGƜˤƓŝĮ˙ƛˤ\n",
            "--------------------------------------------------\n",
            "Chunk 141:\n",
            "íđƙƙƓçũĦƜˤƘĵˤíĵˤÊˤĎÊĮíŝŤÊĮíˤđƙˤƆĵũˤĠũŝƜˤŝŤÊĮíˤĵĮˤƆĵũŗˤĎÊĮíŝʛˤɿʛˤGƜˤçÊũĈĎƜˤĎđĭˤĵƙƙˤĈũÊŗíˤƛĎÊƜˤŝŔÊçòˤŝĭòĦĦòíˤĵƙˤ\n",
            "ŝòÊŗòíˤŝŤòÊģʛˤʀʛˤµĎòĮˤŝĎòˤíƓíĮ˒ƛˤĦđģòˤÊˤĈũƆˤƀĎĵˤƀÊŝˤƛŗƆđĮĈˤƘĵˤŔƓçģˤĎòŗˤũŔʜˤŝĎòˤŝŤÊŗŤòíˤũŝđĮĈˤŝƓĈĮˤĦÊĮĈũÊĈòʛˤʁʛˤ\n",
            "--------------------------------------------------\n",
            "Chunk 142:\n",
            ")ÊçĎˤŔòŗŝĵĮˤƀĎĵˤģĮĵƀŝˤƆĵũˤĎÊŝˤÊˤíđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮˤĵƙˤƀĎĵˤƆĵũˤÊŗòʛˤˤ\n",
            "ɾʛˤGĮƜŗĵíũçòˤÊĮíˤòƅŔĦÊđĮˤƛĎòˤƘòçĎĮƓŖũòˤ\n",
            "ĵƙˤíĵđĮĈˤÊˤĎÊĮíŝŤÊĮíˤɿʛˤƀƓŤçĎˤƘĵˤÊˤ\n",
            "ŝŤĵŗƆˤÊæĵũƜˤÊĮˤÊŝƜŗĵĮÊũƜ˙ŝˤƚđŗŝƜˤƛđĭòˤđĮˤ\n",
            "ŝŔÊçòˤʀʛˤ#òŝçŗđæòˤÊˤŝƓƜũÊƜƓĵĮˤƀĎòŗòˤÊˤ\n",
            "--------------------------------------------------\n",
            "Chunk 143:\n",
            "ƀĵĭÊĮˤũŝòŝˤŝƓĈĮˤĦÊĮĈũÊĈòˤƘĵˤÊſĵƓíˤ\n",
            "ũĮƀÊĮŤòíˤÊƜŤòĮƜƓĵĮˤʁʛˤĎòˤƚđĮÊĦˤ\n",
            "ŔÊŗÊĈŗÊŔĎˤòƅŔĦÊđĮŝˤĎĵƀˤòſòŗƆĵĮòˤĎÊŝˤ\n",
            "íđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮŝˤĵƙˤĵƜĎòŗŝ\n",
            "ɾʛˤGĮƜŗĵíũçƜƓĵĮˤƘĵˤÊĮˤũĮũŝũÊĦˤŝòĦƙˊĎòĦŔˤæĵĵģʜˤ\n",
            "ĭòĮƜƓĵĮđĮĈˤÊˤĎÊĮíŝŤÊĮíˤÊŝˤÊˤĭòŤÊŔĎĵŗˤƗĵŗˤ\n",
            "--------------------------------------------------\n",
            "Chunk 144:\n",
            "òĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʛˤɿʛˤ#ƓŝçũŝŝˤƛĎòˤũĮòƅŔòçŤòíˤ\n",
            "ƛĎđĮĈŝˤĦòÊŗĮòíˤƚŗĵĭˤÊŝƜŗĵĮÊũŤŝʜˤđĮçĦũíđĮĈˤƛĎòˤŝĭòĦĦˤĵƙˤ\n",
            "ŝŔÊçòʛˤʀʛˤ#òŝçŗđæòˤÊˤƀĵĭÊĮ˙ŝˤçĦòſòŗˤƘÊçƜƓçˤƗĵŗˤÊſĵƓíđĮĈˤ\n",
            "ũĮƀÊĮŤòíˤÊƜŤòĮƜƓĵĮˤÊƜˤÊˤæÊŗʛˤʁʛˤ\u001dĵĮŤòĭŔĦÊŤòˤĎĵƀˤ\n",
            "--------------------------------------------------\n",
            "Chunk 145:\n",
            "íđƙćòŗòĮƜˤŔòŗçòŔƜƓĵĮŝˤĵƙˤĵĮòŝòĦƙˤçÊĮˤŝĎÊŔòˤĵĮò˙ŝˤ\n",
            "ƓíòĮƜƓŤƆʛ\n",
            "ʱÊʲˤ\n",
            "GĮŔũƜ\n",
            "ʱæʲˤ\n",
            "ĦÊĮŝ\n",
            "ʱçʲˤ\n",
            "´ĵŤòŝ\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "--------------------------------------------------\n",
            "Chunk 146:\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤ\n",
            "ŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "--------------------------------------------------\n",
            "Chunk 147:\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "GĮŔũƜ\n",
            "ĦÊĮˤɾˤĦÊĮˤɿˤˤʟʟ\n",
            "ÊŝŝÊĈò\n",
            "ɾ\n",
            "ÊŝŝÊĈò\n",
            "ɿ ʟʟ\n",
            "ɽʫʂˤſĵŤòŝ\n",
            "ĦÊĮˤɾˤˤˤ\n",
            "ʟʛʟʛɾʟʛ\n",
            "ɿʟ\n",
            "ʟ\n",
            "ʀʫʂˤſĵŤòŝ\n",
            "ĦÊĮˤʀˁʂˤˤˤ\n",
            "8VH\u0003UHG\u0012JUHHQ\u0003WR\u0003VKRZ\u0003ILQDO\u0003FKRLFH\n",
            "ĮʫʂˤſĵŤòŝ\n",
            "--------------------------------------------------\n",
            "Chunk 148:\n",
            "ĦÊĮˤɿˤˤˤ\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤ\n",
            "--------------------------------------------------\n",
            "Chunk 149:\n",
            "ŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "--------------------------------------------------\n",
            "Chunk 150:\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤ\n",
            "--------------------------------------------------\n",
            "Chunk 151:\n",
            "ŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "--------------------------------------------------\n",
            "Chunk 152:\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤ\n",
            "--------------------------------------------------\n",
            "Chunk 153:\n",
            "ŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "--------------------------------------------------\n",
            "Chunk 154:\n",
            "\u0001ĮÊĦƆƏđĮĈˤòÊçĎˤçĎĵƓçòˤđĮˤíòŤÊƓĦʝˤˤ\u001dĎĵƓçòˤɾʜˤƀĎƓĦòˤđĮçĵŗŔĵŗÊƜđĮĈˤƛĎòˤŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝʜˤŝòòĭŝˤƘĵˤĦÊçģˤÊˤ\n",
            "çĦòÊŗˤçĵĮĮòçƜƓĵĮˤæòŤƀòòĮˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤʳʛʛʛʴˤ\u001dĎĵƓçòˤɿˤĵƙćòŗŝˤÊĮˤđĮŤòŗòŝƜđĮĈˤŔòŗŝŔòçƜƓſòˤæƆˤũŝđĮĈˤƛĎòˤ\n",
            "--------------------------------------------------\n",
            "Chunk 155:\n",
            "ŗòŖũđŗòíˤòĮíˤŝòĮŤòĮçòŝˤƘĵˤŔŗòŝòĮƜˤÊˤŝòĦƙˊĎòĦŔˤæĵĵģ˙ŝˤçĵĮŤòĮƜʛˤGƜˤçĵĮĮòçŤŝˤƛĎòˤŔÊŗÊĈŗÊŔĎŝˤƀƓƜĎˤƛĎòˤƛĎòĭòˤĵƙˤ\n",
            "ŝòĦƙˊđĭŔŗĵſòĭòĮƜˤÊĮíˤòĭæŗÊçđĮĈˤçĎÊĦĦòĮĈòŝʜˤĭÊģđĮĈˤƗĵŗˤÊˤçĵĎòŗòĮƜˤŔÊŝŝÊĈòʛˤˤʳʛʛʛʴˤĎòˤæòŝƜˤçĎĵƓçòˤƓŝˤɿʛ\n",
            "--------------------------------------------------\n",
            "Chunk 156:\n",
            "Figure 4: A step of deliberate search in a randomly picked Creative Writing task. Given the input, the\n",
            "LM samples 5 different plans, then votes 5 times to decide which plan is best. The majority choice is\n",
            "--------------------------------------------------\n",
            "Chunk 157:\n",
            "used to consequently write the output passage with the same sample-vote procedure.\n",
            "IO CoT ToT IO\n",
            "+refine\n",
            "ToT\n",
            "+refine\n",
            "4\n",
            "6\n",
            "8\n",
            "(a) GPT-4 coherency scores\n",
            "CoT > ToT Similar ToT > CoT\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "21\n",
            "38 41\n",
            "(b) Human coherency comparison\n",
            "--------------------------------------------------\n",
            "Chunk 158:\n",
            "Figure 5: Creative Writing results.\n",
            "Method Success Rate (%)\n",
            "Letter Word Game\n",
            "IO 38.7 14 0\n",
            "CoT 40.6 15.6 1\n",
            "ToT (ours) 78 60 20\n",
            "+best state 82.4 67.5 35\n",
            "-prune 65.4 41.5 5\n",
            "-backtrack 54.6 20 5\n",
            "Table 3: Mini Crosswords results.\n",
            "--------------------------------------------------\n",
            "Chunk 159:\n",
            "it improves IO coherency score from 6.19 to 7.67, and ToT coherency score from 7.56 to 7.91. We\n",
            "believe it could be thought of as a third approach to thought generation in the ToT framework, where\n",
            "--------------------------------------------------\n",
            "Chunk 160:\n",
            "new thoughts can arise from refining old thoughts instead of i.i.d. or sequentially generated.\n",
            "4.3 Mini crosswords\n",
            "In Game of 24 and Creative Writing, ToT is relatively shallow — at most 3 thought steps are needed\n",
            "--------------------------------------------------\n",
            "Chunk 161:\n",
            "to reach the final output. Here we explore 5 ×5 mini crosswords as a harder search problem involving\n",
            "natural language. Again, the goal is not just to solve the task, as more general crosswords can be\n",
            "--------------------------------------------------\n",
            "Chunk 162:\n",
            "readily solved with specialized NLP pipelines [34] that leverages large-scale retrieval instead of LM.\n",
            "Rather, we aim to explore the limit of LM as a general problem solver that explores its own thoughts\n",
            "--------------------------------------------------\n",
            "Chunk 163:\n",
            "and guides its own exploration with deliberate reasoning as heuristics.\n",
            "Task setup. We scrape data from GooBix, which contains 156 games of 5 × 5 mini crosswords. As\n",
            "--------------------------------------------------\n",
            "Chunk 164:\n",
            "we observe adjacent games contain similar clues, we use 20 games with indices 1, 6, ··· , 91, 96 for\n",
            "testing, and games 136, 141, 146, 151, 156 for prompting. For each task, the input describes the 5\n",
            "--------------------------------------------------\n",
            "Chunk 165:\n",
            "horizontal clues and 5 vertical clues, and the output should be a board of 5 × 5 = 25letters to solve\n",
            "the crosswords. For evaluation, we consider three levels of success: the portion of correct letters (25\n",
            "per game), words (10 per game), and games.\n",
            "--------------------------------------------------\n",
            "Chunk 166:\n",
            "Baselines. We provide 5 example input-output pairs in the IO prompt, and in the CoT prompt\n",
            "additionally include intermediate words in the order h1..5 then v1..5. We run each prompt for 10\n",
            "samples and average the results.\n",
            "--------------------------------------------------\n",
            "Chunk 167:\n",
            "ToT setup. We leverage a depth-first search (Algorithm 2) that keeps exploring the most promising\n",
            "subsequent word clue until the state is no longer promising, then backtrack to the parent state to\n",
            "--------------------------------------------------\n",
            "Chunk 168:\n",
            "explore alternative thoughts. To make search tractable, subsequent thoughts are constrained not to\n",
            "change any filled words or letters, so that the ToT has at most 10 intermediate steps. For thought\n",
            "--------------------------------------------------\n",
            "Chunk 169:\n",
            "generation, at each state we translate all existing thoughts (e.g. “h2.motor; h1.tasks” for the state\n",
            "in Figure 6(a)) into letter constraints for remaining clues (e.g. “v1.To heap: tm ;...”) and prompt\n",
            "--------------------------------------------------\n",
            "Chunk 170:\n",
            "a proposal prompt 5 times to come up with candidates for where and what to fill in the next word.\n",
            "Importantly, we also prompt the LM to give a confidence level for different thoughts, and aggregate\n",
            "7\n",
            "--------------------------------------------------\n",
            "Chunk 171:\n",
            ">\u000b\n",
            "Y\u0016\u0011\u0003HORSH\n",
            "\u000f\u0003\u0016\u0011\u0015\f\u000f\u0003\u000b\n",
            "K\u0015\u0011\u0003YDOXH\n",
            "\u000f\u0003\u0015\u0011\u0013\f\u000f\u0003\u000b\n",
            "K\u0014\u0011\u0003SDUFK\n",
            "\u000f\u0003\n",
            "\u0014\u0011\u001c\f\u000f\u0003\u000b\n",
            "Y\u0018\u0011\u0003FRYHW\n",
            "\u000f\u0003\u0013\u0011\u0019\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0014\f\u000f\u0003\u000b\n",
            "K\u0015\u0011\u0003\n",
            "PHULW\n",
            "\u000f\u0003\u0013\u0011\u0017\f\u000f\u0003\u000b\n",
            "Y\u0014\u0011\u0003DOORZ\n",
            "\u000f\u0003\u0013\u0011\u0015\f\u000f\u0003\u000b\n",
            "Y\u0015\u0011\u0003JULQG\n",
            "\u000f\u0003\u0013\u0011\u0014\f\u000f\u0003\n",
            "\u000b\n",
            "K\u0017\u0011\u0003OHSHU\n",
            "\u000f\u0003\u0013\u0011\u0014\f@\n",
            "--------------------------------------------------\n",
            "Chunk 172:\n",
            "Y\u0016\u0011\u0003HORSH\n",
            "0XOWLSOH\u0003\n",
            "UXQV\n",
            "3DUVH\u000f\u0003ILOWHU\u0003RXW\u0003\n",
            "QRQ\u0010ILYH\u0010OHWWHU\u000f\u0003\n",
            "VFRUH\u000f\u0003DJJUHJDWH\n",
            "&KRRVH\u0003\n",
            "\u000bVRIW\u0003\n",
            "VHOI\u0010FRQVLVWHQF\\\"\f\n",
            "\u0014\u0011 0D[\n",
            "\u0015\u0011 0D[\u0003ZLWKRXW\u0003\n",
            "YLRODWH\n",
            "\u0016\u0011 ')6\n",
            "GĮŔũƜˤ\u001dĦũòŝ\n",
            "ĎɿʛĭĵŤĵŗ\n",
            "ĎɾʛƘÊŝģŝ\n",
            "ĎʁʛŝÊĦĵĮ\n",
            "ƘÊŝģŝ\n",
            "ƘÊŝģŝ\n",
            "ĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲ\n",
            "ſʂʛˤŝŗíŗƆˤʱĦĵƀʲ\n",
            "ſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲ\n",
            "--------------------------------------------------\n",
            "Chunk 173:\n",
            "ʟʟ\n",
            "7KRXJKW\u00033URSRVDOV\n",
            "ÊĈĈŗòĈÊŤò\n",
            "ſʀʛˤŗòŤòĮƜƓĵũŝʞˤƚĦĵƀòŗƆʝˤˈˈˈˈˈˤŝũŗò\n",
            "6WDWH\u0003(YDOXDWRU\u0003\u000bRYHU\u0003HDFK\u0003FOXH\f\n",
            "ſɾʛˤÉĵˤĎòÊŔʝˤƛĭˈŝˈˤʳʛʛʛʴˤđĭŔĵŝŝđæĦò\n",
            "ſʂʛˤ#òŝƓççÊŤĵŗʞˤĭĵŗòˤíŗƆʝˤŝŗˈĮˈˤʳʛʛʛʴˤĭÊƆæò\n",
            "ʟʟ\n",
            "ʱæÊçģƜŗÊçģʲ\n",
            "ĎʀʛĈŗÊĮí\n",
            "ʟʟ ʱŝũæƜŗòòˤŔŗũĮòíʲ\n",
            "ĎʁʛˤŝÊĦĵĮ\n",
            "ĎʀʛˤĈŗÊĮí\n",
            "ſʀʛˤŝƜŗđĮĈ\n",
            "--------------------------------------------------\n",
            "Chunk 174:\n",
            "ʟʟ\n",
            "')6\u0003\n",
            "2UGHUĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲ\n",
            "ſʂʛˤŝŗíŗƆˤʱĦĵƀʲ\n",
            "ſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲ\n",
            "ʟʟ\n",
            "7KRXJKW\u00033URSRVDOV\n",
            "ĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲ\n",
            "ſʂʛˤŝŗíŗƆˤʱĦĵƀʲ\n",
            "ſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲ\n",
            "ʟʟ\n",
            "7KRXJKW\u00033URSRVDOV\n",
            "ĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲ\n",
            "ſʂʛˤŝŗíŗƆˤʱĦĵƀʲ\n",
            "ſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲ\n",
            "ʟʟ\n",
            "7KRXJKW\u00033URSRVDOV\n",
            "ĎʁʛˤŝÊĦĵĮˤʱŝũŗòʲ\n",
            "--------------------------------------------------\n",
            "Chunk 175:\n",
            "ſʂʛˤŝŗíŗƆˤʱĦĵƀʲ\n",
            "ſʀʛˤŝƜŗđĮĈˤʱĎƓĈĎʲ\n",
            "ʟʟ\n",
            "7KRXJKW\u00033URSRVDOV\n",
            "ʱÊʲ\n",
            "ʱæʲ\n",
            "ƛˤÊˤŝˤģˤŝ\n",
            "ĭˤĵˤƛˤĵˤŗ\n",
            "ˈˤˈˤˈˤˈˤˈ\n",
            "ŝˤÊˤĦˤĵˤĮ\n",
            "ˈˤˈˤˈˤˈˤˈ\n",
            "Figure 6: In Mini Crosswords, (a) how thoughts are proposed and aggregated in a priority queue\n",
            "--------------------------------------------------\n",
            "Chunk 176:\n",
            "for depth-first search (DFS), and (b) how a state is evaluated based on the possibility of filling in\n",
            "each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM.\n",
            "--------------------------------------------------\n",
            "Chunk 177:\n",
            "Then DFS backtracks to the parent state and explore the next promising thought for clue.\n",
            "these across proposals to obtain a sorted list of next thoughts to explore (Figure 6(a)). For state\n",
            "--------------------------------------------------\n",
            "Chunk 178:\n",
            "evaluations, we similarly translate each state into letter constraints for remaining clues, then evaluate\n",
            "for each clue if it is possible to fill given the constraints. If any remaining clue is deemed “impossible”\n",
            "--------------------------------------------------\n",
            "Chunk 179:\n",
            "to fill in (e.g. “v1. To heap: tm s ”), then the exploration of the state’s subtree is pruned and DFS\n",
            "backtracks to its parent to explore the next promising thought. We limit DFS search steps to 100, and\n",
            "--------------------------------------------------\n",
            "Chunk 180:\n",
            "simply render the deepest explored state (the first explored one if multiple) into the final output.\n",
            "Results. As shown in Table 3, IO and CoT prompting methods perform poorly with a word-level\n",
            "--------------------------------------------------\n",
            "Chunk 181:\n",
            "success rate less than 16%, while ToT significantly improves all metrics, achieving a word-level\n",
            "success rate of 60% and solving 4 out of 20 games. Such an improvement is not surprising, given IO\n",
            "--------------------------------------------------\n",
            "Chunk 182:\n",
            "and CoT lack mechanisms to try different clues, make changes to decisions, or backtrack.\n",
            "Oracle and ablation studies. When outputting from the oracle best DFS state (instead of the\n",
            "--------------------------------------------------\n",
            "Chunk 183:\n",
            "heuristically determined best state) per task, ToT performance is even higher and actually solves\n",
            "7/20 games (Table 3, “+best state”), indicating our simple output heuristics can be readily improved.\n",
            "--------------------------------------------------\n",
            "Chunk 184:\n",
            "Interestingly, sometimes when the crosswords game is actually solved, the state evaluator might still\n",
            "deem some words as “impossible” and prune — possibly because 5 × 5 crosswords by design have\n",
            "--------------------------------------------------\n",
            "Chunk 185:\n",
            "some rare or obselete words that GPT-4 cannot recognize2. Given the state evaluation as a pruning\n",
            "heuristic is imperfect, we also explore ablating the pruning, and find the performance generally worse\n",
            "--------------------------------------------------\n",
            "Chunk 186:\n",
            "(Table 3, “-prune”). However, it could actually find the correct solution for 4/20 games (though only\n",
            "outputting 1 via heuristic), 3 of which are games ToT+pruning cannot solve within 100 steps. Thus,\n",
            "--------------------------------------------------\n",
            "Chunk 187:\n",
            "better heuristics for DFS pruning are critical for problem solving in this case. Lastly, we confirm the\n",
            "importance of backtracking by running an ablation that keeps filling the most promising clue for at\n",
            "--------------------------------------------------\n",
            "Chunk 188:\n",
            "most 20 steps, allowing overwrites. This is similar to a “greedy” BFS search with breadth limit of\n",
            "b = 1, and performs poorly with a word level success of only 20% (Table 3, “-backtrack”).\n",
            "5 Related Work\n",
            "--------------------------------------------------\n",
            "Chunk 189:\n",
            "Planning and decision making. Smart planning and decision making are critical to achieving\n",
            "predefined goals. As they are trained on vast amount of world knowledge and human examples, LMs\n",
            "--------------------------------------------------\n",
            "Chunk 190:\n",
            "are known to have already absorbed rich commonsense that makes it possible to propose reasonable\n",
            "plans conditioned on problem setting and environmental states [ 12, 42, 37, 13, 35, 41, 40]. Our\n",
            "--------------------------------------------------\n",
            "Chunk 191:\n",
            "proposed ToT approach extends existing planning formulations by considering multiple potentially\n",
            "feasible plans simultaneously at each problem-solving step, and proceeding with the most promising\n",
            "--------------------------------------------------\n",
            "Chunk 192:\n",
            "ones. The integration between thought sampling and value feedback organically integrates planning\n",
            "and decision-making mechanisms, enabling effective search inside a solution tree. On the other hand,\n",
            "--------------------------------------------------\n",
            "Chunk 193:\n",
            "traditional decision-making procedures usually require training dedicated reward and policy models\n",
            "as in reinforcement learning (for example CHAI [ 33]), whereas we use the LM itself to provide\n",
            "--------------------------------------------------\n",
            "Chunk 194:\n",
            "the value estimates for decision making. RAP [9] is a concurrent work that treats language model\n",
            "2For example, “agend” is an obsolete form of “agendum”, but GPT-4 deems it a typo for “agenda”. External\n",
            "--------------------------------------------------\n",
            "Chunk 195:\n",
            "retrieval or web interaction could augment LM for problem solving under knowledge uncertainty.\n",
            "8\n",
            "--------------------------------------------------\n",
            "Chunk 196:\n",
            "reasoning as planning with its internal world model, and proposes a MCTS-based method similar to\n",
            "ToT. However, its tasks are simpler than ours, and its framework lacks the modularity to incorporate\n",
            "different tree search algorithms.\n",
            "--------------------------------------------------\n",
            "Chunk 197:\n",
            "Self-reflection. Using LLMs to assess the viability of their own predictions is becoming an in-\n",
            "creasingly important procedure in problem solving. [ 28, 20, 24] introduced the “self-reflection”\n",
            "--------------------------------------------------\n",
            "Chunk 198:\n",
            "mechanism, in which LMs provide feedback to their generation candidates. [4] improves LMs code\n",
            "generation accuracy by injecting feedback messages generated by the LM itself based on its code\n",
            "--------------------------------------------------\n",
            "Chunk 199:\n",
            "execution results. Similarly, [17] also introduces “critic” or review steps over the actions and states,\n",
            "deciding the next action to take in solving computer operation tasks. Another recent work very\n",
            "--------------------------------------------------\n",
            "Chunk 200:\n",
            "relevant to ours is “self-eval guided decoding” [ 39]. Similar to our method, self-eval decoding\n",
            "also follows a tree-search procedure with leaves sampled from stochastic beam search decoding,\n",
            "--------------------------------------------------\n",
            "Chunk 201:\n",
            "which are then evaluated by LLM itself with carefully prepared self-eval prompts. Their approach\n",
            "however, uses the PAL formulation [8] which represents thoughts as codes, which makes it difficult\n",
            "--------------------------------------------------\n",
            "Chunk 202:\n",
            "to tackle challenging tasks like creative writing which we consider in this paper. Our Tree-of-Thought\n",
            "formulation is thus more versatile and handles challenging tasks on which GPT-4 only achieves very\n",
            "low accuracy with standard prompts.\n",
            "--------------------------------------------------\n",
            "Chunk 203:\n",
            "Program-guided LLM generation. Our proposal is also related to recent advancements that organize\n",
            "LM’s behavior with systematic procedures [14, 44, 6, 43] or symbolic program guidance. For example,\n",
            "--------------------------------------------------\n",
            "Chunk 204:\n",
            "Schlag et al. [27] embeds LMs in an algorithmic search procedure to help solve problems like question\n",
            "answering step-by-step, in which the search trees are expanded by relevant paragraphs that might\n",
            "--------------------------------------------------\n",
            "Chunk 205:\n",
            "provide answers. This approach however differs from ours in that trees are expanded by sampling\n",
            "external paragraphs instead of the LM’s own thoughts, and there is no reflection or voting steps.\n",
            "--------------------------------------------------\n",
            "Chunk 206:\n",
            "Another approach, LLM+P [18], goes one step further and delegates the actual planning process to a\n",
            "classical planner.\n",
            "Classical search methods. Last but not least, our approach can be treated as a modern rendition\n",
            "--------------------------------------------------\n",
            "Chunk 207:\n",
            "of classical search methods for problem solving. For example it can be considered as a heuristic\n",
            "search algorithm like A* [10], in which the heuristic at each search node is provided by the LM’s self-\n",
            "--------------------------------------------------\n",
            "Chunk 208:\n",
            "assessment. From this perspective, our method is also related to NeuroLogic A*esque decoding [19],\n",
            "which is inspired by A* search but introduces look-ahead heuristics that are efficient for LMs to\n",
            "--------------------------------------------------\n",
            "Chunk 209:\n",
            "improve the beam-search or top-k sampling decoding. This method however is constrained to\n",
            "sentence generation tasks, whereas our framework are designed for complex, multi-step problem\n",
            "solving guarded by value feedback.\n",
            "6 Discussion\n",
            "--------------------------------------------------\n",
            "Chunk 210:\n",
            "Limitations and future directions. Deliberate search such as ToT might not be necessary for many\n",
            "existing tasks that GPT-4 already excels at (see Appendix B.1), and as an initial step this work only\n",
            "--------------------------------------------------\n",
            "Chunk 211:\n",
            "explores three relatively simple tasks that challenges GPT-4 (see Appendix B.2 for some GPT-3.5\n",
            "experiment results) and calls of better search and planning abilities incorporated with LMs. However,\n",
            "--------------------------------------------------\n",
            "Chunk 212:\n",
            "as we begin to deploy LMs for more real-world decision making applications (e.g. coding, data\n",
            "analysis, robotics, etc.), more complex tasks could emerge and present new opportunities to study\n",
            "--------------------------------------------------\n",
            "Chunk 213:\n",
            "these research questions. Also, search methods like ToT requires more resources (e.g. GPT-4 API\n",
            "cost) than sampling methods in order to improve task performances, but the modular flexibility of\n",
            "--------------------------------------------------\n",
            "Chunk 214:\n",
            "ToT allows users to customize such performance-cost tradeoffs, and ongoing open-source efforts [32]\n",
            "should readily reduce such costs in the near future. More details about cost and efficiency are in\n",
            "--------------------------------------------------\n",
            "Chunk 215:\n",
            "Appendix B.3. Lastly, this work focuses on using an off-the-shelf LM, and fine-tuning LMs using\n",
            "a ToT-style high-level counterfactual decision making (e.g. deliberating over potential choices for\n",
            "--------------------------------------------------\n",
            "Chunk 216:\n",
            "the next paragraph, instead of predicting the next token) might present opportunities to enhance the\n",
            "problem-solving capabilities of LMs.\n",
            "Conclusion. The associative “System 1” of LMs can be beneficially augmented by a “System 2”\n",
            "--------------------------------------------------\n",
            "Chunk 217:\n",
            "based on searching a tree of possible paths to the solution to a problem. The Tree of Thoughts\n",
            "framework provides a way to translate classical insights about problem-solving into actionable\n",
            "--------------------------------------------------\n",
            "Chunk 218:\n",
            "methods for contemporary LMs. At the same time, LMs address a weakness of these classical\n",
            "methods, providing a way to solve complex problems that are not easily formalized, such as creative\n",
            "--------------------------------------------------\n",
            "Chunk 219:\n",
            "writing. We see this intersection of LMs with classical approaches to AI as an exciting direction.\n",
            "9\n",
            "--------------------------------------------------\n",
            "Chunk 220:\n",
            "Broader Impact\n",
            "ToT is a framework that empowers LMs to more autonomously and intelligently make decisions\n",
            "and solve problems. While current tasks are limited to reasoning and search problems, future\n",
            "--------------------------------------------------\n",
            "Chunk 221:\n",
            "applications involving interaction with external environments or humans could bring potential danger,\n",
            "e.g. facilitating harmful uses of LMs. On the other hand, ToT also improves the interpretability\n",
            "--------------------------------------------------\n",
            "Chunk 222:\n",
            "of model decisions and the opportunity for human alignment, as the resulting representations are\n",
            "readable, high-level language reasoning instead of implicit, low-level token values.\n",
            "Acknowledgements\n",
            "--------------------------------------------------\n",
            "Chunk 223:\n",
            "SY and KN acknowledge support from an Oracle Collaborative Research award and the National\n",
            "Science Foundation under Grant No. 2239363. Any opinions, findings, conclusions, or recommenda-\n",
            "--------------------------------------------------\n",
            "Chunk 224:\n",
            "tions expressed in this material are those of the author(s) and do not necessarily reflect the views of\n",
            "the National Science Foundation. SY is also supported by the Harold W. Dodds Fellowship from\n",
            "Princeton.\n",
            "References\n",
            "--------------------------------------------------\n",
            "Chunk 225:\n",
            "[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\n",
            "G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\n",
            "information processing systems, 33:1877–1901, 2020.\n",
            "--------------------------------------------------\n",
            "Chunk 226:\n",
            "[2] C. Browne, E. J. Powley, D. Whitehouse, S. M. M. Lucas, P. I. Cowling, P. Rohlfshagen,\n",
            "S. Tavener, D. P. Liebana, S. Samothrakis, and S. Colton. A survey of monte carlo tree search\n",
            "--------------------------------------------------\n",
            "Chunk 227:\n",
            "methods. IEEE Transactions on Computational Intelligence and AI in Games, 4:1–43, 2012.\n",
            "[3] M. Campbell, A. J. Hoane Jr, and F.-h. Hsu. Deep blue. Artificial intelligence, 134(1-2):57–83,\n",
            "2002.\n",
            "--------------------------------------------------\n",
            "Chunk 228:\n",
            "[4] X. Chen, M. Lin, N. Sch¨arli, and D. Zhou. Teaching large language models to self-debug, 2023.\n",
            "[5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\n",
            "--------------------------------------------------\n",
            "Chunk 229:\n",
            "Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\n",
            "preprint arXiv:2204.02311, 2022.\n",
            "[6] A. Creswell and M. Shanahan. Faithful reasoning using large language models. arXiv preprint\n",
            "arXiv:2208.14271, 2022.\n",
            "--------------------------------------------------\n",
            "Chunk 230:\n",
            "[7] N. D. Daw, Y . Niv, and P. Dayan. Uncertainty-based competition between prefrontal and\n",
            "dorsolateral striatal systems for behavioral control. Nature neuroscience, 8(12):1704–1711,\n",
            "2005.\n",
            "--------------------------------------------------\n",
            "Chunk 231:\n",
            "[8] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y . Yang, J. Callan, and G. Neubig. Pal: Program-\n",
            "aided language models, 2023.\n",
            "[9] S. Hao, Y . Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language\n",
            "--------------------------------------------------\n",
            "Chunk 232:\n",
            "model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.\n",
            "[10] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of\n",
            "--------------------------------------------------\n",
            "Chunk 233:\n",
            "minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2):100–107,\n",
            "1968. doi: 10.1109/TSSC.1968.300136.\n",
            "[11] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of\n",
            "--------------------------------------------------\n",
            "Chunk 234:\n",
            "minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100–107,\n",
            "1968.\n",
            "[12] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\n",
            "Extracting actionable knowledge for embodied agents, 2022.\n",
            "--------------------------------------------------\n",
            "Chunk 235:\n",
            "[13] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\n",
            "Y . Chebotar, et al. Inner monologue: Embodied reasoning through planning with language\n",
            "models. arXiv preprint arXiv:2207.05608, 2022.\n",
            "10\n",
            "--------------------------------------------------\n",
            "Chunk 236:\n",
            "[14] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, R. L. Bras, and Y . Choi. Maieu-\n",
            "tic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint\n",
            "arXiv:2205.11822, 2022.\n",
            "--------------------------------------------------\n",
            "Chunk 237:\n",
            "[15] D. Kahneman. Thinking, fast and slow. Macmillan, 2011.\n",
            "[16] D. Kahneman, S. Frederick, et al. Representativeness revisited: Attribute substitution in intuitive\n",
            "judgment. Heuristics and biases: The psychology of intuitive judgment, 49(49-81):74, 2002.\n",
            "--------------------------------------------------\n",
            "Chunk 238:\n",
            "[17] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks, 2023.\n",
            "[18] B. Liu, Y . Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+p: Empowering\n",
            "large language models with optimal planning proficiency, 2023.\n",
            "--------------------------------------------------\n",
            "Chunk 239:\n",
            "[19] X. Lu, S. Welleck, P. West, L. Jiang, J. Kasai, D. Khashabi, R. L. Bras, L. Qin, Y . Yu,\n",
            "R. Zellers, N. A. Smith, and Y . Choi. Neurologic a*esque decoding: Constrained text generation\n",
            "--------------------------------------------------\n",
            "Chunk 240:\n",
            "with lookahead heuristics. In North American Chapter of the Association for Computational\n",
            "Linguistics, 2021.\n",
            "[20] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri,\n",
            "--------------------------------------------------\n",
            "Chunk 241:\n",
            "S. Prabhumoye, Y . Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark.\n",
            "Self-refine: Iterative refinement with self-feedback, 2023.\n",
            "[21] A. Newell, J. C. Shaw, and H. A. Simon. Report on a general problem solving program. In IFIP\n",
            "--------------------------------------------------\n",
            "Chunk 242:\n",
            "congress, volume 256, page 64. Pittsburgh, PA, 1959.\n",
            "[22] A. Newell, H. A. Simon, et al. Human problem solving. Prentice-Hall, 1972.\n",
            "[23] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n",
            "--------------------------------------------------\n",
            "Chunk 243:\n",
            "[24] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Refiner:\n",
            "Reasoning feedback on intermediate representations, 2023.\n",
            "--------------------------------------------------\n",
            "Chunk 244:\n",
            "[25] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding\n",
            "by generative pre-training. OpenAI blog, 2018.\n",
            "[26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\n",
            "--------------------------------------------------\n",
            "Chunk 245:\n",
            "unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
            "[27] I. Schlag, S. Sukhbaatar, A. Celikyilmaz, W. tau Yih, J. Weston, J. Schmidhuber, and X. Li.\n",
            "Large language model programs, 2023.\n",
            "--------------------------------------------------\n",
            "Chunk 246:\n",
            "[28] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory\n",
            "and self-reflection, 2023.\n",
            "[29] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,\n",
            "--------------------------------------------------\n",
            "Chunk 247:\n",
            "M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. nature, 550\n",
            "(7676):354–359, 2017.\n",
            "[30] S. A. Sloman. The empirical case for two systems of reasoning. Psychological bulletin, 119(1):\n",
            "3, 1996.\n",
            "--------------------------------------------------\n",
            "Chunk 248:\n",
            "[31] K. E. Stanovich. Who is rational? Studies of individual differences in reasoning. Psychology\n",
            "Press, 1999.\n",
            "[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal,\n",
            "--------------------------------------------------\n",
            "Chunk 249:\n",
            "E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\n",
            "preprint arXiv:2302.13971, 2023.\n",
            "[33] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with\n",
            "--------------------------------------------------\n",
            "Chunk 250:\n",
            "offline reinforcement learning. In Proceedings of the 2022 Conference of the North American\n",
            "Chapter of the Association for Computational Linguistics: Human Language Technologies ,\n",
            "pages 4471–4491, 2022.\n",
            "11\n",
            "--------------------------------------------------\n",
            "Chunk 251:\n",
            "[34] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated\n",
            "crossword solving. arXiv preprint arXiv:2205.09665, 2022.\n",
            "[35] L. Wang, W. Xu, Y . Lan, Z. Hu, Y . Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting:\n",
            "--------------------------------------------------\n",
            "Chunk 252:\n",
            "Improving zero-shot chain-of-thought reasoning by large language models, 2023.\n",
            "[36] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain\n",
            "of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n",
            "--------------------------------------------------\n",
            "Chunk 253:\n",
            "[37] Z. Wang, S. Cai, A. Liu, X. Ma, and Y . Liang. Describe, explain, plan and select: Interactive\n",
            "planning with large language models enables open-world multi-task agents, 2023.\n",
            "--------------------------------------------------\n",
            "Chunk 254:\n",
            "[38] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\n",
            "prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n",
            "--------------------------------------------------\n",
            "Chunk 255:\n",
            "[39] Y . Xie, K. Kawaguchi, Y . Zhao, X. Zhao, M.-Y . Kan, J. He, and Q. Xie. Decomposition\n",
            "enhances reasoning via self-evaluation guided decoding, 2023.\n",
            "[40] S. Yang, O. Nachum, Y . Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for\n",
            "--------------------------------------------------\n",
            "Chunk 256:\n",
            "decision making: Problems, methods, and opportunities, 2023.\n",
            "[41] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao. ReAct: Synergizing\n",
            "reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n",
            "--------------------------------------------------\n",
            "Chunk 257:\n",
            "[42] S. Zhang, Z. Chen, Y . Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large\n",
            "language models for code generation. In The Eleventh International Conference on Learning\n",
            "Representations, 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL.\n",
            "--------------------------------------------------\n",
            "Chunk 258:\n",
            "[43] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet,\n",
            "Q. Le, et al. Least-to-most prompting enables complex reasoning in large language models.\n",
            "arXiv preprint arXiv:2205.10625, 2022.\n",
            "--------------------------------------------------\n",
            "Chunk 259:\n",
            "[44] X. Zhu, J. Wang, L. Zhang, Y . Zhang, R. Gan, J. Zhang, and Y . Yang. Solving math word\n",
            "problem via cooperative reasoning induced language models. arXiv preprint arXiv:2210.16257,\n",
            "2022.\n",
            "12\n",
            "--------------------------------------------------\n",
            "Chunk 260:\n",
            "A Code, Prompts, Trajectories\n",
            "All code is available at https://github.com/princeton-nlp/tree-of-thought-llm .\n",
            "All prompts are available at https://github.com/princeton-nlp/tree-of-thought-llm/\n",
            "tree/master/src/tot/prompts.\n",
            "--------------------------------------------------\n",
            "Chunk 261:\n",
            "Trajectories are available at https://github.com/princeton-nlp/tree-of-thought-llm/\n",
            "tree/master/logs.\n",
            "B Additional Experiment Results\n",
            "Given the motivation of exploring and extending the capability frontier of language models, our\n",
            "--------------------------------------------------\n",
            "Chunk 262:\n",
            "experiments in the main paper have focused on a setup with the state-of-the-art language model\n",
            "(GPT-4), and three hard tasks invented to challenge it. Here, we report additional experiments with\n",
            "weaker LLM or easier tasks, and discuss cost and efficiency.\n",
            "--------------------------------------------------\n",
            "Chunk 263:\n",
            "GSM8K StrategyQA\n",
            "IO 51 73\n",
            "CoT 86 82\n",
            "ToT 90 83\n",
            "Table 4: New tasks with\n",
            "zero-shot ToT and GPT-4.\n",
            "GPT-4 GPT-3.5\n",
            "IO 7.3% 6%\n",
            "CoT 4.0% 3%\n",
            "ToT 74% 19%\n",
            "Table 5: Game of 24 with\n",
            "GPT-4 vs GPT-3.5.\n",
            "GPT-4 GPT-3.5\n",
            "IO 6.19 4.47\n",
            "CoT 6.93 5.16\n",
            "ToT 7.56 6.62\n",
            "--------------------------------------------------\n",
            "Chunk 264:\n",
            "Table 6: Creative Writing with\n",
            "GPT-4 vs. GPT-3.5.\n",
            "B.1 Extension to new tasks (GSM8k, StrategyQA) with zero-shot ToT\n",
            "While more common NLP tasks might be too easy for GPT-4 and do not require ToT (which is why\n",
            "--------------------------------------------------\n",
            "Chunk 265:\n",
            "we considered harder new tasks), we believe applying ToT to new tasks could be straightforward.\n",
            "For example, we implemented a simple and generic zero-shot ToT-BFS similar to creative writing\n",
            "--------------------------------------------------\n",
            "Chunk 266:\n",
            "(sample 5 problem solving strategies then vote for the best one; then sample 5 solutions based on the\n",
            "best strategy then vote for the best one) for GSM8K and StrategyQA with few extra lines of code:\n",
            "# define the answer format of new tasks\n",
            "--------------------------------------------------\n",
            "Chunk 267:\n",
            "gsm8k_format = ‘\"the answer is n\" where n is a number’\n",
            "strategyqa_format = ‘either \"the answer is yes\" or \"the answer is no\"’\n",
            "# define zero-shot io prompting\n",
            "standard_prompt = ‘Answer the following question with {format}: {input}’\n",
            "--------------------------------------------------\n",
            "Chunk 268:\n",
            "# define thought format for zero-shot cot and zero-shot tot\n",
            "cot_prompt = ‘‘‘Answer the following question: {input}\n",
            "Make a strategy then write. Your output should be of the following format:\n",
            "Strategy:\n",
            "Your strategy about how to answer the question.\n",
            "Answer:\n",
            "--------------------------------------------------\n",
            "Chunk 269:\n",
            "Your answer to the question. It should end with {format}.\n",
            "’’’\n",
            "# define zero-shot voting used for zero-shot tot\n",
            "vote_prompt = ‘‘‘Given an instruction and several choices,\n",
            "decide which choice is most promising.\n",
            "--------------------------------------------------\n",
            "Chunk 270:\n",
            "Analyze each choice in detail, then conclude in the last line\n",
            "\"The best choice is {s}\", where s the integer id of the choice.\n",
            "’’’\n",
            "13\n",
            "--------------------------------------------------\n",
            "Chunk 271:\n",
            "We evaluated on a subset of 100 random GSM8K test and StrategyQA dev questions. As shown\n",
            "in Table 4 and as expected, ToT improves over CoT on both tasks (but only slightly, given GPT-4\n",
            "--------------------------------------------------\n",
            "Chunk 272:\n",
            "+ CoT is already very good on such tasks, and StrategyQA’s bottleneck is external knowledge, not\n",
            "reasoning). Considering computational costs, it is more suitable to try smaller LLMs + ToT for\n",
            "--------------------------------------------------\n",
            "Chunk 273:\n",
            "traditional NLP tasks, or GPT-4 + ToT for hard tasks that challenge GPT-4 + CoT’s reasoning.\n",
            "B.2 Extension to new LMs (GPT-3.5)\n",
            "To understand how ToT works with other LLMs, we also ran GPT-3.5-turbo for Creative Writing\n",
            "--------------------------------------------------\n",
            "Chunk 274:\n",
            "(Table 6) and Game of 24 (Table 5). On both tasks, “ToT> CoT > IO” remains true for GPT-3.5. On\n",
            "Creative Writing, we find GPT-3.5+ToT outperform GPT-4+IO, and similar to GPT-4+CoT, which\n",
            "suggests ToT could also work well on weaker language models.\n",
            "--------------------------------------------------\n",
            "Chunk 275:\n",
            "On Game of 24 (we changed 1-shot proposal prompt to 3-shot to make it work), GPT-3.5+ToT’s\n",
            "19% is far worse than GPT-4+ToT’s 74%. To further understand the importance of generation\n",
            "--------------------------------------------------\n",
            "Chunk 276:\n",
            "vs. evaluation, we ran GPT-4 generation + GPT-3.5 evaluation (64%) and GPT-3.5 generation +\n",
            "GPT-4 evaluation (31%). This suggests the game’s bottleneck is thought generation, and different\n",
            "--------------------------------------------------\n",
            "Chunk 277:\n",
            "generation/evaluation language models might attain decent results while reducing costs.\n",
            "B.3 Cost and efficiency\n",
            "Running ToT requires significantly more computations than IO or CoT prompting. For example, in\n",
            "--------------------------------------------------\n",
            "Chunk 278:\n",
            "Game of 24 (Table 7 below), solving a problem with ToT requires 5.5k completion tokens, close to\n",
            "100 CoT trials (6.7k tokens). But the performance of ToT is better than best of 100 independent CoT\n",
            "trials.\n",
            "--------------------------------------------------\n",
            "Chunk 279:\n",
            "Game of 24 Generate/Prompt tokens Cost per case Success\n",
            "IO (best of 100) 1.8k / 1.0k $0.13 33%\n",
            "CoT (best of 100) 6.7k / 2.2k $0.47 49%\n",
            "ToT 5.5k / 1.4k $0.74 74%\n",
            "Table 7: Cost analysis on Game of 24.\n",
            "--------------------------------------------------\n",
            "Chunk 280:\n",
            "On Creative Writing (Table 8 below), we found ToT takes around 5x completion tokens and money\n",
            "cost, which is intuitive as b = 5and most tokens are generated passages.\n",
            "Creative Writing Generate/Prompt tokens Cost per case\n",
            "IO 0.9k / 0.4k $0.06\n",
            "--------------------------------------------------\n",
            "Chunk 281:\n",
            "CoT 0.9k / 0.4k $0.07\n",
            "ToT 4k / 2.9k $0.32\n",
            "Table 8: Cost analysis on Game of 24.\n",
            "So completing Game of 24 and Creative Writing’s main ToT experiments cost around0.74 × 100 +\n",
            "--------------------------------------------------\n",
            "Chunk 282:\n",
            "0.32 × 100 = 106dollars. Crosswords’ DFS experiments should be also within 100 dollars. In\n",
            "general, cost and efficiency of ToT highly depend on the prompts and search algorithms used, and\n",
            "--------------------------------------------------\n",
            "Chunk 283:\n",
            "could require 5-100 times more generated tokens than CoT. Some actionable insights:\n",
            "• We recommend using ToT on tasks requiring deliberate reasoning, on which CoT struggles.\n",
            "--------------------------------------------------\n",
            "Chunk 284:\n",
            "• Flexibility of ToT allows some performance-cost tradeoff, e.g., change beam size or vote\n",
            "number in BFS, few-shot vs. zero-shot prompting, GPT-3.5 vs. GPT-4, etc. One could\n",
            "configure the setup based on some resource constraints or performance goal.\n",
            "--------------------------------------------------\n",
            "Chunk 285:\n",
            "• There is much space for improving efficiency, e.g., BFS could early stop when solution is\n",
            "found, or trim down beam size to when some thoughts are ”impossible”.\n",
            "• We believe that more computation is indeed required in order for the model to achieve\n",
            "--------------------------------------------------\n",
            "Chunk 286:\n",
            "stronger intelligence, and this should not become a blocking issue as in the long run, (open-\n",
            "source) LMs will become much cheaper and more efficient. It is also a great direction how\n",
            "to better train/finetune LMs for thought generation and/or evaluation.\n",
            "--------------------------------------------------\n",
            "Chunk 287:\n",
            "14\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i + 1}:\")\n",
        "    print(chunk.page_content)  # Assuming chunks have a `page_content` attribute\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV0CGE9YvOiH"
      },
      "outputs": [],
      "source": [
        "def insert_or_fetch_embeddings(index_name, docs):\n",
        "  import pinecone\n",
        "  from langchain_community.vectorstores import Pinecone\n",
        "  from langchain_openai import OpenAIEmbeddings\n",
        "  # from pinecone import PodSpec\n",
        "  from pinecone import Pinecone, ServerlessSpec\n",
        "  from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "  from uuid import uuid4\n",
        "\n",
        "  from langchain_core.documents import Document\n",
        "\n",
        "  import time\n",
        "\n",
        "  # pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "  pc = pinecone.Pinecone(api_key=pc_apikey)\n",
        "\n",
        "  # from openai import OpenAI\n",
        "  # client = OpenAI()\n",
        "\n",
        "  embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=512, api_key=openai_api_key)  # 512 works as well\n",
        "\n",
        "  existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "  # loading from existing index\n",
        "  if index_name in pc.list_indexes().names():\n",
        "      print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
        "  else:\n",
        "      # creating the index and embedding the docs into the index\n",
        "      print(f'Creating index {index_name} and embeddings ...', end='')\n",
        "\n",
        "      # creating a new index\n",
        "      pc.create_index(\n",
        "          name=index_name,\n",
        "          dimension=512,\n",
        "          metric='cosine',\n",
        "          # spec=PodSpec(environment='gcp-starter')\n",
        "          spec=ServerlessSpec(\n",
        "              cloud=\"aws\",\n",
        "              region=\"us-east-1\"\n",
        "          )\n",
        "      )\n",
        "\n",
        "      print(\"index created....\")\n",
        "\n",
        "  # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
        "  # inserting the embeddings into the index and returning a new Pinecone vector store object.\n",
        "  index = pc.Index(index_name)\n",
        "  vector_store = PineconeVectorStore(index=index, embedding=embeddings) # Initialize with the index and embedding\n",
        "  uuids = [str(uuid4()) for _ in range(len(docs))]\n",
        "  vector_store.add_documents(docs, ids=uuids)\n",
        "  print('Ok')\n",
        "  return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt1txsO60x6P"
      },
      "outputs": [],
      "source": [
        "def delete_pinecone_index(index_name='all'):\n",
        "    import pinecone\n",
        "    pc = pinecone.Pinecone(api_key=pc_apikey)\n",
        "\n",
        "    if index_name == 'all':\n",
        "        indexes = pc.list_indexes().names()\n",
        "        print('Deleting all indexes ... ')\n",
        "        for index in indexes:\n",
        "            pc.delete_index(index)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        print(f'Deleting index {index_name} ...', end='')\n",
        "        pc.delete_index(index_name)\n",
        "        print('Ok')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quuV79MiCipe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGxMUxXdATzg",
        "outputId": "254412c1-0e92-4753-def3-3dde835d9201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting all indexes ... \n",
            "Ok\n"
          ]
        }
      ],
      "source": [
        "delete_pinecone_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPlZVYBsDnFq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa6WZtP37NIF",
        "outputId": "0b615863-21c4-4322-e9d1-c2c01b6ec3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/KR-reasoning-LLM-framework - .pdf\n",
            "Loading /content/KR-reasoning-LLM-framework.pdf\n",
            "Length of data 24\n",
            "Length of data 415\n"
          ]
        }
      ],
      "source": [
        "data = load_document('/content/KR-reasoning-LLM-framework.pdf')\n",
        "print(f'Length of data {len(data)}')\n",
        "chunks = chunk_data(data)\n",
        "print(f'Length of data {len(chunks)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i7LaXjuWespb",
        "outputId": "34a917bf-a7c6-46e4-c205-4d56a7df9ccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='K-Level Reasoning: Establishing Higher Order Beliefs\n",
            "in Large Language Models for Strategic Reasoning\n",
            "Yadong Zhang1,2,*, Shaoguang Mao2,†, Tao Ge2, Xun Wang2,\n",
            "Yan Xia2, Man Lan1, Furu Wei2,\n",
            "1East China Normal University, 2Microsoft Research Asia\n",
            "Abstract' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 0}\n",
            "--------------------------------------------------\n",
            "page_content='Strategic reasoning is a complex yet essential\n",
            "capability for intelligent agents. It requires\n",
            "Large Language Model (LLM) agents to adapt\n",
            "their strategies dynamically in multi-agent en-\n",
            "vironments. Unlike static reasoning tasks, suc-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 1}\n",
            "--------------------------------------------------\n",
            "page_content='cess in these contexts depends on anticipating\n",
            "other agents’ beliefs and actions while contin-\n",
            "uously adjusting strategies to achieve individ-\n",
            "ual goals. LLMs and LLM agents often strug-\n",
            "gle with strategic reasoning due to the absence' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 2}\n",
            "--------------------------------------------------\n",
            "page_content='of a reasoning framework that enables them\n",
            "to dynamically infer others’ perspectives and\n",
            "adapt to changing environments. Inspired by\n",
            "the Level-K framework1 from game theory and\n",
            "behavioral economics, which extends reason-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 3}\n",
            "--------------------------------------------------\n",
            "page_content='ing from simple reactions to structured strate-\n",
            "gic depth, we propose a novel framework: \"K-\n",
            "Level Reasoning with Large Language Mod-\n",
            "els (K-R).\" This framework employs recursive\n",
            "mechanisms to enable LLMs to achieve vary-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 4}\n",
            "--------------------------------------------------\n",
            "page_content='ing levels of strategic depth, allowing agents to\n",
            "form higher order beliefs—beliefs about others’\n",
            "beliefs. We validate this framework through\n",
            "rigorous testing on four testbeds: two classi-\n",
            "cal game theory problems and two social in-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 5}\n",
            "--------------------------------------------------\n",
            "page_content='telligence tasks. The results demonstrate the\n",
            "advantages of K-R in strategic reasoning. Our\n",
            "work presents the first recursive implementa-\n",
            "tion of strategic depth in large language models\n",
            "(LLMs). It establishes a foundation for future' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 6}\n",
            "--------------------------------------------------\n",
            "page_content='research into theory of mind and strategic rea-\n",
            "soning in LLMs.\n",
            "1 Introduction\n",
            "Strategic reasoning—decision-making in multi-\n",
            "participant environments—presents unique chal-\n",
            "lenges for Large Language Models (LLMs) and' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 7}\n",
            "--------------------------------------------------\n",
            "page_content='LLM agents(Zhang et al., 2024b). In these settings,\n",
            "*Work was done when interning at Microsoft Research\n",
            "Asia. † Correspondence to: shaoguang.mao@microsoft.com\n",
            "1According to the Level-k Framework, k-level thinking' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 8}\n",
            "--------------------------------------------------\n",
            "page_content='involves considering what opponent/partner are likely to do,\n",
            "what they think you will do, and what they believe you think\n",
            "they will do, and so on.\n",
            "First-level Thinking Second-level Thinking\n",
            "…\n",
            "Third-level Thinking\n",
            "……' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 9}\n",
            "--------------------------------------------------\n",
            "page_content='Figure 1: Level-K Framework: In first-level thinking,\n",
            "agents respond directly to the environment. In second-\n",
            "level thinking, agents consider the first-level thinking of\n",
            "others. This process continues iteratively, with agents' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 10}\n",
            "--------------------------------------------------\n",
            "page_content='forming higher order beliefs based on assumptions about\n",
            "others’ thoughts.\n",
            "agents must respond to the actions of others while\n",
            "adapting to dynamic environments. They also need\n",
            "to align their decisions with their own goals during' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 11}\n",
            "--------------------------------------------------\n",
            "page_content='these interactions. Strategic reasoning is essential\n",
            "for intelligent agents and is widely applied in real-\n",
            "world tasks, such as investment, business strategy\n",
            "making(Zhao et al., 2023), negotiation(Hua et al.,\n",
            "2023), and policy-making(Li et al., 2024).' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 12}\n",
            "--------------------------------------------------\n",
            "page_content='Effective strategic reasoning relies on under-\n",
            "standing others’ perspectives and anticipating their\n",
            "strategies. While there are some research efforts on\n",
            "LLMs’ strategic reasoning, most methods rely on\n",
            "static prompting (Fu et al., 2023; Xu et al., 2023b).' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 13}\n",
            "--------------------------------------------------\n",
            "page_content='This typically involves instructing the model to ac-\n",
            "count for others’ beliefs and decisions during its\n",
            "own decision-making process in the prompt. How-\n",
            "ever, these approaches fall short in enabling LLMs\n",
            "to form true higher order beliefs—beliefs about' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 14}\n",
            "--------------------------------------------------\n",
            "page_content='what others believe, and lack the flexibility needed\n",
            "for deeper strategic reasoning.\n",
            "K-level thinking (Figure 1) (Nagel, 1995; Cui\n",
            "et al., 2021), a classical concept in behavioral eco-\n",
            "nomics and game theory, categorizes reasoning into' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 15}\n",
            "--------------------------------------------------\n",
            "page_content='varying depths of strategic thought. It involves not\n",
            "only predicting others’ actions but also considering\n",
            "1\n",
            "arXiv:2402.01521v2  [cs.CL]  17 Oct 2024' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 16}\n",
            "--------------------------------------------------\n",
            "page_content='Please choose an integer between 1 and 100. The player \n",
            "whose chosen number is closest to 0.8 * the average of all \n",
            "chosen numbers wins the round. Let's start the 1st round.\n",
            "I think in the\n",
            "1st  round …\n",
            " …\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 17}\n",
            "--------------------------------------------------\n",
            "page_content='(40 + 50 + 34 + 40 + 36) / 5 = 40  (Average)\n",
            "40 * 0.8 = 32 (Target) \n",
            "The daily available water can meet the needs of only one \n",
            "resident. An auction is conducted, and the highest bidder \n",
            "will obtain today's water supply.\n",
            "$55\n",
            "$100\n",
            "8 1\n",
            "$60\n",
            "$ 100\n",
            "$50\n",
            "$ 100' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 18}\n",
            "--------------------------------------------------\n",
            "page_content='$70\n",
            "$ 100\n",
            "$30\n",
            "$ 100\n",
            "In today’s bidding,        placed the highest bid of $70, \n",
            "securing the water resources for the day.        +2     and \n",
            "resets the number of days thirsty =0 . Other residents \n",
            "have their HP deducted based on the number of days they' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 19}\n",
            "--------------------------------------------------\n",
            "page_content='have been thirsty （ = - ）. \n",
            "A new day! All residents receive a daily salary of  $100. \n",
            "Today, the daily available water can meet the needs of only \n",
            "one resident ...\n",
            "Today I will bid \n",
            "$40 because …\n",
            "8 1\n",
            " 8 1\n",
            " 8 1\n",
            " 8 1\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "$90\n",
            "$200\n",
            "7 2\n",
            "$80\n",
            "$ 200' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 20}\n",
            "--------------------------------------------------\n",
            "page_content='$100\n",
            "$ 200\n",
            "$50\n",
            "$ 130\n",
            "$80\n",
            "$ 200\n",
            "In yesterday, \n",
            "  ………\n",
            "7 2\n",
            " 7 2\n",
            " 10 1\n",
            " 7 2\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "Guessing 0.8 of the Average Survival Auction Game\n",
            "40 50 34 40 36\n",
            "Let's start the 2nd round.\n",
            "In last \n",
            "round …\n",
            " …\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "…\n",
            "31 36 24 25 21\n",
            "Closest to the target.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 21}\n",
            "--------------------------------------------------\n",
            "page_content='40-32=8 50-32=18 34-32=2 40-32=8 36-32=4\n",
            "Private Utilities\n",
            "Private Utilities\n",
            "5\n",
            " 0 8\n",
            "We have 5         , 5       , and 5        . \n",
            "Please negotiate how to distribute \n",
            "them.\n",
            "7\n",
            " 2 1\n",
            "I want more        , as a result, I can \n",
            "accept less       , so I propose …' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 22}\n",
            "--------------------------------------------------\n",
            "page_content='I propose …\n",
            "Agreement achieved. \n",
            "      has 4      , 4        , and 0        , \n",
            "while     has 1       , 1       , and 5      . \n",
            "The total utilities:  \n",
            "     :36,      : 45.          wins!\n",
            "Negotiation' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 23}\n",
            "--------------------------------------------------\n",
            "page_content='Figure 2: The illustration of three reasoning problems in dynamic, interactive environments in this paper. Left:\n",
            "Guessing 0.8 of the Average; Middle: Survival Auction Game; Right: Negotiation.\n",
            "their beliefs about one’s actions, and even further' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 24}\n",
            "--------------------------------------------------\n",
            "page_content='layers of recursive thinking.\n",
            "Inspired by K-level thinking, we propose a novel\n",
            "strategic reasoning framework termed \"K-Level\n",
            "Reasoning with LLMs (K-R).\" K-R organizes rea-\n",
            "soning into hierarchical levels and employs a recur-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 25}\n",
            "--------------------------------------------------\n",
            "page_content='sive mechanism to integrate varying strategic depth\n",
            "into decision-making. Specifically, it involves: 1)\n",
            "recursively anticipating others’ actions at varying\n",
            "levels of strategic depth with environmental context' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 26}\n",
            "--------------------------------------------------\n",
            "page_content='and historical public information, and 2) reason-\n",
            "ing the optimal action based on these anticipations.\n",
            "To the best of our knowledge, this is the first ap-\n",
            "proach to implementing varying levels of strategic\n",
            "depth in LLMs using a recursive mechanism and' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 27}\n",
            "--------------------------------------------------\n",
            "page_content='enables deeper reasoning in LLM agents through\n",
            "an algorithmic framework.\n",
            "We validate this framework through rigorous\n",
            "testing on four testbeds: two classical game theory\n",
            "problems and two social intelligence tasks. The' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 28}\n",
            "--------------------------------------------------\n",
            "page_content='game theory problems includes Guessing 0.8 of\n",
            "the Average (Figure 2 left) and Survival Auction\n",
            "Game (Mao et al., 2023) (Figure 2 middle). The\n",
            "social intelligence tasks includes Negotiation (Cao\n",
            "et al., 2018) (Figure 2 right) and SOTOPIA bench-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 29}\n",
            "--------------------------------------------------\n",
            "page_content='mark(Zhou et al., 2024). These settings serve as\n",
            "microcosms of the complex decision-making pro-\n",
            "cesses involved in strategic reasoning. Through\n",
            "extensive experiments, we demonstrate that our\n",
            "framework significantly outperforms existing rea-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 30}\n",
            "--------------------------------------------------\n",
            "page_content='soning methods and flexibly achieves varying lev-\n",
            "els of strategic depth. In addition to empirical evi-\n",
            "dence, we provide a theoretical analysis highlight-\n",
            "ing the benefits of K-R. We show that, leveraging\n",
            "the in-context learning capabilities of LLMs, K-R' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 31}\n",
            "--------------------------------------------------\n",
            "page_content='can effectively model opponents’ behavior using\n",
            "accumulated public and available opponent infor-\n",
            "mation.\n",
            "Furthermore, we align the strategic depth of\n",
            "LLMs with human participants (Nagel, 1995;\n",
            "Bosch-Domenech et al., 2002). Using human as' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 32}\n",
            "--------------------------------------------------\n",
            "page_content='anchors, we observe that the K-R significantly en-\n",
            "hances the strategic depth of LLMs from 0.25 to\n",
            "1.89. Notably, when K=3, the strategic depth (1.89)\n",
            "of the LLM closely approaches that of financial\n",
            "newspaper readers (1.91). This strongly indicates' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 33}\n",
            "--------------------------------------------------\n",
            "page_content='that K-R establishes higher order beliefs in LLMs\n",
            "for strategic reasoning.\n",
            "The contributions of this work are as follows:\n",
            "• We introduce K-R, a novel framework that\n",
            "extends k-level thinking to LLMs, enabling\n",
            "flexible strategic reasoning at varying depths' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 34}\n",
            "--------------------------------------------------\n",
            "page_content='through a recursive mechanism.\n",
            "• We conduct extensive evaluations, including\n",
            "game theory and social intelligence problems,\n",
            "demonstrating that K-R significantly outper-\n",
            "forms existing methods in terms of flexibility' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 35}\n",
            "--------------------------------------------------\n",
            "page_content='and effectiveness, across both closed-source\n",
            "and open-source models.\n",
            "• We provide an in-depth analysis of K-R, con-\n",
            "firming its ability to build higher order beliefs\n",
            "and enhance strategic reasoning. This lays\n",
            "a foundation for future research in theory of' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 36}\n",
            "--------------------------------------------------\n",
            "page_content='mind and strategic reasoning in LLMs.\n",
            "2 K-Level Reasoning with Large\n",
            "Language Models\n",
            "2.1 Methodology\n",
            "Strategic reasoning requires considering both the\n",
            "decision context and the possible actions of other\n",
            "2' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 37}\n",
            "--------------------------------------------------\n",
            "page_content='participants.We employ a multi-round normal form\n",
            "multi-participant game to introduce the proposed\n",
            "method. In this setting, an agent’s decision-making\n",
            "process is formalized as follows: each agent i se-\n",
            "lects an action at\n",
            "i from a set At\n",
            "i at timestep t. The' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 38}\n",
            "--------------------------------------------------\n",
            "page_content='payoff for agent i, resulting from the collective ac-\n",
            "tion profile At = (at\n",
            "1, at\n",
            "2, ..., at\n",
            "N ) and environment\n",
            "Et, is denoted as Ui(Et, At).\n",
            "At k = 1, agents decide based on environment\n",
            "Et without strategic anticipation:\n",
            "at,1\n",
            "i = arg max\n",
            "ai∈At\n",
            "i' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 39}\n",
            "--------------------------------------------------\n",
            "page_content='E[Ui(Et, ai)] (1)\n",
            "At higher level thinking (k ≥ 2), agent i simulates\n",
            "other agents operating at level k − 1 and adjusts\n",
            "their strategy accordingly2:\n",
            "at,k\n",
            "i = arg max\n",
            "ai∈At\n",
            "i\n",
            "E[Ui(Et, ai, ˆat,k−1\n",
            "−i )] (2)\n",
            "where ˆat,k−1' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 40}\n",
            "--------------------------------------------------\n",
            "page_content='−i are the predicted actions of other\n",
            "agents based on their k − 1 level reasoning.\n",
            "We propose a novel strategic reasoning frame-\n",
            "work with recursive mechanisms , termed “K-\n",
            "Level Reasoning with Large Language Models' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 41}\n",
            "--------------------------------------------------\n",
            "page_content='(K-R),” involving 1) recursively anticipating the\n",
            "actions ˆat,k\n",
            "−i of others at different thinking levels\n",
            "using environment contexts and historical public\n",
            "information, followed by 2) reasoning the optimal\n",
            "action at,k' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 42}\n",
            "--------------------------------------------------\n",
            "page_content='i based on anticipation of others’ actions.\n",
            "The K-Level Reasoning process is formulated as\n",
            "follows:\n",
            "1) Anticipation:\n",
            "ˆat,m\n",
            "j =\n",
            "(\n",
            "LLM(Et, Ht\n",
            "j) if m = 1\n",
            "LLM(Et, Ht\n",
            "j, ˆat,m−1\n",
            "−j ) if m >1 (3)\n",
            "where Ht\n",
            "j = {(E1, a1\n",
            "j ), (E2, a2\n",
            "j ), ...,(Et−1, at−1\n",
            "j )}' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 43}\n",
            "--------------------------------------------------\n",
            "page_content='represents public historical data of agent j, and m\n",
            "denotes the specified thinking level.\n",
            "2) Reasoning:\n",
            "at,k\n",
            "i = LLM(Et, Ht\n",
            "i , ˆat,k−1\n",
            "−i ) (4)\n",
            "Algorithm 1 outlines the implementation of K-R.\n",
            "This recursive method enables flexible and progres-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 44}\n",
            "--------------------------------------------------\n",
            "page_content='sively deeper strategic reasoning ( 1, 2, ..., k, k+\n",
            "1, ...), thereby enhancing higher order belief in\n",
            "LLM agents.\n",
            "2 To simplify the formulation, we assume that all oppo-\n",
            "nents are in the same thinking level. In practice, varying' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 45}\n",
            "--------------------------------------------------\n",
            "page_content='thinking level can be implemented.\n",
            "Algorithm 1 K-Level Reasoning with LLMs\n",
            "Require: Et: Current decision context at time t;\n",
            "Ht\n",
            "i : Historical information up to time t for\n",
            "agent i;\n",
            "K: Depth of strategic reasoning;\n",
            "Ensure: at,K' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 46}\n",
            "--------------------------------------------------\n",
            "page_content='i : Action for agent i at time t after\n",
            "K-level reasoning.\n",
            "1: Function K_REASONING(i, k):\n",
            "2: if k == 1then\n",
            "3: return LLM(Et, Ht\n",
            "i )\n",
            "4: else\n",
            "5: for each agent j ̸= i do\n",
            "6: ˆat,k−1\n",
            "j = K_REASONING(j, k− 1)\n",
            "7: end for\n",
            "8: return LLM(Et, Ht\n",
            "i , {ˆat,k−1' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 47}\n",
            "--------------------------------------------------\n",
            "page_content='j | j ̸= i})\n",
            "9: end if\n",
            "10: at,K\n",
            "i = K_REASONING(i, K)\n",
            "11: return at,K\n",
            "i\n",
            "2.2 Theoretical Analysis\n",
            "This section discusses the benefits from K-R from\n",
            "a theoretical perspective. We utilize the in-context\n",
            "learning capabilities of LLMs to effectively model' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 48}\n",
            "--------------------------------------------------\n",
            "page_content='opponents’ behavior. Suppose agent j’s decision-\n",
            "making process follows a hidden strategy θ∗\n",
            "j . Thus,\n",
            "agent j’s decision-making can be expressed as:\n",
            "P(at\n",
            "j | Et, θ∗\n",
            "j ) (5)\n",
            "The in-context learning of LLMs can be formally' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 49}\n",
            "--------------------------------------------------\n",
            "page_content='defined as implicit Bayesian inference(Xie et al.,\n",
            "2021); therefore, given the environment Et, the\n",
            "next action prediction conditioned on Ht\n",
            "j is:\n",
            "P(at\n",
            "j | Et, Ht\n",
            "j) =\n",
            "Z\n",
            "P(at\n",
            "j | Et, θj)P(θj | Ht\n",
            "j)dθj\n",
            "(6)\n",
            "As t → ∞, by the law of large numbers and proper-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 50}\n",
            "--------------------------------------------------\n",
            "page_content='ties of Bayesian updating, the posterior distribution\n",
            "concentrates around the true parameter θ∗\n",
            "j :\n",
            "P(θj | Ht\n",
            "j) → δ(θj − θ∗\n",
            "j ) (7)\n",
            "where δ is the Dirac delta function. Therefore,\n",
            "Z\n",
            "P(at\n",
            "j | Et, θj)P(θj | Ht\n",
            "j)dθj → P(at\n",
            "j | Et, θ∗\n",
            "j )\n",
            "(8)' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 51}\n",
            "--------------------------------------------------\n",
            "page_content='This implies that as the number of interactions\n",
            "increases, K-R can more accurately predict oppo-\n",
            "nents’ behavior.\n",
            "It is also worth noting that interaction data can-\n",
            "not be infinite, and in-context learning is related to\n",
            "3' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 52}\n",
            "--------------------------------------------------\n",
            "page_content='the performance of large language models (LLMs).\n",
            "Therefore, we empircally validate these hypotheses\n",
            "and reasoning in Section 5.2.\n",
            "3 Experiments: Game Theory\n",
            "To fairly compare the strategic reasoning capabili-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 53}\n",
            "--------------------------------------------------\n",
            "page_content='ties of LLMs, we first adopt two widely used game\n",
            "theory settings. These controlled, well-defined\n",
            "game theory problems provide a robust assessment\n",
            "of LLMs’ performance, with detailed setups out-\n",
            "lined in Appendix B.\n",
            "3.1 Task Definition and Metrics' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 54}\n",
            "--------------------------------------------------\n",
            "page_content='3.1.1 Guessing 0.8 of the Average (G0.8A)\n",
            "G0.8A (Figure 2 Left) is a classic game theory\n",
            "problem introduced by Alain Ledoux (Ledoux,\n",
            "1981). It involves 10-round games where each\n",
            "player selects a number between 1 and 100. The' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 55}\n",
            "--------------------------------------------------\n",
            "page_content='objective is to choose a number closest to 80% of\n",
            "the group’s average choice. The key idea is to guess\n",
            "how others will estimate the average and decide the\n",
            "number to submit. This concept is also illustrated\n",
            "in the Keynesian Beauty Contest (Keynes, 1936).' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 56}\n",
            "--------------------------------------------------\n",
            "page_content='This game mirrors the challenge of anticipating\n",
            "collective behavior in financial markets, where in-\n",
            "vestors must predict not only the value of an asset\n",
            "but also how others will value it in the future.\n",
            "The performance of the agent is evaluated using' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 57}\n",
            "--------------------------------------------------\n",
            "page_content='the Win Rate. Specifically, the Win Rate is calcu-\n",
            "lated based on the wins achieved by the agent in in-\n",
            "dividual round, rather than an entire game episode.\n",
            "3.1.2 Survival Auction Game (SAG)\n",
            "SAG (Figure 2 Middle) is derived from the Wa-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 58}\n",
            "--------------------------------------------------\n",
            "page_content='ter Allocation Challenge proposed in (Mao et al.,\n",
            "2023). Each resident’s goal is to survive a 10-day\n",
            "drought period by bidding for water resources and\n",
            "maintaining health points above zero. If a player\n",
            "successfully bids for water, they gain health points;' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 59}\n",
            "--------------------------------------------------\n",
            "page_content='otherwise, they lose health points. This integration\n",
            "of the auction system with the health points mecha-\n",
            "nism creates a dynamic environment where players\n",
            "must balance health and finances.\n",
            "We use Average Survival Roundmeasures the' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 60}\n",
            "--------------------------------------------------\n",
            "page_content='mean round in which a player remains active in the\n",
            "game.\n",
            "3.2 Base Techniques\n",
            "We adapt a variety of approaches, originally from\n",
            "traditional reasoning and agent benchmarks. These\n",
            "base techniques include:\n",
            "Standard Prompting (Direct): This is the con-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 61}\n",
            "--------------------------------------------------\n",
            "page_content='ventional prompting method in which the LLM\n",
            "generates the final answer (Action) in response to\n",
            "the given game setting prompt.\n",
            "Chain-of-Thought (CoT) (Wei et al., 2022): We\n",
            "employ the zero-shot Chain-of-Thought reasoning\n",
            "method (Kojima et al., 2022).' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 62}\n",
            "--------------------------------------------------\n",
            "page_content='Persona Prompting (Persona) (Deshpande\n",
            "et al., 2023): This technique modifies the standard\n",
            "prompting process by incorporating a “Game Ex-\n",
            "pert” persona to enhance the reasoning capabilities\n",
            "of the LLM.\n",
            "Reflexion (Reflect) (Shinn et al., 2023): This' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 63}\n",
            "--------------------------------------------------\n",
            "page_content='method refers to language agents with verbal rein-\n",
            "forcement learning and has been adapted for dy-\n",
            "namic tasks. Detailed modifications are explained\n",
            "in K.\n",
            "Self-Refine (Refine) (Madaan et al., 2023): This\n",
            "is a multi-round iterative reasoning approach where' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 64}\n",
            "--------------------------------------------------\n",
            "page_content='an additional LLM offers comments and adjust-\n",
            "ments prior to reaching a final decision. The dis-\n",
            "tinctions between Self-Refine and Reflect are elab-\n",
            "orated upon in the Appendix I.\n",
            "Prediction Chain of Thought (PCoT) : This' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 65}\n",
            "--------------------------------------------------\n",
            "page_content='strong baseline diverges from CoT by requiring the\n",
            "LLM to explicitly predict opponents’ actions be-\n",
            "fore making decisions. Unlike K-Level Reasoning,\n",
            "which involves a recursive approach, PCoT focuses\n",
            "on direct prediction based on context.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 66}\n",
            "--------------------------------------------------\n",
            "page_content='For implementation details and specific exam-\n",
            "ples, please refer to Appendix K.\n",
            "3.3 Experimental Settings\n",
            "We established a controllable environment and dis-\n",
            "tinguished between two roles: the player (primary\n",
            "focus) and the opponents. The player is equipped' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 67}\n",
            "--------------------------------------------------\n",
            "page_content='with a specific method, while all opponents use\n",
            "another reasoning approach. This well-defined set-\n",
            "ting allows for a clearer comparison of reasoning\n",
            "capabilities between methods.\n",
            "In G0.8A and SAG, there is one player and four' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 68}\n",
            "--------------------------------------------------\n",
            "page_content='opponents for each game. Experiments for each\n",
            "setting are repeated 10 times and have passed the\n",
            "significance test (Appendix H), and each experi-\n",
            "ment consists of a 10-round game.\n",
            "All methods in main experiments were imple-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 69}\n",
            "--------------------------------------------------\n",
            "page_content='mented using GPT-4 (Achiam et al., 2023) (gpt4-\n",
            "32k), with the temperature set at 0.7 and the top-p\n",
            "set at 0.9. We also conducted experiments with\n",
            "open-source LLMs, Details of which are provided\n",
            "in Appendix E. Unless specified otherwise, the\n",
            "4' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 70}\n",
            "--------------------------------------------------\n",
            "page_content='level of thinking in K-Level Reasoning is set to\n",
            "K=2.\n",
            "3.4 Results\n",
            "To distinguish between “Player” and “Opponent”\n",
            "in the table, the headers for Player (bold) and Op-\n",
            "ponents (italics) are formatted accordingly.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 71}\n",
            "--------------------------------------------------\n",
            "page_content='Table 1: Win Rate of the player against different oppo-\n",
            "nents in G0.8A game.\n",
            "Direct CoT Persona Reflect Refine PCoT K-R\n",
            "Direct 0.43 0.67 0.62 0.53 0.43 0.61 0.82\n",
            "CoT 0.07 0.32 0.35 0.14 0.22 0.45 0.63\n",
            "Persona0.05 0.37 0.29 0.05 0.37 0.11 0.46' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 72}\n",
            "--------------------------------------------------\n",
            "page_content='Reflect 0.42 0.68 0.63 0.39 0.64 0.74 0.78\n",
            "Refine 0.10 0.34 0.32 0.31 0.23 0.22 0.46\n",
            "PCoT 0.03 0.44 0.52 0.21 0.51 0.54 0.85\n",
            "K-R 0.04 0.15 0.14 0.04 0.17 0.14 0.52\n",
            "Average0.16\n",
            "± 0.18\n",
            "0.32\n",
            "± 0.19\n",
            "0.41\n",
            "± 0.18\n",
            "0.24\n",
            "± 0.18\n",
            "0.37\n",
            "± 0.17\n",
            "0.40\n",
            "± 0.25\n",
            "0.65\n",
            "± 0.17' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 73}\n",
            "--------------------------------------------------\n",
            "page_content='Table 2: Average Survival Round of the player against\n",
            "different opponents in Survival Auction Game.\n",
            "Direct CoT Persona Reflect Refine PCoT K-R\n",
            "Direct 5.90 7.00 7.50 4.70 8.70 6.60 9.40\n",
            "CoT 5.70 6.50 5.30 4.00 8.10 5.30 10.00' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 74}\n",
            "--------------------------------------------------\n",
            "page_content='Persona5.70 7.70 7.40 5.20 6.30 7.20 9.30\n",
            "Reflect 9.40 9.40 9.90 5.20 8.60 8.20 10.00\n",
            "Refine 6.30 6.40 8.10 4.30 8.20 5.30 7.90\n",
            "PCoT 8.50 9.60 9.90 6.30 8.50 6.20 9.70\n",
            "K-R 4.10 5.50 5.00 4.04 5.70 4.40 6.80\n",
            "Average6.51\n",
            "± 1.82\n",
            "7.44\n",
            "± 1.55\n",
            "7.59\n",
            "± 1.95\n",
            "4.82' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 75}\n",
            "--------------------------------------------------\n",
            "page_content='± 0.82\n",
            "7.73\n",
            "± 1.21\n",
            "6.17\n",
            "± 1.29\n",
            "9.01\n",
            "± 1.21\n",
            "Table 1 presents Win Rate of players utilizing\n",
            "different methods against various opponents in the\n",
            "G0.8A game. Notably, the K-R method demon-\n",
            "strates a superior Win Rate of 0.65, significantly' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 76}\n",
            "--------------------------------------------------\n",
            "page_content='exceeding the win rates of the other strategies. Ta-\n",
            "ble 2 provides insights into the Average Survival\n",
            "Round of players across different auction game\n",
            "strategies in SAG, with the K-R method again\n",
            "standing out. The K-R method achieves an av-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 77}\n",
            "--------------------------------------------------\n",
            "page_content='erage survival round of 9.01, considerably higher\n",
            "than all other methods.\n",
            "The experiment result underscores the effective-\n",
            "ness of the K-R method in enhancing player strat-\n",
            "egy, suggesting its strategic superiority in the con-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 78}\n",
            "--------------------------------------------------\n",
            "page_content='text of this game. Its effectiveness lies in its ability\n",
            "to anticipate opponent moves, outperforming other\n",
            "prompting methods.\n",
            "The performance of Reflect did not demonstrate\n",
            "the effectiveness of the reasoning method. We hy-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 79}\n",
            "--------------------------------------------------\n",
            "page_content='pothesize that this is due to the fact that, in dynamic\n",
            "environments, Reflect on the experiences summa-\n",
            "rized from the previous round (Shinn et al., 2023)\n",
            "may not be applicable to the subsequent round of\n",
            "the game. Furthermore, in both games, Refine did' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 80}\n",
            "--------------------------------------------------\n",
            "page_content='not show an advantage over CoT and was signif-\n",
            "icantly lower than K-R. This is because Refine\n",
            "involves adjustments based on one’sown strategy.\n",
            "However, these adjustments do not explicitly con-\n",
            "sider the hidden strategies of the opponent’s behav-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 81}\n",
            "--------------------------------------------------\n",
            "page_content='ior, rendering them inapplicable against opponents\n",
            "employing different strategies.\n",
            "4 Experiments: Social Intelligence\n",
            "We then evaluate K-R in two social intelligence\n",
            "benchmarks to assess its performance in more open-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 82}\n",
            "--------------------------------------------------\n",
            "page_content='ended realistic scenarios. Compared to the abstract\n",
            "and theoretical settings of Game Theory, these sce-\n",
            "narios involve richer contextual backgrounds and\n",
            "complicated goal pursuits, which better demon-\n",
            "strate the value of LLM-based agents in practical' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 83}\n",
            "--------------------------------------------------\n",
            "page_content='applications, such as in chatbots and strategic deci-\n",
            "sion making.\n",
            "4.1 Task Definition and Metrics\n",
            "4.1.1 Negotiation (NEG)\n",
            "NEG (Figure 2 Right)(Cao et al., 2018; Duan et al.,\n",
            "2024) is an open-ended and realistic task. In this' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 84}\n",
            "--------------------------------------------------\n",
            "page_content='setting, two agents are presented with three types\n",
            "of items: peppers, cherries, and strawberries. Each\n",
            "agent has private utility values for these items and\n",
            "must negotiate to allocate the public item pool.\n",
            "The agent who secures more utility upon reach-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 85}\n",
            "--------------------------------------------------\n",
            "page_content='ing an agreement wins the game, and we calculated\n",
            "the Win Rate to assess the performance of different\n",
            "agents.\n",
            "4.1.2 SOTOPIA Benchmark\n",
            "SOTOPIA (Zhou et al., 2024) is an open-ended en-\n",
            "vironment to simulate complex social interactions' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 86}\n",
            "--------------------------------------------------\n",
            "page_content='between artificial agents and evaluate their social in-\n",
            "telligence. It includes a variety of social scenarios,\n",
            "and each scenario includes a context background,\n",
            "and private social goals of each agent. Meanwhile,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 87}\n",
            "--------------------------------------------------\n",
            "page_content='each agent has a character profiles which consists\n",
            "of name, gender, personality, occupation, etc.\n",
            "For each episode, agents are scored at the end\n",
            "of the interaction along each of seven dimensions\n",
            "in SOTOPIA-Eval, including Goal Completion' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 88}\n",
            "--------------------------------------------------\n",
            "page_content='(GOAL), Believability (BEL), Knowledge (KNO),\n",
            "Secret ( SEC), Relationship ( REL), Social Rules\n",
            "(SOC), Financial and Material Benefits (FIN).\n",
            "5' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 89}\n",
            "--------------------------------------------------\n",
            "page_content='4.2 Experimental Settings\n",
            "We employed the majority of the reasoning ap-\n",
            "proaches introduced in Section 3.2 as baseline mod-\n",
            "els for comparison.\n",
            "In NEG, the experiments followed the settings\n",
            "from (Cao et al., 2018; Duan et al., 2024). There is' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 90}\n",
            "--------------------------------------------------\n",
            "page_content='one player and one opponent for each game. We\n",
            "test the performance of the Baselines and K-Level\n",
            "Reasoning in 100 repeated independent games. To\n",
            "eliminate positional advantages, we swapped the\n",
            "positions of each player for each setting. To en-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 91}\n",
            "--------------------------------------------------\n",
            "page_content='sure the reliability, three trials were conducted, and\n",
            "the results are reported as averages with standard\n",
            "deviation.\n",
            "Meanwhile, We adhered to the SOTOPIA-hard\n",
            "(Zhou et al., 2024) setup comprising a total of 100' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 92}\n",
            "--------------------------------------------------\n",
            "page_content='episodes, which is commonly found to be challeng-\n",
            "ing for LLMs, and utilize a fixed GPT-4o based\n",
            "agent as partner. Additionally, to evaluate the\n",
            "agents’ scores, we utilized GPT-4 as the assessment\n",
            "model, as it has been determined by SOTOPIA' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 93}\n",
            "--------------------------------------------------\n",
            "page_content='benchmark (Zhou et al., 2024) to serve as a reliable\n",
            "serves as a proxy for human judgments in evalu-\n",
            "ating model performance across most dimensions\n",
            "and for human performance on the GOAL dimen-\n",
            "sion.\n",
            "4.3 Results' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 94}\n",
            "--------------------------------------------------\n",
            "page_content='Table 3: Win Rate of the player against opponent in\n",
            "Negotiation Setting.\n",
            "Direct CoT Persona Reflect Refine PCoT K-R\n",
            "Direct 50.00 61.34 49.58 66.67 65.83 63.03 70.83\n",
            "CoT 38.66 50.00 36.67 45.83 45.76 47.27 55.36' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 95}\n",
            "--------------------------------------------------\n",
            "page_content='Persona 50.42 63.33 50.00 70.00 67.50 62.50 70.83\n",
            "Reflection33.33 54.17 30.00 50.00 57.14 55.00 55.00\n",
            "Refine 34.17 54.24 32.50 42.86 50.00 55.77 54.55\n",
            "PCoT 36.97 52.73 37.50 45.00 44.23 50.00 57.00\n",
            "K-R 29.17 44.64 29.17 45.00 45.45 43.00 50.00' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 96}\n",
            "--------------------------------------------------\n",
            "page_content='Average38.96\n",
            "±2.53\n",
            "54.35\n",
            "±0.50\n",
            "37.92\n",
            "±5.84\n",
            "52.19\n",
            "±1.73\n",
            "53.70\n",
            "±4.41\n",
            "53.80\n",
            "±4.34\n",
            "59.08\n",
            "± 2.20\n",
            "The results presented in Table 3 and Table 4 il-\n",
            "lustrate the effectiveness of the K-Level Reasoning\n",
            "in the context of NEG and SOTOPIA-hard settings,\n",
            "respectively.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 97}\n",
            "--------------------------------------------------\n",
            "page_content='In NEG, the K-R method demonstrates a notable\n",
            "win rate of 59.08%, positioning it significantly\n",
            "above the average win rates achieved by other meth-\n",
            "ods. This indicates that, in most cases, the propos-\n",
            "als generated through K-Level Reasoning are more' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 98}\n",
            "--------------------------------------------------\n",
            "page_content='advantageous to itself, as well as suggesting a ten-\n",
            "dency to accept the opponent’s proposals when the\n",
            "Table 4: SOTOPIA-Eval of the player against opponent\n",
            "in SOTOPIA-hard.\n",
            "Direct CoT Refine K-R Direct CoT Refine K-R\n",
            "Metric [GPT-4o] [LLaMA-3.1-70B]' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 99}\n",
            "--------------------------------------------------\n",
            "page_content='BEL[0–10] 8.97 9.00 9.00 8.97 8.88 8.85 8.90 8.97\n",
            "REL[-5–5] 2.38 2.40 2.27 2.67 1.38 1.18 0.82 2.40\n",
            "KNO[0–10] 6.05 6.05 6.25 6.25 5.88 5.53 5.33 6.12\n",
            "SEC[-10-0] 0.00 -0.05 0.00 0.00 -0.28 -0.25 -0.180.00' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 100}\n",
            "--------------------------------------------------\n",
            "page_content='SOC[-10–0] -0.05 0.00 -0.05 0.00 -0.70 -0.72 -0.640.00\n",
            "FIN[-5–5] 0.90 0.78 0.80 0.72 0.38 0.35 -0.08 0.75\n",
            "GOAL[0–10] 6.356.60 6.15 6.47 5.35 5.40 4.95 6.38\n",
            "Overall 3.51\n",
            "± 0.09\n",
            "3.54\n",
            "± 0.08\n",
            "3.49\n",
            "± 0.08\n",
            "3.59\n",
            "± 0.09\n",
            "2.98\n",
            "± 0.23\n",
            "2.90\n",
            "± 0.26\n",
            "2.73\n",
            "± 0.25\n",
            "3.52' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 101}\n",
            "--------------------------------------------------\n",
            "page_content='± 0.13\n",
            "perceived benefits are substantial.\n",
            "The results from SOTOPIA reveal several in-\n",
            "triguing findings. Firstly, while K-R demonstrates\n",
            "some improvement compared to other methods, the\n",
            "results are not statistically significant. We hypothe-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 102}\n",
            "--------------------------------------------------\n",
            "page_content='size that this may be due to the inherent tendency\n",
            "of GPT-4 based models to assign higher scores to\n",
            "responses generated by GPT-4 based agents. No-\n",
            "tably, we observed that employing agents based on\n",
            "LLaMA 3.1 70B with K-R can lead to significant' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 103}\n",
            "--------------------------------------------------\n",
            "page_content='performance enhancements. Meanwhile, the over-\n",
            "all metrics indicate that K-R achieves performance\n",
            "levels comparable to those of the GPT-4 model,\n",
            "highlighting K-R’s potential in the realm of social\n",
            "intelligence.\n",
            "5 Discussions' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 104}\n",
            "--------------------------------------------------\n",
            "page_content='5.1 Does K-R Efficiently Establish a Higher\n",
            "Order Belief in LLMs?\n",
            "Table 5: Human performance in G2/3A.\n",
            "Experiments Lab Classroom Take-home TheoristsInternetNewsgroupNewspaper\n",
            "Mean Choice 35.13 26.84 25.20 17.15 22.16 23.08' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 105}\n",
            "--------------------------------------------------\n",
            "page_content='Strategic Depth 0.87 1.53 1.682.63 2.01 1.91\n",
            "Table 6: LLM performance in G0.8A in the first round.\n",
            "Method Direct CoT Persona Refine Reflect PCoT KR[k=2] KR[k=3]\n",
            "Mean Choice 47.29 37.8 41.0 41.0 45.2 44.0 38.42 32.79' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 106}\n",
            "--------------------------------------------------\n",
            "page_content='Strategic Depth 0.25 1.25 0.89 0.89 0.45 0.57 1.181.89\n",
            "As a classic game theory issue, the G0.8A\n",
            "problem has garnered significant research interest\n",
            "across various disciplines. We reference the exper-\n",
            "imental results of the classic research among hu-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 107}\n",
            "--------------------------------------------------\n",
            "page_content='man participants (Nagel, 1995; Bosch-Domenech\n",
            "et al., 2002) as anchor points and present the av-\n",
            "erage decisions made by the K-Level Reasoning\n",
            "method (GPT-4) in the first round. Through this\n",
            "6' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 108}\n",
            "--------------------------------------------------\n",
            "page_content='comparison, we can observe the relative relation-\n",
            "ship between human cognitive levels and LLMs\n",
            "under different reasoning methods. The specific\n",
            "calculation method on strategic depth is described\n",
            "in Appendix C. The performance of humans and' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 109}\n",
            "--------------------------------------------------\n",
            "page_content='LLMs is shown in the Table 5 and Table 6\n",
            "From these observations, we can conclude that\n",
            "even when employing SOTA models, the strategic\n",
            "depth of GPT-4 under Direct Prompt (0.25) cannot\n",
            "compete with that of lower-strategic-capability un-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 110}\n",
            "--------------------------------------------------\n",
            "page_content='dergraduate students in laboratory settings (0.87).\n",
            "Furthermore, the K-Level reasoning approach sig-\n",
            "nificantly enhances the reasoning depth of large\n",
            "language models, increasing it from 0.25 to 1.89,\n",
            "and the strategic depth of the large language model' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 111}\n",
            "--------------------------------------------------\n",
            "page_content='(1.89) approaches that of a group of financial news-\n",
            "paper readers (1.91) when K=3.\n",
            "5.2 K-Level Reasoning Leads to More\n",
            "Accurate Predictions About Opponents\n",
            "Round 2 Round 4 Round 6 Round 8 Round 10\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20Prediction Deviation\n",
            "K-R Prediction' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 112}\n",
            "--------------------------------------------------\n",
            "page_content='PCoT Prediction\n",
            "Figure 3: The deviation in prediction during the G0.8A\n",
            "between PCoT and K-Level Reasoning.\n",
            "Since K-R involves an intermediate step of mod-\n",
            "eling the opponent’s behavior, we examine the pro-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 113}\n",
            "--------------------------------------------------\n",
            "page_content='gression of prediction accuracy. Figure 3 illustrates\n",
            "the prediction deviation between K-R and PCoT\n",
            "in G0.8A. K-R exhibits higher prediction accuracy\n",
            "than PCoT from Round 1, starting with more pre-\n",
            "cise and less random predictions. Moreover, the' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 114}\n",
            "--------------------------------------------------\n",
            "page_content='predictions converge quickly and become highly\n",
            "accurate in the second half of the game. This trend\n",
            "highlights the LLM’s increasing proficiency in un-\n",
            "derstanding higher order belief with more game-\n",
            "play context. Essentially, K-R instantiates new' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 115}\n",
            "--------------------------------------------------\n",
            "page_content='sessions to compute the opponent’s future actions.\n",
            "This approach leverages the in-context learning ca-\n",
            "pabilities of LLMs more effectively than PCoT’s\n",
            "prediction process (as theoretically discussed in\n",
            "Section 2.2). As a result, K-R achieves better pre-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 116}\n",
            "--------------------------------------------------\n",
            "page_content='diction accuracy.\n",
            "5.3 Better Reasoning Methodology vs.\n",
            "Stronger Foundation Model\n",
            "There is a consensus that LLMs trained with more\n",
            "data and possessing larger parameter sizes demon-\n",
            "strate stronger reasoning capabilities. We explore' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 117}\n",
            "--------------------------------------------------\n",
            "page_content='whether K-Level Reasoning can significantly en-\n",
            "hance the strategic reasoning abilities of relatively\n",
            "weaker LLMs. To investigate, we conducted ex-\n",
            "periments comparing the performance of K-R with\n",
            "GPT-3.5 (K-R[GPT-3.5]) against other reasoning' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 118}\n",
            "--------------------------------------------------\n",
            "page_content='methods based on GPT-4. All experiments were\n",
            "repeated 10 times.\n",
            "Table 7: A comparison of K-Level Reasoning with GPT-\n",
            "3.5 and other reasoning approaches with GPT-4. For\n",
            "the Guessing 0.8 of the Average, we report the win rate;' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 119}\n",
            "--------------------------------------------------\n",
            "page_content='for the Survival Auction Game, we report the average\n",
            "survival round.\n",
            "Guessing 0.8 of the Average Survival Auction Game\n",
            "OpponentDirect K-R Direct K-R Direct K-R Direct K-R\n",
            "[GPT-4][GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4] [GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4]' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 120}\n",
            "--------------------------------------------------\n",
            "page_content='Direct 0.18 0.18 0.43 0.82 5.00 9.40 5.90 9.40\n",
            "CoT 0.14 0.37 0.07 0.63 5.30 8.10 5.7010.00\n",
            "Persona0.10 0.23 0.05 0.46 5.00 7.50 5.70 9.30\n",
            "Reflect 0.24 0.38 0.42 0.78 5.00 8.50 9.4010.00\n",
            "Refine 0.14 0.13 0.10 0.46 5.10 6.70 6.30 7.90' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 121}\n",
            "--------------------------------------------------\n",
            "page_content='PCoT 0.19 0.46 0.03 0.85 4.10 6.80 8.50 9.70\n",
            "Average0.16 0.29 0.18 0.67 4.92 7.83 6.92 9.38\n",
            "From the results in Table 7, we observe that K-\n",
            "R[GPT-3.5] outperforms the standard prompting\n",
            "method of GPT-4 (Direct[GPT4]) from average per-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 122}\n",
            "--------------------------------------------------\n",
            "page_content='formance. Furthermore, when competing against\n",
            "opponents using reasoning methods on GPT-4, K-\n",
            "R[GPT-3.5] demonstrates remarkable capabilities.\n",
            "K-R, with its excellent restoration of the rival’s\n",
            "perspective, enhances the LLM’s ability in compet-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 123}\n",
            "--------------------------------------------------\n",
            "page_content='itive environments. Additionally, we compared the\n",
            "performance of the open-source model LLAMA2-\n",
            "7B with GPT-3.5/4 in Appendix E, finding that\n",
            "K-R significantly enhances reasoning in interactive\n",
            "contexts across different LLMs.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 124}\n",
            "--------------------------------------------------\n",
            "page_content='5.4 The Deeper Thinking Level, the Better\n",
            "Strategic Performance?\n",
            "Table 8: Comparison between K-Level Reasoning[K=2]\n",
            "and K-Level Reasoning[K=3] in the two games.\n",
            "Guessing 0.8 of the Average Survival Auction Game' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 125}\n",
            "--------------------------------------------------\n",
            "page_content='OpponentDirect K-R[K=2] K-R[K=3] Direct K-R[K=2] K-R[K=3]\n",
            "Direct 0.43 0.82 0.77 (-0.05)5.90 9.40 9.40(+0.00)K-R[K=2]0.04 0.52 0.60(+0.08)4.10 6.80 8.30(+1.50)\n",
            "K-R models opponents’ thinking processes re-\n",
            "cursively. We examine how thinking level affect\n",
            "7' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 126}\n",
            "--------------------------------------------------\n",
            "page_content='reasoning outcomes by comparing K-R[K=2] and\n",
            "K-R[K=3] in two games. The results, detailed in Ta-\n",
            "ble 8, reveal the impact of increased thinking level.\n",
            "Against the Direct method (first-level thinking), K-\n",
            "R[K=3] showed a decreased win rate in G0.8A but' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 127}\n",
            "--------------------------------------------------\n",
            "page_content='maintained performance in SAG, suggesting pos-\n",
            "sible overthinking. However, K-R[K=3] improved\n",
            "significantly against K-R[K=2] in both games. It\n",
            "suggests that the key factor in K-R is the relative\n",
            "depth of thought compared to the opponent. A one-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 128}\n",
            "--------------------------------------------------\n",
            "page_content='level deeper approach offers a strategic advantage,\n",
            "but advancing two levels may lead to diminishing\n",
            "returns due to over-anticipation. In interactive en-\n",
            "vironments, identifying opponents’ thinking levels' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 129}\n",
            "--------------------------------------------------\n",
            "page_content='is difficult. Adapting to varying levels and using K-\n",
            "Level Reasoning for deeper analysis is a valuable\n",
            "direction for future research.\n",
            "Additionally, a higher thinking level with the re-\n",
            "cursive prompting implementation increases com-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 130}\n",
            "--------------------------------------------------\n",
            "page_content='putational cost. The computational cost of K-R is\n",
            "thoroughly discussed in Appendix G.\n",
            "6 Related Work\n",
            "6.1 Reasoning with LLMs\n",
            "Large Language Models (LLMs) excel in diverse\n",
            "complex reasoning tasks, such as mathematical' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 131}\n",
            "--------------------------------------------------\n",
            "page_content='(Miao et al., 2021; Patel et al., 2021), common\n",
            "sense (Talmor et al., 2022; Bhakthavatsalam et al.,\n",
            "2021), and symbolic reasoning (Srivastava et al.,\n",
            "2022; Suzgun et al., 2022). A notable reason-\n",
            "ing approach involves breaking down complex' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 132}\n",
            "--------------------------------------------------\n",
            "page_content='questions into a series of intermediate steps, a\n",
            "technique known as the Chain-of-Thought (CoT)\n",
            "method (Wei et al., 2022; Kojima et al., 2022).\n",
            "Subsequently, some works have emerged to ex-\n",
            "tend CoT, with innovations like Tree of Thought' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 133}\n",
            "--------------------------------------------------\n",
            "page_content='(ToT) (Yao et al., 2023), Graph of Thought (GoT)\n",
            "(Besta et al., 2023) and Skeleton-of-thought (Ning\n",
            "et al., 2023). Besides, approaches like Self-Refine\n",
            "(Madaan et al., 2023) and Reflexion (Shinn et al.,\n",
            "2023) enhance CoT’s consistency by having LLMs' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 134}\n",
            "--------------------------------------------------\n",
            "page_content='review and refine their responses. Moreover, recent\n",
            "research has revealed that integrating persona in-\n",
            "formation into LLMs significantly improves their\n",
            "reasoning processes (Deshpande et al., 2023). A\n",
            "series of studies (Fu et al., 2023; Wang et al., 2023)' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 135}\n",
            "--------------------------------------------------\n",
            "page_content='have been conducted to incorporate more persona\n",
            "information, aiming to enhance the rationality and\n",
            "knowledge ability of the LLM reasoning process.\n",
            "These methods have been applied to various static\n",
            "tasks, but have not been adequately evaluated in' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 136}\n",
            "--------------------------------------------------\n",
            "page_content='dynamic problems (multi-agent environment) to\n",
            "validate their efficacy in reasoning capabilities.\n",
            "6.2 Strategic Reasoning within Multiple\n",
            "Agent System\n",
            "Dynamic problems arise when multiple participants\n",
            "are involved in multi-round interactions. One key' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 137}\n",
            "--------------------------------------------------\n",
            "page_content='factor is the simultaneous interactions of multiple\n",
            "participants with the environment. Unlike single-\n",
            "agent systems, multiple agent system (MAS) en-\n",
            "counters a broader range of issues and challenges,\n",
            "as noted by (Wong et al., 2021), including compu-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 138}\n",
            "--------------------------------------------------\n",
            "page_content='tational complexity (Ding and Dong, 2020), nonsta-\n",
            "tionarity (Papoudakis et al., 2019), partial observ-\n",
            "ability (Mahajan et al., 2019; Foerster et al., 2016),\n",
            "and challenges in credit assignment (Sunehag et al.,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 139}\n",
            "--------------------------------------------------\n",
            "page_content='2017). Particularly, in the context of inference us-\n",
            "ing LLMs, the nonstationarity of the environment\n",
            "poses a distinct challenge.\n",
            "Recently, research on LLMs in strategic reason-\n",
            "ing has been conducted across various MAS includ-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 140}\n",
            "--------------------------------------------------\n",
            "page_content='ing social behavior(Zhou et al., 2024; Hua et al.,\n",
            "2023), economic simulations(Zhao et al., 2023; Li\n",
            "et al., 2023), game theory(Duan et al., 2024; Xu\n",
            "et al., 2023a), and game playing(Ma et al., 2023;\n",
            "Xu et al., 2023b). To enhance the performance of' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 141}\n",
            "--------------------------------------------------\n",
            "page_content='LLMs in strategic reasoning scenarios, researchers\n",
            "have utilized the concepts of Theory of Mind (ToM)\n",
            "(Gandhi et al., 2023; Guo et al., 2023) and Re-\n",
            "inforcement Learning (Xu et al., 2023c; Zhang\n",
            "et al., 2024a) to optimize the reasoning processes of' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 142}\n",
            "--------------------------------------------------\n",
            "page_content='LLMs. These approaches involve prompting LLMs\n",
            "to recognize the intricacies of strategic tasks, like\n",
            "our proposed Prediction Chain-of-Thought base-\n",
            "line. However, our experimental results indicate\n",
            "that this approach fails to establish a clear cogni-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 143}\n",
            "--------------------------------------------------\n",
            "page_content='tive hierarchy necessary for recursive and deeper\n",
            "strategic thinking.\n",
            "7 Conclusion\n",
            "This paper represents a significant stride in under-\n",
            "standing and enhancing the strategic reasoning ca-\n",
            "pabilities of LLMs. We propose “K-Level Rea-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 144}\n",
            "--------------------------------------------------\n",
            "page_content='soning with LLMs.” This innovative approach\n",
            "leverages recursive mechanisms to achieve vary-\n",
            "ing thinking level within LLMs, enabling them\n",
            "to engage in deeper strategic thinking. Through\n",
            "extensive experiments, we validate the advantage' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 145}\n",
            "--------------------------------------------------\n",
            "page_content='offered by this method. It establishes a foundation\n",
            "for future research into theory of mind and strategic\n",
            "reasoning in LLMs.\n",
            "8' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 146}\n",
            "--------------------------------------------------\n",
            "page_content='8 Limitations\n",
            "We validate the effectiveness of the K-Level Rea-\n",
            "soning framework from two perspectives: game\n",
            "theory and social intelligence. While our exper-\n",
            "imental results provide substantial evidence sup-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 147}\n",
            "--------------------------------------------------\n",
            "page_content='porting the framework’s validity, further research\n",
            "is necessary to explore the performance of large\n",
            "language models (LLMs) in few-shot agent model-\n",
            "ing (He et al., 2016) across various environments,\n",
            "strategic factors, and action sets.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 148}\n",
            "--------------------------------------------------\n",
            "page_content='Additionally, K-R predicts opponents’ most\n",
            "likely behavior by initiating a new LLM inference\n",
            "session. The recursive mechanism employed to\n",
            "achieve varying levels of strategic depth inevitably\n",
            "increases computational cost. Appendix G pro-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 149}\n",
            "--------------------------------------------------\n",
            "page_content='vides a detailed discussion on how K-R relates\n",
            "to this rise in computational cost and compares it\n",
            "across different reasoning methods. Despite the in-\n",
            "creased demands, K-R outperforms other methods\n",
            "with comparable computational costs.\n",
            "References' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 150}\n",
            "--------------------------------------------------\n",
            "page_content='Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\n",
            "Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\n",
            "Diogo Almeida, Janko Altenschmidt, Sam Altman,\n",
            "Shyamal Anadkat, et al. 2023. Gpt-4 technical report.\n",
            "arXiv preprint arXiv:2303.08774.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 151}\n",
            "--------------------------------------------------\n",
            "page_content='Maciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\n",
            "stenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\n",
            "Lehmann, Michal Podstawski, Hubert Niewiadomski,\n",
            "Piotr Nyczyk, et al. 2023. Graph of thoughts: Solv-\n",
            "ing elaborate problems with large language models.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 152}\n",
            "--------------------------------------------------\n",
            "page_content='arXiv preprint arXiv:2308.09687.\n",
            "Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar\n",
            "Khot, Bhavana Dalvi Mishra, Kyle Richardson,\n",
            "Ashish Sabharwal, Carissa Schoenick, Oyvind\n",
            "Tafjord, and Peter Clark. 2021. Think you have' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 153}\n",
            "--------------------------------------------------\n",
            "page_content='solved direct-answer question answering? try arc-\n",
            "da, the direct-answer ai2 reasoning challenge. arXiv\n",
            "preprint arXiv:2102.03315.\n",
            "Antoni Bosch-Domenech, Jose G Montalvo, Rosemarie\n",
            "Nagel, and Albert Satorra. 2002. One, two,(three),' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 154}\n",
            "--------------------------------------------------\n",
            "page_content='infinity,. . . : Newspaper and lab beauty-contest ex-\n",
            "periments. American Economic Review, 92(5):1687–\n",
            "1701.\n",
            "Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z\n",
            "Leibo, Karl Tuyls, and Stephen Clark. 2018. Emer-\n",
            "gent communication through negotiation. arXiv' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 155}\n",
            "--------------------------------------------------\n",
            "page_content='preprint arXiv:1804.03980.\n",
            "Brandon Cui, Hengyuan Hu, Luis Pineda, and Jakob\n",
            "Foerster. 2021. K-level reasoning for zero-shot coor-\n",
            "dination in hanabi. Advances in Neural Information\n",
            "Processing Systems, 34:8215–8228.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 156}\n",
            "--------------------------------------------------\n",
            "page_content='Ameet Deshpande, Vishvak Murahari, Tanmay Rajpuro-\n",
            "hit, Ashwin Kalyan, and Karthik Narasimhan. 2023.\n",
            "Toxicity in chatgpt: Analyzing persona-assigned lan-\n",
            "guage models. arXiv preprint arXiv:2304.05335.\n",
            "Zihan Ding and Hao Dong. 2020. Challenges of rein-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 157}\n",
            "--------------------------------------------------\n",
            "page_content='forcement learning. Deep Reinforcement Learning:\n",
            "Fundamentals, Research and Applications , pages\n",
            "249–272.\n",
            "Jinhao Duan, Renming Zhang, James Diffenderfer,\n",
            "Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin,\n",
            "Mohit Bansal, Tianlong Chen, and Kaidi Xu. 2024.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 158}\n",
            "--------------------------------------------------\n",
            "page_content='Gtbench: Uncovering the strategic reasoning limita-\n",
            "tions of llms via game-theoretic evaluations. arXiv\n",
            "preprint arXiv:2402.12348.\n",
            "Jakob Foerster, Ioannis Alexandros Assael, Nando\n",
            "De Freitas, and Shimon Whiteson. 2016. Learning' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 159}\n",
            "--------------------------------------------------\n",
            "page_content='to communicate with deep multi-agent reinforcement\n",
            "learning. Advances in neural information processing\n",
            "systems, 29.\n",
            "Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.\n",
            "2023. Improving language model negotiation with' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 160}\n",
            "--------------------------------------------------\n",
            "page_content='self-play and in-context learning from ai feedback.\n",
            "arXiv preprint arXiv:2305.10142.\n",
            "Kanishk Gandhi, Dorsa Sadigh, and Noah D Goodman.\n",
            "2023. Strategic reasoning with language models.\n",
            "arXiv preprint arXiv:2305.19165.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 161}\n",
            "--------------------------------------------------\n",
            "page_content='Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen\n",
            "Lin, Yusuke Iwasawa, and Yutaka Matsuo. 2023.\n",
            "Suspicion-agent: Playing imperfect information\n",
            "games with theory of mind aware gpt-4. arXiv\n",
            "preprint arXiv:2309.17277.\n",
            "He He, Jordan Boyd-Graber, Kevin Kwok, and Hal' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 162}\n",
            "--------------------------------------------------\n",
            "page_content='Daumé III. 2016. Opponent modeling in deep rein-\n",
            "forcement learning. In International conference on\n",
            "machine learning, pages 1804–1813. PMLR.\n",
            "Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei,\n",
            "Jianchao Ji, Yingqiang Ge, Libby Hemphill, and' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 163}\n",
            "--------------------------------------------------\n",
            "page_content='Yongfeng Zhang. 2023. War and peace (waragent):\n",
            "Large language model-based multi-agent simulation\n",
            "of world wars. arXiv preprint arXiv:2311.17227.\n",
            "John Maynard Keynes. 1936. The general theory of\n",
            "employment. The quarterly journal of economics ,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 164}\n",
            "--------------------------------------------------\n",
            "page_content='51(2):209–223.\n",
            "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\n",
            "taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\n",
            "guage models are zero-shot reasoners. Advances in\n",
            "neural information processing systems , 35:22199–\n",
            "22213.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 165}\n",
            "--------------------------------------------------\n",
            "page_content='Alain Ledoux. 1981. Concours résultats complets. Les\n",
            "victimes se sont plu à jouer le, 14:10–11.\n",
            "Nian Li, Chen Gao, Mingyu Li, Yong Li, and Qing-\n",
            "min Liao. 2024. EconAgent: Large language model-\n",
            "empowered agents for simulating macroeconomic ac-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 166}\n",
            "--------------------------------------------------\n",
            "page_content='tivities. In Proceedings of the 62nd Annual Meeting\n",
            "9' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 167}\n",
            "--------------------------------------------------\n",
            "page_content='of the Association for Computational Linguistics (Vol-\n",
            "ume 1: Long Papers), pages 15523–15536, Bangkok,\n",
            "Thailand. Association for Computational Linguistics.\n",
            "Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and\n",
            "Khaldoun Khashanah. 2023. Tradinggpt: Multi-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 168}\n",
            "--------------------------------------------------\n",
            "page_content='agent system with layered memory and distinct char-\n",
            "acters for enhanced financial trading performance.\n",
            "arXiv preprint arXiv:2309.03736.\n",
            "Weiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin,\n",
            "Haifeng Zhang, and Jun Wang. 2023. Large lan-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 169}\n",
            "--------------------------------------------------\n",
            "page_content='guage models play starcraft ii: Benchmarks and a\n",
            "chain of summarization approach. arXiv preprint\n",
            "arXiv:2312.11865.\n",
            "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler\n",
            "Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 170}\n",
            "--------------------------------------------------\n",
            "page_content='Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,\n",
            "et al. 2023. Self-refine: Iterative refinement with\n",
            "self-feedback. arXiv preprint arXiv:2303.17651.\n",
            "Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and\n",
            "Shimon Whiteson. 2019. Maven: Multi-agent varia-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 171}\n",
            "--------------------------------------------------\n",
            "page_content='tional exploration. Advances in neural information\n",
            "processing systems, 32.\n",
            "Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu,\n",
            "Xun Wang, Fengyi Wang, Tao Ge, and Furu Wei.\n",
            "2023. Alympics: Language agents meet game theory.\n",
            "arXiv preprint arXiv:2311.03220.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 172}\n",
            "--------------------------------------------------\n",
            "page_content='Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n",
            "2021. A diverse corpus for evaluating and developing\n",
            "english math word problem solvers. arXiv preprint\n",
            "arXiv:2106.15772.\n",
            "Rosemarie Nagel. 1995. Unraveling in guessing games:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 173}\n",
            "--------------------------------------------------\n",
            "page_content='An experimental study. The American economic re-\n",
            "view, 85(5):1313–1326.\n",
            "Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang,\n",
            "and Yu Wang. 2023. Skeleton-of-thought: Large\n",
            "language models can do parallel decoding. arXiv\n",
            "preprint arXiv:2307.15337.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 174}\n",
            "--------------------------------------------------\n",
            "page_content='Georgios Papoudakis, Filippos Christianos, Arrasy Rah-\n",
            "man, and Stefano V Albrecht. 2019. Dealing with\n",
            "non-stationarity in multi-agent deep reinforcement\n",
            "learning. arXiv preprint arXiv:1906.04737.\n",
            "Arkil Patel, Satwik Bhattamishra, and Navin Goyal.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 175}\n",
            "--------------------------------------------------\n",
            "page_content='2021. Are nlp models really able to solve\n",
            "simple math word problems? arXiv preprint\n",
            "arXiv:2103.07191.\n",
            "Noah Shinn, Federico Cassano, Ashwin Gopinath,\n",
            "Karthik R Narasimhan, and Shunyu Yao. 2023. Re-\n",
            "flexion: Language agents with verbal reinforcement' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 176}\n",
            "--------------------------------------------------\n",
            "page_content='learning. In Thirty-seventh Conference on Neural\n",
            "Information Processing Systems.\n",
            "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\n",
            "Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\n",
            "Adam R Brown, Adam Santoro, Aditya Gupta,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 177}\n",
            "--------------------------------------------------\n",
            "page_content='Adrià Garriga-Alonso, et al. 2022. Beyond the\n",
            "imitation game: Quantifying and extrapolating the\n",
            "capabilities of language models. arXiv preprint\n",
            "arXiv:2206.04615.\n",
            "Dale O Stahl and Paul W Wilson. 1995. On players’' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 178}\n",
            "--------------------------------------------------\n",
            "page_content='models of other players: Theory and experimental ev-\n",
            "idence. Games and Economic Behavior, 10(1):218–\n",
            "254.\n",
            "Peter Sunehag, Guy Lever, Audrunas Gruslys, Woj-\n",
            "ciech Marian Czarnecki, Vinicius Zambaldi, Max\n",
            "Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 179}\n",
            "--------------------------------------------------\n",
            "page_content='Leibo, Karl Tuyls, et al. 2017. Value-decomposition\n",
            "networks for cooperative multi-agent learning. arXiv\n",
            "preprint arXiv:1706.05296.\n",
            "Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\n",
            "bastian Gehrmann, Yi Tay, Hyung Won Chung,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 180}\n",
            "--------------------------------------------------\n",
            "page_content='Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\n",
            "Zhou, et al. 2022. Challenging big-bench tasks and\n",
            "whether chain-of-thought can solve them. arXiv\n",
            "preprint arXiv:2210.09261.\n",
            "Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bha-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 181}\n",
            "--------------------------------------------------\n",
            "page_content='gavatula, Yoav Goldberg, Yejin Choi, and Jonathan\n",
            "Berant. 2022. Commonsenseqa 2.0: Exposing the\n",
            "limits of ai through gamification. arXiv preprint\n",
            "arXiv:2201.05320.\n",
            "Zhenhailong Wang, Shaoguang Mao, Wenshan Wu,\n",
            "Tao Ge, Furu Wei, and Heng Ji. 2023. Unleash-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 182}\n",
            "--------------------------------------------------\n",
            "page_content='ing cognitive synergy in large language models:\n",
            "A task-solving agent through multi-persona self-\n",
            "collaboration. arXiv preprint arXiv:2307.05300.\n",
            "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
            "Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 183}\n",
            "--------------------------------------------------\n",
            "page_content='et al. 2022. Chain-of-thought prompting elicits rea-\n",
            "soning in large language models. Advances in Neural\n",
            "Information Processing Systems, 35:24824–24837.\n",
            "Annie Wong, Thomas Bäck, Anna V Kononova, and\n",
            "Aske Plaat. 2021. Deep multiagent reinforcement' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 184}\n",
            "--------------------------------------------------\n",
            "page_content='learning: Challenges and directions. arXiv preprint\n",
            "arXiv:2106.15691.\n",
            "Sang Michael Xie, Aditi Raghunathan, Percy Liang, and\n",
            "Tengyu Ma. 2021. An explanation of in-context learn-\n",
            "ing as implicit bayesian inference. arXiv preprint\n",
            "arXiv:2111.02080.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 185}\n",
            "--------------------------------------------------\n",
            "page_content='Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen\n",
            "Dong, Kurt Keutzer, See-Kiong Ng, and Jiashi Feng.\n",
            "2023a. Magic: Investigation of large language model\n",
            "powered multi-agent in cognition, adaptability, ratio-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 186}\n",
            "--------------------------------------------------\n",
            "page_content='nality and collaboration. In ICLR 2024 Workshop on\n",
            "Large Language Model (LLM) Agents.\n",
            "Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-\n",
            "aolong Wang, Weidong Liu, and Yang Liu. 2023b.\n",
            "Exploring large language models for communica-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 187}\n",
            "--------------------------------------------------\n",
            "page_content='tion games: An empirical study on werewolf. arXiv\n",
            "preprint arXiv:2309.04658.\n",
            "10' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 188}\n",
            "--------------------------------------------------\n",
            "page_content='Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu.\n",
            "2023c. Language agents with reinforcement learn-\n",
            "ing for strategic play in the werewolf game. arXiv\n",
            "preprint arXiv:2310.18940.\n",
            "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 189}\n",
            "--------------------------------------------------\n",
            "page_content='Thomas L Griffiths, Yuan Cao, and Karthik\n",
            "Narasimhan. 2023. Tree of thoughts: Deliberate\n",
            "problem solving with large language models. arXiv\n",
            "preprint arXiv:2305.10601.\n",
            "Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 190}\n",
            "--------------------------------------------------\n",
            "page_content='Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li,\n",
            "Yueting Zhuang, and Weiming Lu. 2024a. Agent-\n",
            "pro: Learning to evolve via policy-level reflection\n",
            "and optimization. arXiv preprint arXiv:2402.17574.\n",
            "Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 191}\n",
            "--------------------------------------------------\n",
            "page_content='Yan Xia, Wenshan Wu, Ting Song, Man Lan, and\n",
            "Furu Wei. 2024b. LLM as a mastermind: A survey\n",
            "of strategic reasoning with large language models. In\n",
            "First Conference on Language Modeling.\n",
            "Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 192}\n",
            "--------------------------------------------------\n",
            "page_content='Kaijie Zhu, Hao Chen, and Xing Xie. 2023. Com-\n",
            "peteai: Understanding the competition behaviors in\n",
            "large language model-based agents. arXiv preprint\n",
            "arXiv:2310.17512.\n",
            "Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 193}\n",
            "--------------------------------------------------\n",
            "page_content='Haofei Yu, Zhengyang Qi, Louis-Philippe Morency,\n",
            "Yonatan Bisk, Daniel Fried, Graham Neubig, and\n",
            "Maarten Sap. 2024. SOTOPIA: Interactive evalua-\n",
            "tion for social intelligence in language agents. In\n",
            "The Twelfth International Conference on Learning' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 194}\n",
            "--------------------------------------------------\n",
            "page_content='Representations.\n",
            "11' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 195}\n",
            "--------------------------------------------------\n",
            "page_content='A Impact Statements\n",
            "Our research introduces the K-Level Reasoning\n",
            "framework, designed to formulate strategies in dy-\n",
            "namic, interactive and competitive scenarios by\n",
            "anticipating the reactions of adversaries, poten-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 196}\n",
            "--------------------------------------------------\n",
            "page_content='tially users. Theoretically, this approach offers a\n",
            "novel perspective for understanding and optimizing\n",
            "decision-making processes. However, we recog-\n",
            "nize when the goal setting diverges from user inter-\n",
            "ests, the application of K-Level Reasoning could' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 197}\n",
            "--------------------------------------------------\n",
            "page_content='result in manipulative behaviors by adapting to the\n",
            "predicted user’s reactions. This risk is notably pro-\n",
            "nounced in scenarios designed to influence user\n",
            "decisions or behaviors, such as in recommenda-\n",
            "tion systems, advertising placements, and content' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 198}\n",
            "--------------------------------------------------\n",
            "page_content='distribution on social media platforms.\n",
            "Although K-Level Reasoning provides a potent\n",
            "powerful tool for strategic planning, interacting and\n",
            "reasoning, ethical considerations must be meticu-\n",
            "lously managed in its practical application. This' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 199}\n",
            "--------------------------------------------------\n",
            "page_content='ensures that the development and utilization of tech-\n",
            "nology do not detrimentally impact user and soci-\n",
            "etal interests. To this end, we advocate for height-\n",
            "ened transparency, ensuring users have a compre-\n",
            "hensive understanding and control over how their' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 200}\n",
            "--------------------------------------------------\n",
            "page_content='data is utilized.\n",
            "B Game Setting\n",
            "B.1 Guessing 0.8 of the Average\n",
            "Initial Setup: For each round, each player select\n",
            "a number between 1 and 100. The objective is to\n",
            "select a number that is closest to 80% of the group’s' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 201}\n",
            "--------------------------------------------------\n",
            "page_content='average choice. Formally, each player i chooses a\n",
            "number ni, aiming for ni ≈ 0.8 × n, where n is\n",
            "the average of all chosen numbers.\n",
            "Scoring and Continuation: A player scores a\n",
            "point if his/her chosen number is closest to 80%' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 202}\n",
            "--------------------------------------------------\n",
            "page_content='of the average number chosen by the group. If\n",
            "all players select the same number, no points are\n",
            "awarded for this round. Mathematically, the score\n",
            "for player i in in round t is given by st\n",
            "i, which is 1\n",
            "if |ni −0.8 ×n| is the minimum among all players,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 203}\n",
            "--------------------------------------------------\n",
            "page_content='and 0 otherwise.\n",
            "B.2 Survival Auction Game\n",
            "Initial Setup: Players start with 8 health points,\n",
            "out of a maximum of 10. Every day, each player\n",
            "possesses a fixed income of $100. The daily water\n",
            "supply can only satisfy one resident’s requirement.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 204}\n",
            "--------------------------------------------------\n",
            "page_content='Scoring and Continuation: Everyday, players\n",
            "engage in a daily auction to secure the necessary\n",
            "water resources, and the highest bidder wins. In\n",
            "case of a tie, the resources are not allocated to\n",
            "any player. If a player successfully bid the water' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 205}\n",
            "--------------------------------------------------\n",
            "page_content='resources, they will gain 2 health points; otherwise,\n",
            "they will lose health points equal to the number\n",
            "of consecutive days, denoted as n, during which\n",
            "they have not obtained water resources. Once a\n",
            "player’s health points fall to 0 or below, they will' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 206}\n",
            "--------------------------------------------------\n",
            "page_content='be eliminated. The health point of player i on day\n",
            "t, denoted as ht\n",
            "i, is crucial in determining their\n",
            "survival and bidding strategy.\n",
            "C Detailed Metric Computational\n",
            "Formulas\n",
            "Win Rate is calculated based on the number of' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 207}\n",
            "--------------------------------------------------\n",
            "page_content='wins over game going, providing a measure of the\n",
            "overall ability.\n",
            "WinRate = Num of Wins\n",
            "Total Round per Test× Num of Test(9)\n",
            "Average Survival Roundcalculates the average\n",
            "round in which the player remains in the game.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 208}\n",
            "--------------------------------------------------\n",
            "page_content='It’s an effective way to assess performance in\n",
            "elimination-based game, like SAG.\n",
            "AvgSurvivalRound =\n",
            "PSurvival Round in Each Test\n",
            "Num of Test (10)\n",
            "Prediction Accuracy evaluates the accuracy of\n",
            "player’s predictions regarding rivals’ future moves.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 209}\n",
            "--------------------------------------------------\n",
            "page_content='In the G0.8A, it involves calculating the absolute\n",
            "difference between the player’s predicted average\n",
            "and the actual average in each round:\n",
            "Pred Acc =\n",
            "P|AvgP red− AvgActual|\n",
            "Num of Test (11)\n",
            "In the SAG, the focus shifts to measuring the ab-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 210}\n",
            "--------------------------------------------------\n",
            "page_content='solute error between the player’s prediction of the\n",
            "highest bid by opponents and the actual highest bid\n",
            "made by them.\n",
            "Pred Acc =\n",
            "P|Max BidP red− Max BidActual|\n",
            "Num of Test (12)\n",
            "Strategic Level\n",
            "StrategicDepth(choice) = logα(choice/50)\n",
            "(13)' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 211}\n",
            "--------------------------------------------------\n",
            "page_content='Here, α represents the target value coefficient, and\n",
            "50 represents the average of a random choice be-\n",
            "tween 0 and 100, which is used to represent level-0\n",
            "players. In the settings of [1][2], the parameter α is\n",
            "set to 2' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 212}\n",
            "--------------------------------------------------\n",
            "page_content='3 . In our configuration, α is set to 0.8, which\n",
            "is the origin of the name G0.8A.\n",
            "12' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 213}\n",
            "--------------------------------------------------\n",
            "page_content='Introduction Prompt:\n",
            "You are Alex and involved in a survive challenge. You are one of five players in the game. Everyone is required to choose an integer' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 214}\n",
            "--------------------------------------------------\n",
            "page_content='between 1 and 100 in each round. The player whose chosen number is closest to (0.8 * the average of all chosen numbers) wins the' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 215}\n",
            "--------------------------------------------------\n",
            "page_content='round without any HP deduction. All other players will have 1 HP deducted. But if all players choose the same number, their h ealth \n",
            "points are deducted together.\n",
            "Round Start Prompt (Standard):' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 216}\n",
            "--------------------------------------------------\n",
            "page_content='Ok, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.”\n",
            "Notification Result Prompt :' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 217}\n",
            "--------------------------------------------------\n",
            "page_content='Thank you all for participating in Round 1. In this round, Alex chose 45, Bob chose 45, Cindy chose 50, David chose 45, Eric chose 45.\n",
            "The average is (45 + 45 + 50 + 45 + 45) / 5 = 46.0.  The average 46.0 multiplied by 0.8 equals 36.8.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 218}\n",
            "--------------------------------------------------\n",
            "page_content='Alex: |45 - 36.8| = 8, Bob: |45 - 36.8| = 8, Cindy: |50 - 36.8| = 13, David: |45 - 36.8| = 8, Eric: |45 - 36.8| = 8\n",
            "[Alex, Bob, David, Eric]'s choice of 45 is closest to 36.8.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 219}\n",
            "--------------------------------------------------\n",
            "page_content='Round winner: Alex, Bob, David, Eric. All other players lose 1 point. After the deduction, player information is: \n",
            "name: Alex  HP: 10, name: Bob HP: 10, name: Cindy HP: 9, name: David HP: 10, name: Eric  HP: 10.\n",
            "Congratulation Prompt (When Win):' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 220}\n",
            "--------------------------------------------------\n",
            "page_content='You have successfully chosen the number closest to the target number, which is the average of all players' selected numbers \n",
            "multiplied by 0.8. As a result, you have won this round. All other players will now deduct 1 HP. \n",
            "Warning Prompt (When Loss):' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 221}\n",
            "--------------------------------------------------\n",
            "page_content='WARNING: You have lost 1 point of HP in this round! You now have only 9 points of health left. You are one step closer to dea th.\n",
            "G0.8A Game Prompt\n",
            "Figure 4: Prompts used in Guessing 0.8 of the Average game.\n",
            "Introduction Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 222}\n",
            "--------------------------------------------------\n",
            "page_content='You are Alex and a resident living in W-Town. W Town is experiencing a rare drought. Every residents in Town W is ensuring their survival \n",
            "over a period of 10 days by acquiring the water resources.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 223}\n",
            "--------------------------------------------------\n",
            "page_content='Attention, all W-Town residents, welcome to the Water Allocation Challenge!\n",
            "In this challenge, you are tasked with ensuring your survival over a period of 10 days by acquiring the necessary water resources to maintain' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 224}\n",
            "--------------------------------------------------\n",
            "page_content='your health. You will participate in daily auctions to bid for water resources to meet your individual needs.\n",
            "Here are the game rules and settings:\n",
            "1. You are one of five residents with same water requirements, budgets, and health points.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 225}\n",
            "--------------------------------------------------\n",
            "page_content='2. Your goal is to survive until the end of the 10 days.\n",
            "3. Each resident has a maximum of 10 health points and starts with 8 health points. If your health points drop below or equal to 0, you will' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 226}\n",
            "--------------------------------------------------\n",
            "page_content='be considered dead and eliminated from the game! All your accumulated money will be reset to Zero! \n",
            "4. Every day, you will bid on water resources to meet your needs. If your consecutive days without obtaining water resource (No-Drink Days)' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 227}\n",
            "--------------------------------------------------\n",
            "page_content='reach n, your health will be deducted by n points on that day. If your water needs are met, 2 points will be added to your health, and the \n",
            "No-Drink Days will be reset to 0.\n",
            "5. Daily water resources can only meet the needs of one resident.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 228}\n",
            "--------------------------------------------------\n",
            "page_content='6. Each resident has $100 daily income;\n",
            "7. To allocate water resources, a sealed-bid auction will be conducted daily. Each resident submits a single bid for their entire water need. \n",
            "The resident with the highest bid is eligible to obtain water resources.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 229}\n",
            "--------------------------------------------------\n",
            "page_content='8. If the highest bid results in a tie, no residents will have access to water . \n",
            "All bidding information will be made public after the allocation of water resources on the same day.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 230}\n",
            "--------------------------------------------------\n",
            "page_content='Remember , the key to success is effective bidding and strategizing to ensure your survival. Good luck!!\n",
            "Round Start Prompt (Standard):\n",
            "Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 231}\n",
            "--------------------------------------------------\n",
            "page_content='name: Alex  balance:100  HP:8  no_drink : 1  \n",
            "Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! Now, if you' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 232}\n",
            "--------------------------------------------------\n",
            "page_content='want to participate in today's water resource auction, please provide your bid.\n",
            "Notification Result Prompt :\n",
            "Thank you all for participating in Round 1. In this round, Alex bid 25, Bob bid 40, Cindy bid 40, David bid 30, Eric bid 60.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 233}\n",
            "--------------------------------------------------\n",
            "page_content='Total water resource supply is 10. According to the principle of the highest bidder and the rule when the game is tied, Eric won this auction \n",
            "and obtain water resource. \n",
            "After allocation, all survival residents' information is as follows:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 234}\n",
            "--------------------------------------------------\n",
            "page_content='name: Alex balance: 100  HP:7 no_drink : 2,   name: Bob  balance: 100  HP:7 no_drink:2 \n",
            "name :Cindy  balance:100 HP:7 no_drink:2,   name: David  balance: 100 HP: :7 no_drink:2\n",
            "name:Eric balance:40  HP:10 no_drink:1\n",
            "Congratulation Prompt (When Win):' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 235}\n",
            "--------------------------------------------------\n",
            "page_content='You have successfully won the bidding for today's water resources and restored 2 points of HP .\n",
            "Warning Prompt (When Lose):\n",
            "WARNING: You have lost 1point of HP in this round! You now have only 7 points of health left. You are one step closer to death.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 236}\n",
            "--------------------------------------------------\n",
            "page_content='Survival Auction Game Prompt\n",
            "Bidding Result Parse Prompt:\n",
            "By reading the conversation, extract the number chosen by player . Output format: number . If the player does not bid, Output: 0.\n",
            "Figure 5: Prompts used in Survival Auction Game.\n",
            "13' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 237}\n",
            "--------------------------------------------------\n",
            "page_content='D Performance of Large Language\n",
            "Models Competing with Programmatic\n",
            "Strategies\n",
            "In addition to using the LLM-LLM Combat com-\n",
            "parison setting in Section 4, we have also designed\n",
            "a set of LLM-Programmatic Strategy Combat' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 238}\n",
            "--------------------------------------------------\n",
            "page_content='comparisons. In Player-Programmatic Strategy\n",
            "Combat setting, the “player\" will be equipped with\n",
            "a specific reasoning method, while opponents will\n",
            "play according to programmatic strategic patterns,\n",
            "which will not be adjusted with the game going.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 239}\n",
            "--------------------------------------------------\n",
            "page_content='This mode is to check different methods’ adapta-\n",
            "tion to different predefined but fixed patterns. For\n",
            "the Player-Programmatic Player Combat Combat\n",
            "in G0.8A, the programmatic strategies include:\n",
            "1) 0-Level (Fix): The characteristic of a 0-Level' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 240}\n",
            "--------------------------------------------------\n",
            "page_content='player is that their choice of strategy is uniform\n",
            "(Stahl and Wilson, 1995). We limit the choice space\n",
            "of the 0-Level Computer player to only 40.\n",
            "2) 0-Level (Var): Modified from the 0-Level\n",
            "(Fix) strategy, the selection is sampled from a Gaus-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 241}\n",
            "--------------------------------------------------\n",
            "page_content='sian distribution with a mean of 40 and a variance\n",
            "of 5.\n",
            "3) MonoTrend (Fix): The numbers chosen by\n",
            "the computer players follow an arithmetic sequence\n",
            "with a decreasing common difference, and the com-\n",
            "mon differences for all four computer players are' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 242}\n",
            "--------------------------------------------------\n",
            "page_content='the same.\n",
            "4) MonoTrend (Var): The numbers chosen by\n",
            "the computer players follow an arithmetic sequence\n",
            "with a decreasing common difference, and the com-\n",
            "mon differences for the four computer players are\n",
            "randomly generated from 1 to 5.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 243}\n",
            "--------------------------------------------------\n",
            "page_content='5) LastBids (Fix): The computer player chooses\n",
            "the target number from the previous round (selects\n",
            "40 in the first round).\n",
            "6) LastBids (Var): Modified from the LastBids\n",
            "strategy, the selection is sampled from a Gaussian' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 244}\n",
            "--------------------------------------------------\n",
            "page_content='distribution with a mean equal to the target number\n",
            "of the previous round and a variance of 5.\n",
            "Overall, the dynamic changes of these three set-\n",
            "tings are LastBids > MonoTrend > 0-Level. We\n",
            "use these three programs to test the reasoning capa-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 245}\n",
            "--------------------------------------------------\n",
            "page_content='bility to adapt to and counter different patterns.\n",
            "Table 9 reveals a significant trend in the perfor-\n",
            "mance of players against other approaches. The\n",
            "effectiveness of reasoning approaches decreases\n",
            "in the following order: 0-Level, MonoTrend, and' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 246}\n",
            "--------------------------------------------------\n",
            "page_content='LastBids. This pattern highlights a reduction in the\n",
            "efficacy of the LLM in more dynamic environments.\n",
            "We found that only K-Level Reasoning shows an\n",
            "advantage in LastBids (Fix), indicating that com-\n",
            "pared to K-Level Reasoning, previous reasoning' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 247}\n",
            "--------------------------------------------------\n",
            "page_content='methods on static problems lack observation and\n",
            "judgment of the opponent. Conversely, this also\n",
            "demonstrates that K-Level Reasoning can implic-\n",
            "itly infer the behavior pattern of the opponent based\n",
            "on their historical actions.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 248}\n",
            "--------------------------------------------------\n",
            "page_content='Intriguingly, reasoning methods significantly in-\n",
            "fluence the performance in dynamic settings. Meth-\n",
            "ods like CoT and Self-Refine, traditionally favored\n",
            "in static reasoning, also demonstrate substantial\n",
            "improvements over the Standard Prompt approach.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 249}\n",
            "--------------------------------------------------\n",
            "page_content='This finding underscores the necessity for more\n",
            "elaborate reasoning processes in dynamic decision-\n",
            "making, akin to static problem-solving scenarios.\n",
            "Table 9: Win Rate of the player against different pro-\n",
            "grammatic strategies in Guessing 0.8 of the Average' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 250}\n",
            "--------------------------------------------------\n",
            "page_content='game.\n",
            "Opponent Direct CoT Persona Reflect Refine PCoT K-R\n",
            "Player VS Programmatic Strategies\n",
            "0-Level (Fix) 0.65 0.87 0.87 0.81 0.99 0.800.97\n",
            "0-Level (Var)0.44 0.67 0.69 0.61 0.54 0.760.77\n",
            "MonoTrend (Fix)0.05 0.06 0.15 0.00 0.29 0.150.48' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 251}\n",
            "--------------------------------------------------\n",
            "page_content='MonoTrend (Var)0.34 0.44 0.57 0.33 0.49 0.460.74\n",
            "LastBids (Fix)0.01 0.12 0.16 0.01 0.27 0.060.75\n",
            "LastBids (Var)0.06 0.15 0.18 0.19 0.18 0.140.18\n",
            "E Open Source LLM with K-Level\n",
            "Reasoning\n",
            "In addition to the experiments on GPT3.5 /GPT4,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 252}\n",
            "--------------------------------------------------\n",
            "page_content='we conducted tests with an open-source smaller-\n",
            "scale model, LLaMA-7B-Chat, in the \"G0.8A\"\n",
            "game. To clearly compare the performance of dif-\n",
            "ferent LLMs, we adopted the LLM-Programmatic\n",
            "Strategy Combat setting described in Appendix B.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 253}\n",
            "--------------------------------------------------\n",
            "page_content='From the experimental results, we can see that:\n",
            "1. Llama2-7B and GPT3.5, even including GPT4,\n",
            "perform poorly when competing against program-\n",
            "matic strategies using standard prompting, strug-\n",
            "gling even with some very simple strategies. It' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 254}\n",
            "--------------------------------------------------\n",
            "page_content='underscores the significance of our research; 2. For\n",
            "all tested models, applying k-level reasoning effec-\n",
            "tively enhanced the win rate, highlighting the stabil-\n",
            "ity of the improvement our proposed model brings' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 255}\n",
            "--------------------------------------------------\n",
            "page_content='to different base models. Interestingly, for stronger\n",
            "LLMs, applying K-R can achieve a higher relative\n",
            "improvement, we speculate that the enhancement is\n",
            "simultaneously derived from the opponent’s action\n",
            "simulation and reasoning capabilities of the base' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 256}\n",
            "--------------------------------------------------\n",
            "page_content='model.\n",
            "14' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 257}\n",
            "--------------------------------------------------\n",
            "page_content='Table 10: Win Rate of the reasoning methods with open\n",
            "source LLMs against different programmatic strategies\n",
            "in Guessing 0.8 of the Average game.\n",
            "Methods Direct K-R[K=2] Direct K-R[K=2] Direct K-R[K=2]' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 258}\n",
            "--------------------------------------------------\n",
            "page_content='Base Model [LLaMA-7B] [LLaMA-7B] [GPT3.5] [GPT3.5] [GPT4] [GPT4]\n",
            "Player VS Programmatic Strategies' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 259}\n",
            "--------------------------------------------------\n",
            "page_content='0-Level (Fix)0.30 0.43 0.30 0.49 0.65 0.970-Level (Var)0.13 0.22 0.11 0.36 0.44 0.77MonoTrend (Fix)0.12 0.12 0.17 0.32 0.05 0.48MonoTrend (Var)0.17 0.13 0.19 0.42 0.34 0.74LastBids (Fix)0.04 0.11 0.22 0.38 0.01 0.75LastBids (Var)0.03 0.08 0.11 0.10 0.06' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 260}\n",
            "--------------------------------------------------\n",
            "page_content='0.18' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 261}\n",
            "--------------------------------------------------\n",
            "page_content='Avg 0.13 0.18 (+0.05) 0.18 0.35 (+0.17) 0.26 0.65 (+0.39)\n",
            "F Batter Nash Equilibrium\n",
            "Approximation with K-Level Reasoning\n",
            "To assess the impact of K-Level Reasoning on Nash\n",
            "equilibrium, we conducted experiments testing the' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 262}\n",
            "--------------------------------------------------\n",
            "page_content='performance of LLMs at different K-Levels in the\n",
            "Prisoner’s Dilemma. The Prisoner’s Dilemma is\n",
            "a classic concept in game theory that illustrates\n",
            "a paradoxical situation in which individuals act-\n",
            "ing in their own self-interest lead to a suboptimal' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 263}\n",
            "--------------------------------------------------\n",
            "page_content='outcome for everyone involved. It’s often used\n",
            "to study decision-making in situations involving\n",
            "cooperation and competition.\n",
            "From Table 11, it is evident that when K=1, play-\n",
            "ers exhibit weaker rationality: they tend to choose' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 264}\n",
            "--------------------------------------------------\n",
            "page_content='cooperation over betrayal. As the K-Level of one\n",
            "player increases (first row from left to right), play-\n",
            "ers with higher K-Levels demonstrate stronger ra-\n",
            "tionality: they tend to choose betrayal over coop-\n",
            "eration. Moreover, when both players’ K-Levels' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 265}\n",
            "--------------------------------------------------\n",
            "page_content='increase, they are more likely to reach a state of\n",
            "Nash equilibrium: both choosing to betray each\n",
            "other.\n",
            "Table 11: Statistical analysis of the Payoff Matrix\n",
            "for LLM in the Prisoner’s Dilemma across different' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 266}\n",
            "--------------------------------------------------\n",
            "page_content='K-Levels. The Nash equilibrium of the Prisoner’s\n",
            "Dilemma, where both players choose to betray each\n",
            "other, is highlighted with a green background in the\n",
            "bottom-right corner of each cell.\n",
            "Level-1 Level-2 Level-3 Level-4\n",
            "10 0 7 3 1 9 0 10Level-1' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 267}\n",
            "--------------------------------------------------\n",
            "page_content='0 0 0 0 0 0 0 0\n",
            "7 0 2 0 0 2 0 1Level-2\n",
            "3 0 5 3 0 8 0 9\n",
            "1 0 0 0 0 0 0 0Level-3\n",
            "9 0 2 8 0 10 1 9\n",
            "0 0 0 0 0 1 0 0Level-4\n",
            "10 0 1 9 0 9 1 9\n",
            "G Computational Cost Analysis\n",
            "Table 12: Average input/output/total token consumption\n",
            "per game test. Unit: Kilo tokens.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 268}\n",
            "--------------------------------------------------\n",
            "page_content='Game Metric Direct CoT Persona Reflect Refine PCoT K-R\n",
            "GO8A\n",
            "Input 14.80 23.30 18.70 39.50145.5030.80 123.40Output 0.10 1.80 0.60 1.20 6.901.30 1.60Total 14.90 25.10 19.30 40.70152.4032.10 124.90\n",
            "Avg Win Rate 0.16 0.42 0.41 0.24 0.37 0.400.65\n",
            "SAG' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 269}\n",
            "--------------------------------------------------\n",
            "page_content='Input 21.30 29.00 27.10 51.40130.5037.00 90.10Output 0.70 1.00 0.90 1.80 3.301.30 1.40Total 22.00 30.00 28.10 53.20133.8038.20 91.60\n",
            "Avg Surv Round 6.51 7.44 7.59 4.82 7.33 6.179.01\n",
            "To understand how different reasoning meth-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 270}\n",
            "--------------------------------------------------\n",
            "page_content='ods affect token consumption, thereby assisting\n",
            "users in utilizing models more effectively and opti-\n",
            "mizing resource utilization and cost control, we\n",
            "conducted an analysis of the computational ex-\n",
            "penditure associated with various reasoning ap-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 271}\n",
            "--------------------------------------------------\n",
            "page_content='proaches. Specifically, we calculated the average\n",
            "input/output/total token consumption per game test\n",
            "for both the \"Guessing 0.8 of the Average\" and\n",
            "\"Survival Auction Game.\"\n",
            "K-Level Reasoning, due to simulating the oppo-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 272}\n",
            "--------------------------------------------------\n",
            "page_content='nents’ actions based on public historical behavior\n",
            "information with new sessions, inevitably causes\n",
            "more token consumption. The increase in token\n",
            "consumption linearly correlates with the number of\n",
            "opponents that need to be simulated and the rounds.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 273}\n",
            "--------------------------------------------------\n",
            "page_content='We also find that more token consumption does\n",
            "not necessarily lead to better results. The Self-\n",
            "Refine method, due to its action-feedback-refine\n",
            "pipeline, leads to a substantial increase in token\n",
            "consumption for both input and output, yet it did' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 274}\n",
            "--------------------------------------------------\n",
            "page_content='not outperform our proposed K-Level Reasoning\n",
            "in terms of performance. Compared to baselines,\n",
            "the proposed K-Level Reasoning method is an ef-\n",
            "fective and efficient way to recursively simulate the\n",
            "decision-making process of the opponent and make' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 275}\n",
            "--------------------------------------------------\n",
            "page_content='decisions based on the simulation results.\n",
            "H Statistical Significance of K-Level\n",
            "Reasoning efficacy\n",
            "Due to the utilization of a large language model for\n",
            "evaluation, inherent stochasticity is present, and\n",
            "the sample size for testing is limited (only con-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 276}\n",
            "--------------------------------------------------\n",
            "page_content='ducted 10 experiments). To objectively assess the\n",
            "reliability of the experiment result, we subjected\n",
            "the results from G0.8A and SAG to a t-test for sig-\n",
            "nificance with p = 0.05. Specifically, different\n",
            "significance testing methods were employed for' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 277}\n",
            "--------------------------------------------------\n",
            "page_content='different evaluation metrics, as outlined below:\n",
            "15' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 278}\n",
            "--------------------------------------------------\n",
            "page_content='Table 13: The significance of metric result on G0.8A and SAG, comparing K-R against differentbaseline models in\n",
            "terms of Win Rate, Average Survival Round, and Adaptation Index.\n",
            "Game Metric Direct CoT Persona Reflect Refine PCoT' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 279}\n",
            "--------------------------------------------------\n",
            "page_content='G0.8A Win Rate ✓(0.001) ✓(0.005) ✓(0.001) ✓(0.001) ✓(0.001) ✓(0.001)\n",
            "Adaption Index ✓(0.007) ✓(0.010) ✓(0.039) ×(0.062) ✓(0.003) ✓(0.021)\n",
            "SAG Average Survival Round ✓(0.003) ✓(0.010) ×(0.074) ✓(0.001) ✓(0.001) ✓(0.001)' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 280}\n",
            "--------------------------------------------------\n",
            "page_content='Adaption Index ×(0.337) ×(0.786) ×(0.659) ×(0.066) ×(0.672) ×(0.894)\n",
            "Table 14: The significance of comparing K-R with PCoT concerning the differential performance across various\n",
            "Baseline models on the metric of Prediction Accuracy.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 281}\n",
            "--------------------------------------------------\n",
            "page_content='Game Direct CoT Persona Reflect Refine PCoT K-R\n",
            "G0.8A ✓(0.003) ×(0.058) ✓(0.031) ✓(0.003) ✓(0.022) ✓(0.023) ×(0.107)\n",
            "SAG ×(0.091) ✓(0.023) ✓(0.049) ×(0.907) ×(0.956) ×(0.656) ✓(0.028)\n",
            "• Win Rate & Average Survival Round: A t-test' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 282}\n",
            "--------------------------------------------------\n",
            "page_content='was conducted on the Win Rate and Average\n",
            "Survival Round of K-R versus other baselines\n",
            "when facing different agents. The null hypoth-\n",
            "esis posited no significant difference in perfor-\n",
            "mance between K-R and other baselines con-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 283}\n",
            "--------------------------------------------------\n",
            "page_content='cerning Win Rate & Average Survival Round.\n",
            "• Adaptation Index: The method akin to that\n",
            "of Win Rate & Average Survival Round was\n",
            "applied.\n",
            "• Prediction Accuracy: Since only PCoT and\n",
            "K-R explicitly predicted opponents, we com-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 284}\n",
            "--------------------------------------------------\n",
            "page_content='pared the significance of the prediction differ-\n",
            "ences between PCoT and K-R. Unlike Win\n",
            "Rate and Adaptation Index, we assessed the\n",
            "differences in predictions (averaged over mul-\n",
            "tiple experiments) between PCoT and K-R' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 285}\n",
            "--------------------------------------------------\n",
            "page_content='when facing the same opponents (e.g., CoT)\n",
            "across different game rounds.\n",
            "From Table 13 and Table 14, it is evident that\n",
            "K-R exhibits a significant advantage over other\n",
            "baselines in Win Rate and Average Survival Round' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 286}\n",
            "--------------------------------------------------\n",
            "page_content='metrics. This indicates K-R’s superior rational-\n",
            "ity in both G0.8A and SAG. Additionally, we ob-\n",
            "served weaker significance of K-R’s Adaptation\n",
            "Index compared to baselines in SAG. We attribute\n",
            "this observation to: 1) the more factors considered' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 287}\n",
            "--------------------------------------------------\n",
            "page_content='by each agent in SAG (e.g., health status), result-\n",
            "ing in a more dynamic and complex environment;\n",
            "and 2) the wider bidding range in later rounds of\n",
            "SAG games, potentially leading to greater bias in\n",
            "agent bidding and rendering the Adaptation Index' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 288}\n",
            "--------------------------------------------------\n",
            "page_content='a challenging metric.\n",
            "I Timing in Dynamic Reasoning\n",
            "In our experiment, we implemented two LLM self-\n",
            "refinement reasoning methods: Reflect (Madaan\n",
            "et al., 2023) and Refine (Shinn et al., 2023), and\n",
            "noticed that Refine performed significantly better' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 289}\n",
            "--------------------------------------------------\n",
            "page_content='than Reflect in the experimental results. To further\n",
            "explore the differences in performance between\n",
            "these two methods, we analyzed their respective\n",
            "working principles and applicable scenarios.\n",
            "The Reflect method involves making decisions' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 290}\n",
            "--------------------------------------------------\n",
            "page_content='first, then summarizing the experience based on\n",
            "feedback from the environment. This method\n",
            "may be effective in scenarios where the environ-\n",
            "ment does not change much or where the decision-\n",
            "making cycle is long, as it allows for quick decision-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 291}\n",
            "--------------------------------------------------\n",
            "page_content='making. However, its drawback is that, in dynamic\n",
            "environments, the experience from the previous\n",
            "round may not be suitable for the next round. In\n",
            "fact, in the Survival Auction Game (SAG), a rapidly\n",
            "changing environment, the survival rate of the Re-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 292}\n",
            "--------------------------------------------------\n",
            "page_content='flect method is even lower compared to making\n",
            "direct decisions. This is likely because this method\n",
            "does not sufficiently take into account the dynamic\n",
            "nature of the environment.\n",
            "In contrast, the Refine method involves mul-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 293}\n",
            "--------------------------------------------------\n",
            "page_content='tiple analyses before making a decision, includ-\n",
            "ing an initial analysis and improvements to that\n",
            "initial analysis. Importantly, both of these anal-\n",
            "yses are conducted in the context of the current\n",
            "decision-making environment. This makes the Re-\n",
            "16' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 294}\n",
            "--------------------------------------------------\n",
            "page_content='Ok, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the \n",
            "next round. Let's think step by step, and finally answer a number you think you can win.\n",
            "Prompt\n",
            "Ok, Alex! Now is the ROUND 2, and your HP is at 9. Please' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 295}\n",
            "--------------------------------------------------\n",
            "page_content='choose an integer between 1 and 100 for this round.\n",
            "In the first round, since there is no prior information about the other players' choices, it's \n",
            "difficult to accurately predict which number will win. However, we can make some' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 296}\n",
            "--------------------------------------------------\n",
            "page_content='assumptions and try to optimize our chances of winning.\n",
            "Let's assume that all players, including myself, choose numbers uniformly and \n",
            "independently between 1 and 100. Then, the expected average of all chosen numbers' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 297}\n",
            "--------------------------------------------------\n",
            "page_content='would be (1 + 100) / 2 = 50.5. If we multiply this by 0.8, we get 0.8 * 50.5 = 40.4. Since we \n",
            "need to choose an integer, we can round this to 40.\n",
            "Therefore, my initial guess for the first round is 40.\n",
            "Historical Targets: \n",
            "Round1:38\n",
            "Last Round choices:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 298}\n",
            "--------------------------------------------------\n",
            "page_content='Player1:50 Player2:45 Plyaer3:50 Player4:45 Player5:50\n",
            "Historical Targets: \n",
            "None\n",
            "Last Round choices: \n",
            "None\n",
            "Round-1: Player1Prompt\n",
            "Result:\n",
            "Player1: 40 Player2:47 Player3: 40 Player4: 60 Player5:42\n",
            "Target number: (40 + 47 + 40 + 60 + 42) / 5 * 0.8 = 37' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 299}\n",
            "--------------------------------------------------\n",
            "page_content='Winner: Player 1, Player3 \n",
            "Result:\n",
            "Player1: 32 Player2: 42 Player3: 50  Player4: 50 Player5: 45\n",
            "Target number(32 + 42 + 50 + 50 + 45) / 5 * 0.8 = 35\n",
            "Winner: Player 1\n",
            "delayed response\n",
            "Lose Win\n",
            "Round-2: Player1\n",
            "Reflect\n",
            "……\n",
            "Self-Refine\n",
            "……\n",
            "Output\n",
            "OutputOutput' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 300}\n",
            "--------------------------------------------------\n",
            "page_content='I choose 40 for this round.\n",
            "Reflect Prompt\n",
            "In Round 2, the chosen numbers were 40, 47, 40, 60, and 42. The average of these numbers was 45.8, \n",
            "and 0.8 times the average was 36.64. Alex and Cindy's choices of 40 were the closest to 36.64,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 301}\n",
            "--------------------------------------------------\n",
            "page_content='making them the winners of the round. \n",
            "From this round, we can observe that choosing a number closer to the expected average, considering \n",
            "the choices of other players, and adapting based on their choices is a good strategy. It's also' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 302}\n",
            "--------------------------------------------------\n",
            "page_content='important to note that the chosen numbers in this round were more diverse compared to the first \n",
            "round, which affected the average and the target number. Learning from the previous round and' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 303}\n",
            "--------------------------------------------------\n",
            "page_content='adjusting your choice accordingly can improve your chances of winning.\n",
            "Review the previous round games, summarize the experience.\n",
            "Prompt\n",
            "Ok, Alex! Now is the ROUND 3, and your HP is at 9. Please choose an integer between 1 and 100 \n",
            "for this round.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 304}\n",
            "--------------------------------------------------\n",
            "page_content='Round-3: Player1\n",
            "Output\n",
            "I choose 44 for this round.\n",
            "Win\n",
            "Result:\n",
            "Player1: 44 Player2:44 Player3: 55 Player4: 48 Player5:38\n",
            "Target number: (44 + 44 + 55 + 48 + 38)/ 5 * 0.8 = 37\n",
            "Winner: Player5\n",
            "Reflect: Player1\n",
            "outdated experience\n",
            "Feedback Prompt' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 305}\n",
            "--------------------------------------------------\n",
            "page_content='I have a game expert's advice on your strategy in this round. You can adjust your \n",
            "strategy just now according to his suggestion. Here are his suggestions:\n",
            "“As a game expert, I suggest a slight modification to the user's strategy to improve their' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 306}\n",
            "--------------------------------------------------\n",
            "page_content='winning rate in this round. Instead of assuming that all players will choose numbers \n",
            "uniformly and independently between 1 and 100, we can recognize that other players \n",
            "might also use a similar strategy and choose a number close to 40.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 307}\n",
            "--------------------------------------------------\n",
            "page_content='To optimize our chances of winning, we can try to be one step ahead of them. If we \n",
            "assume that the average of the chosen numbers will be around 40, we can multiply this' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 308}\n",
            "--------------------------------------------------\n",
            "page_content='by 0.8, which gives us 0.8 * 40 = 32. This way, we are closer to the 0.8 * the average if \n",
            "other players also cluster around 40. So, my optimized suggestion for the first round is to \n",
            "choose the number 32.”\n",
            "Refine Output' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 309}\n",
            "--------------------------------------------------\n",
            "page_content='Taking the game expert's advice into consideration, it's possible that other players might \n",
            "also use a similar strategy and choose numbers close to 40. To stay one step ahead, I will' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 310}\n",
            "--------------------------------------------------\n",
            "page_content='adjust my strategy and choose the number 32 for the first round. This way, I can be closer \n",
            "to the 0.8 * the average if other players also cluster around 40.\n",
            "real-time experience\n",
            "timely adjustment' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 311}\n",
            "--------------------------------------------------\n",
            "page_content='Figure 6: Illustration of Reflect and Refine methods in the Guessing 0.8 of the Average game.\n",
            "fine method more adaptable to dynamic environ-\n",
            "ments, as it can consider real-time changes in the\n",
            "current environment, thus making more accurate\n",
            "decisions.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 312}\n",
            "--------------------------------------------------\n",
            "page_content='In summary, the reason why the Refine method\n",
            "performed better in our experiment is mainly that\n",
            "it adapts better to rapidly changing dynamic envi-\n",
            "ronments.\n",
            "J Better Adaptability: Self-Refine vs.\n",
            "K-Level Reasoning' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 313}\n",
            "--------------------------------------------------\n",
            "page_content='As described in Section 2.1, K-R optimizes ini-\n",
            "tial actions through predictions of opponent behav-\n",
            "ior, thereby implementing higher-order beliefs. Al-\n",
            "though Self-Refine also optimizes initial actions via\n",
            "self-feedback, previous experiments have demon-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 314}\n",
            "--------------------------------------------------\n",
            "page_content='strated that K-R significantly outperforms Self-\n",
            "Refine. To further investigate the source of K-R’s\n",
            "performance enhancement, we present in Figure 7 a\n",
            "statistical analysis of action differences before and\n",
            "after optimization for both K-R and Self-Refine.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 315}\n",
            "--------------------------------------------------\n",
            "page_content='Analysis of the results reveals that Self-Refine\n",
            "initially exerts a substantial influence on actions.\n",
            "However, as time progresses, its optimization scope\n",
            "gradually diminishes, leading to a decrease in ef-\n",
            "Round 2 Round 4 Round 6 Round 8 Round 10\n",
            "0\n",
            "10\n",
            "20' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 316}\n",
            "--------------------------------------------------\n",
            "page_content='30\n",
            "40T uning Range\n",
            "K-R T uning Range\n",
            "Refine T uning Range\n",
            "Figure 7: The tuning range before and after adjustment\n",
            "during the G0.8A between PCoT and K-Level Reason-\n",
            "ing. A larger tuning range indicates that the method\n",
            "exerts a more substantial influence.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 317}\n",
            "--------------------------------------------------\n",
            "page_content='ficacy. In contrast, K-R exhibits a broader range\n",
            "of optimization, indicating that the higher-order\n",
            "belief mechanism has a more pronounced impact\n",
            "on K-R’s performance.\n",
            "This disparity primarily stems from the distinct' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 318}\n",
            "--------------------------------------------------\n",
            "page_content='feedback mechanisms employed by K-R and Self-\n",
            "Refine. K-R generates feedback based on predic-\n",
            "tions of opponent behavior, enabling it to capture\n",
            "trends in opponent actions and form an understand-\n",
            "17' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 319}\n",
            "--------------------------------------------------\n",
            "page_content='ing of the environment, thus facilitating rapid adap-\n",
            "tation. Conversely, Self-Refine’s feedback is de-\n",
            "rived from its own strategy, which often adheres to\n",
            "fixed patterns. Consequently, Self-Refine underper-' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 320}\n",
            "--------------------------------------------------\n",
            "page_content='forms K-R in terms of environmental adaptability.\n",
            "K Reasoning Method Implementation\n",
            "Details\n",
            "18' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 321}\n",
            "--------------------------------------------------\n",
            "page_content='Round Start Prompt:\n",
            "Ok, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.\n",
            "Example Output:\n",
            "I choose 45.\n",
            "Figure 8: G0.8A: Direct Prompt\n",
            "Reflect Prompt (Following Notification Result Prompt ):' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 322}\n",
            "--------------------------------------------------\n",
            "page_content='Review the previous round games, summarize the experience.\n",
            "Example Output:\n",
            "In the previous round, the majority of the players chose numbers around 40, which resulted in a lower average of 42.8. When m ultiplied by' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 323}\n",
            "--------------------------------------------------\n",
            "page_content='0.8, the target number was 34.24. My choice of 50 was too high, causing me to lose 1 HP . T o improve my chances in the next ro und, I should \n",
            "consider choosing a number closer to the majority's choice, taking into account the 0.8 multiplier .' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 324}\n",
            "--------------------------------------------------\n",
            "page_content='Figure 9: G0.8A-Reflect Prompt: Because the Reflect method requires correct labels, which is unrealistic in dynamic\n",
            "problems, we modified this method to have the LLM attempt to summarize the experience of victory or failure at' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 325}\n",
            "--------------------------------------------------\n",
            "page_content='the end of each round, and save the generated feedback content to assist the decision-making process in the next\n",
            "round. The content in Appendix I shows the implementation differences between Reflexion and Self-Refine.\n",
            "Round Start Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 326}\n",
            "--------------------------------------------------\n",
            "page_content='Ok, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round. Let's think step by step, and finally \n",
            "answer a number you think you can win.\n",
            "Example Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 327}\n",
            "--------------------------------------------------\n",
            "page_content='Step 1: Consider the range of possible numbers. Since the integers can be between 1 and 100, the average of all chosen number s will likely \n",
            "fall within this range as well.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 328}\n",
            "--------------------------------------------------\n",
            "page_content='Step 2: Determine the target number . Since the winning number is closest to 0.8 * the average, we should focus on finding a n umb er that \n",
            "will be close to this value.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 329}\n",
            "--------------------------------------------------\n",
            "page_content='Step 3: Make an educated guess. Since it's difficult to predict the exact numbers other players will choose, we can make an e ducated guess' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 330}\n",
            "--------------------------------------------------\n",
            "page_content='based on the range of possible numbers. Assuming players choose numbers evenly across the range, the average should be around  the \n",
            "midpoint, which is 50. \n",
            "Step 4: Calculate the target number based on our guess. 0.8 * 50 = 40.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 331}\n",
            "--------------------------------------------------\n",
            "page_content='Step 5: Choose a number close to the target number . To increase the chances of winning, we should choose a number close to 40 .\n",
            "My final answer: I will choose 40.\n",
            "Figure 10: G0.8A: CoT Prompt\n",
            "19' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 332}\n",
            "--------------------------------------------------\n",
            "page_content='Round Start Prompt:\n",
            "Ok, Alex! Now is the ROUND 2, and your HP is at 9. Please choose an integer between 1 and 100 for this round.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 333}\n",
            "--------------------------------------------------\n",
            "page_content='First of all, predict the next round of choices based on the choices of other players in the previous round. The players' cho ices in the \n",
            "previous rounds are as follows:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 334}\n",
            "--------------------------------------------------\n",
            "page_content='Round 1: Alex chose 50, Bob chose 47, Cindy chose 50, David chose 45, Eric chose 50. Your output should be of the following f ormat:\n",
            "Predict:\n",
            "The choice of each player in the next round here.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 335}\n",
            "--------------------------------------------------\n",
            "page_content='Based on the prediction of other players, the average number in the next round here, and the target number in the next round (0.8 * the \n",
            "average of all chosen numbers) here.\n",
            "Answer:\n",
            "the number will you choose to win the next round game here.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 336}\n",
            "--------------------------------------------------\n",
            "page_content='Example Output:\n",
            "Predict:\n",
            "Based on the choices of other players in the previous round, I predict the following choices for the next round:\n",
            "Player 1 (Bob): 47\n",
            "Player 2 (Cindy): 50\n",
            "Player 3 (David): 45\n",
            "Player 4 (Eric): 50' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 337}\n",
            "--------------------------------------------------\n",
            "page_content='Based on these predictions, the average number in the next round would be (47 + 50 + 45 + 50)/4 = 48, and the target number ( 0.8 * the \n",
            "average of all chosen numbers) would be 0.8 * 48 = 38.4.\n",
            "Answer:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 338}\n",
            "--------------------------------------------------\n",
            "page_content='For this round, I will choose 39 to get closer to the target number of 38.4.\n",
            "Figure 11: G0.8A PCot Prompt: PCot diverges from the standard Chain of Thought (CoT) by explicitly requiring' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 339}\n",
            "--------------------------------------------------\n",
            "page_content='the LLM to predict opponents’ actions before making a decision. This method responds to the immediate problem\n",
            "by anticipating future scenarios, which is crucial in dynamic reasoning.\n",
            "Introduction Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 340}\n",
            "--------------------------------------------------\n",
            "page_content='You are Alex and involved in a survive challenge. You are a game expert, good at predicting other people‘s behavior and deducing' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 341}\n",
            "--------------------------------------------------\n",
            "page_content='calculations, and using the most favorable strategy to win the game. SURVIVE CHALLENGE!!! You are one of five players in the game.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 342}\n",
            "--------------------------------------------------\n",
            "page_content='Everyone is required to choose an integer between 1 and 100 in each round. The player whose chosen number is closest to (0.8 * the' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 343}\n",
            "--------------------------------------------------\n",
            "page_content='average of all chosen numbers) wins the round without any HP deduction. All other players will have 1 HP deducted. But if all  players choose \n",
            "the same number , their health points are deducted together .\n",
            "Round Start Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 344}\n",
            "--------------------------------------------------\n",
            "page_content='Ok, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.Don't forget your expert \n",
            "status, use your expertise to win this round!\n",
            "Example Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 345}\n",
            "--------------------------------------------------\n",
            "page_content='Based on my expertise in predicting other people's behavior and deducing calculations, I will choose the number 40 for this r ound. I'm \n",
            "assuming that the average of all chosen numbers will be around 50, and 0.8 times that is 40.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 346}\n",
            "--------------------------------------------------\n",
            "page_content='Figure 12: G0.8A:Persona Prompt\n",
            "20' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 347}\n",
            "--------------------------------------------------\n",
            "page_content='Round Start Prompt:\n",
            "Ok, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round. Let's think step by step, and finally \n",
            "answer a number you think you can win.\n",
            "First Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 348}\n",
            "--------------------------------------------------\n",
            "page_content='I'll start by assuming that the other players might choose numbers randomly or evenly distributed between 1 and 100. In this case, the \n",
            "average of the chosen numbers could be around 50.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 349}\n",
            "--------------------------------------------------\n",
            "page_content='Next, I'll calculate 0.8 times the average, which would be 0.8 * 50 = 40. \n",
            "Considering this, I will choose 40 as my number for this round.\n",
            "Refine Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 350}\n",
            "--------------------------------------------------\n",
            "page_content='I have a game expert's advice on your strategy in this round. You can adjust your strategy just now according to his suggesti on. Here are his \n",
            "suggestions:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 351}\n",
            "--------------------------------------------------\n",
            "page_content='”Certainly! To optimize the user's strategy, we can apply a more advanced approach, taking into consideration the other playe rs' potential \n",
            "strategies. This is known as \"iterated elimination of dominated strategies.\"' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 352}\n",
            "--------------------------------------------------\n",
            "page_content='Let's assume that all players will think similarly to Alex and also aim to choose a number closest to 0.8 * the average. We c an then narrow \n",
            "down the range of numbers that they would likely choose.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 353}\n",
            "--------------------------------------------------\n",
            "page_content='Since the range is between 1 and 100, and the average would be around 50, we can eliminate numbers below 0.8 * 50 = 40, as th ese \n",
            "numbers would not be an optimal choice for the other players.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 354}\n",
            "--------------------------------------------------\n",
            "page_content='Now, we'll focus on the range between 40 and 100. The average within this range is (40 + 100) / 2 = 70. Then, we can calculat e 0.8 * 70 = 56.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 355}\n",
            "--------------------------------------------------\n",
            "page_content='Given this information, Alex should choose 56 to improve his winning rate in this round. This strategy assumes that the other  players are \n",
            "also thinking strategically and eliminating dominated options. ”\n",
            "Second Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 356}\n",
            "--------------------------------------------------\n",
            "page_content='Thank you for the expert's advice. Based on the iterated elimination of dominated strategies, I will adjust my choice and sel ect 56 for this \n",
            "round to improve my chances of winning.\n",
            "Feedback Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 357}\n",
            "--------------------------------------------------\n",
            "page_content='Carefully study the user's strategy in this round of the game. As a game expert, can you give a suggestion to optimize the us er's strategy so \n",
            "that he can improve his winning rate in this round?\n",
            "Figure 13: G0.8A: Refine Prompt\n",
            "Round Start Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 358}\n",
            "--------------------------------------------------\n",
            "page_content='Ok, Alex! Now is the ROUND 1, and your HP is at 10. Another game expert's prediction for the next round of other players is a s follows: Bob' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 359}\n",
            "--------------------------------------------------\n",
            "page_content='might choose 50, Cindy might choose 57, David might choose 45, Eric might choose 55. According to the game expert's predictio n, what \n",
            "number will you give in the next round is most likely to be close to the target number?\n",
            "Example Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 360}\n",
            "--------------------------------------------------\n",
            "page_content='First, let's find the average of the predicted numbers chosen by the other players: (50 + 57 + 45 + 55) / 4 = 207 / 4 = 51.75\n",
            "Now, let's calculate 0.8 times the average: 0.8 * 51.75 = 41.4' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 361}\n",
            "--------------------------------------------------\n",
            "page_content='To have the best chance of winning the round, you should choose a number close to 41.4. Since you can only choose integers, y ou should \n",
            "choose either 41 or 42 in the next round.\n",
            "Figure 14: G0.8A: K-Level Reasoning\n",
            "Round Start Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 362}\n",
            "--------------------------------------------------\n",
            "page_content='Hello, Alex! T oday is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Y our status:\n",
            "name: Alex   balance: 100 HP:8 no_drink:1' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 363}\n",
            "--------------------------------------------------\n",
            "page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you \n",
            "want to participate in today's water resource auction, please provide your bid.\n",
            "Example Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 364}\n",
            "--------------------------------------------------\n",
            "page_content='I will bid $25 for today's water resource auction.\n",
            "Figure 15: SAG: Direct Prompt\n",
            "21' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 365}\n",
            "--------------------------------------------------\n",
            "page_content='Reflect Prompt (Following Notification Result Prompt ):\n",
            "Review the previous round games, summarize the experience.\n",
            "Example Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 366}\n",
            "--------------------------------------------------\n",
            "page_content='In the previous round, I bid $20 which was not enough to secure the water resources. I need to adjust my bidding strategy by increasing my' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 367}\n",
            "--------------------------------------------------\n",
            "page_content='bid to have a better chance of winning the auction. I should also observe and adapt to other residents' bidding patterns to i mprove my \n",
            "chances of survival.\n",
            "Figure 16: SAG: Reflect Prompt\n",
            "Round Start Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 368}\n",
            "--------------------------------------------------\n",
            "page_content='Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n",
            "NAME:Alex  BALANCE:100  HEALTH POINT:8  NO_DRINK:1' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 369}\n",
            "--------------------------------------------------\n",
            "page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 370}\n",
            "--------------------------------------------------\n",
            "page_content='want to participate in today‘s water resource auction, please provide your bid. Think carefully about your next round of bidding strategy to \n",
            "be most likely to survive. Let's think step by step, and finally provide your bid.\n",
            "Example Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 371}\n",
            "--------------------------------------------------\n",
            "page_content='I need to carefully consider my bidding strategy for today. With a balance of $100, I have some room to bid, but I also need to think about' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 372}\n",
            "--------------------------------------------------\n",
            "page_content='the other residents and their bids. Since we all have the same daily water requirement and budget, I need to outbid at least one resident to \n",
            "secure my water resources for today.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 373}\n",
            "--------------------------------------------------\n",
            "page_content='Considering that the total daily water supply is 10 units, which is enough to meet only one resident's water requirements, th e competition \n",
            "will be tough. However , bidding too high might leave me with less money for the upcoming days.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 374}\n",
            "--------------------------------------------------\n",
            "page_content='Taking all these factors into account, I will place a bid of $60 for today's water resource auction. This should give me a de cent chance of \n",
            "outbidding at least one resident while still maintaining some funds for the next few days.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 375}\n",
            "--------------------------------------------------\n",
            "page_content='Figure 17: SAG: CoT Prompt\n",
            "Round Start Prompt:\n",
            "Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n",
            "NAME:Alex  BALANCE:100  HEALTH POINT:8  NO_DRINK:1' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 376}\n",
            "--------------------------------------------------\n",
            "page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 377}\n",
            "--------------------------------------------------\n",
            "page_content='want to participate in today‘s water resource auction, please provide your bid. First of all, predict the next round of bidding of opponents' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 378}\n",
            "--------------------------------------------------\n",
            "page_content='based on the choices of other players in the previous round. Since this is the first round, there is no historical informatio n about the last \n",
            "round. You can predict according to your understanding. Your output should be of the following format:\n",
            "Predict:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 379}\n",
            "--------------------------------------------------\n",
            "page_content='The choice of each player in the next round here.\n",
            "Based on the prediction of other players, think carefully about your next round of bidding strategy to be most likely to surv ive. Let's think \n",
            "step by step, and finally provide your bid. Answer:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 380}\n",
            "--------------------------------------------------\n",
            "page_content='the bidding will you choose in the next round game here.\n",
            "Example Output:\n",
            "Predict:\n",
            "Player 1: $30\n",
            "Player 2: $40\n",
            "Player 3: $50\n",
            "Player 4: $60\n",
            "Answer:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 381}\n",
            "--------------------------------------------------\n",
            "page_content='Since this is the first round and we don't have any historical information about the other players, I will choose a bid of $3 5 f or the next' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 382}\n",
            "--------------------------------------------------\n",
            "page_content='round. This bid is higher than the lowest predicted bid but not too high, which should give me a chance to secure water resou rces while \n",
            "preserving my budget.\n",
            "Figure 18: SAG: PCoT Prompt\n",
            "22' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 383}\n",
            "--------------------------------------------------\n",
            "page_content='Introduction Prompt:\n",
            "You are Alex and a resident living in W-Town. W Town is experiencing a rare drought. Every residents in Town W is ensuring their survival' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 384}\n",
            "--------------------------------------------------\n",
            "page_content='over a period of 10 days by acquiring the water resources.  You are a game expert, good at predicting other people‘s behavior and' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 385}\n",
            "--------------------------------------------------\n",
            "page_content='deducing calculations, and using the most favorable strategy to win the game. 。。。(The following is the same as the Introduction \n",
            "Prompt.)\n",
            "Round Start Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 386}\n",
            "--------------------------------------------------\n",
            "page_content='Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n",
            "NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 387}\n",
            "--------------------------------------------------\n",
            "page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 388}\n",
            "--------------------------------------------------\n",
            "page_content='want to participate in today's water resource auction, please provide your bid. Don't forget your expert status, use your exp ertise to win this \n",
            "round!\n",
            "Example Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 389}\n",
            "--------------------------------------------------\n",
            "page_content='Given that everyone starts with the same budget and water requirements, it's important to strategize and predict the bidding behavior of' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 390}\n",
            "--------------------------------------------------\n",
            "page_content='the other residents. As it is the first day of the challenge, I will assume that other residents may bid aggressively to secu re their water \n",
            "resources.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 391}\n",
            "--------------------------------------------------\n",
            "page_content='To increase my chances of winning, I will place a bid slightly higher than the average expected bid. Since the daily salary i s $100, I predict \n",
            "most residents might bid around $60 to $80.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 392}\n",
            "--------------------------------------------------\n",
            "page_content='Considering this, I will place a bid of $85 for Day 1 of the Water Allocation Challenge.\n",
            "Figure 19: SAG: Persona Prompt\n",
            "23' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 393}\n",
            "--------------------------------------------------\n",
            "page_content='Round Start Prompt:\n",
            "Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n",
            "NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 394}\n",
            "--------------------------------------------------\n",
            "page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 395}\n",
            "--------------------------------------------------\n",
            "page_content='want to participate in today's water resource auction, please provide your bid. Think carefully about your next round of bidd ing strategy to \n",
            "be most likely to survive. Let's think step by step, and finally provide your bid.\n",
            "First Output:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 396}\n",
            "--------------------------------------------------\n",
            "page_content='Considering the current situation, my status is:\n",
            "- Balance: $100\n",
            "- Health Points: 8\n",
            "- No Drink Days: 1\n",
            "As it's the first day of the challenge, I want to ensure that I have enough water to meet my needs and avoid any health point  deductions.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 397}\n",
            "--------------------------------------------------\n",
            "page_content='However , I also need to be strategic about my bidding to ensure I have enough money for future bids. \n",
            "Since the total daily water supply is 10 units, and I need 10 units of water per day, I will need to secure all the available  water resources for' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 398}\n",
            "--------------------------------------------------\n",
            "page_content='today. As it's the first day, other residents might also be bidding aggressively to secure their water resources.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 399}\n",
            "--------------------------------------------------\n",
            "page_content='Therefore, I will bid $60 for the water resources today, which is a little over half of my daily salary. This should give me a competitive edge \n",
            "while also leaving me with enough money for future bids.\n",
            "My bid for today's water resource auction: $60.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 400}\n",
            "--------------------------------------------------\n",
            "page_content='Refine Prompt:\n",
            "I have a game expert's advice on your strategy in this round. You can adjust your strategy just now according to his suggesti on. Here are his' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 401}\n",
            "--------------------------------------------------\n",
            "page_content='suggestions: While the user's strategy of bidding $60 is a good starting point, there is room for optimization to improve the  winning rate in' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 402}\n",
            "--------------------------------------------------\n",
            "page_content='this round. Since everyone has the same daily water requirement and budget, it's essential to consider the bidding behavior o f the other \n",
            "residents.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 403}\n",
            "--------------------------------------------------\n",
            "page_content='A suggested optimization would be to slightly increase the bid to $65. This increment gives an added advantage over competito rs who might' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 404}\n",
            "--------------------------------------------------\n",
            "page_content='also be bidding around half of their daily salary. Additionally, bidding an odd number reduces the chances of a tie, as other  residents may \n",
            "place round number bids.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 405}\n",
            "--------------------------------------------------\n",
            "page_content='With this optimized strategy, the user still maintains a good balance of aggressive bidding and budget management, increasing  their chances \n",
            "of securing the water resources for today.\n",
            "Optimized bid for today's water resource auction: $65.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 406}\n",
            "--------------------------------------------------\n",
            "page_content='Finally provide your bid. Answer: the bidding will you choose.\n",
            "Second Output:\n",
            "After considering the game expert's advice, I have decided to adjust my bid for today's water resource auction. I will increa se my bid to $65,' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 407}\n",
            "--------------------------------------------------\n",
            "page_content='as this gives me a better chance of winning the auction and reduces the risk of a tie.\n",
            "My final bid for today's water resource auction: $65.\n",
            "Feedback Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 408}\n",
            "--------------------------------------------------\n",
            "page_content='Carefully study the user's strategy in this round of the game. As a game expert, can you give a suggestion to optimize the us er's strategy so \n",
            "that he can improve his winning rate in this round?\n",
            "Figure 20: SAG: Refine Prompt\n",
            "Round Start Prompt:' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 409}\n",
            "--------------------------------------------------\n",
            "page_content='Hello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n",
            "NAME:Alex BALANCE:100 HEALTH POINT:8. NO_DRINK:1' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 410}\n",
            "--------------------------------------------------\n",
            "page_content='Please carefully analyze your situation to decide on this round of bidding. Remember , the most important thing is to SURVIVE! ! N ow, if you' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 411}\n",
            "--------------------------------------------------\n",
            "page_content='want to participate in today's water resource auction, please provide your bid. Another game expert's prediction for the next  round of other' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 412}\n",
            "--------------------------------------------------\n",
            "page_content='players is as follows: Bob might choose 30, Cindy might choose 30, David might choose 40, Eric might choose 25.  Based on the  prediction of \n",
            "other players, think carefully about your next round of bidding strategy to be most likely to survive.' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 413}\n",
            "--------------------------------------------------\n",
            "page_content='Example Output:\n",
            "Given the game expert's prediction, I will bid $35 to increase my chances of winning the water resources for today.\n",
            "Figure 21: SAG: K-Level Reasoning Prompt\n",
            "24' metadata={'source': 'KR-reasoning-LLM-framework.pdf', 'chunk_index': 414}\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "def convert_chunks_to_documents(chunks, metadata=None):\n",
        "    \"\"\"\n",
        "    Converts chunks into a list of Document objects with metadata.\n",
        "\n",
        "    Args:\n",
        "    - chunks: List of chunks (list of strings or objects with `page_content`).\n",
        "    - metadata: A dictionary containing metadata to attach to each document.\n",
        "\n",
        "    Returns:\n",
        "    - List of Document objects.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        content = chunk.page_content if hasattr(chunk, 'page_content') else chunk\n",
        "        doc_metadata = metadata.copy() if metadata else {}\n",
        "        doc_metadata['chunk_index'] = i  # Add chunk-specific metadata\n",
        "        documents.append(Document(page_content=content, metadata=doc_metadata))\n",
        "    return documents\n",
        "\n",
        "# Example usage:\n",
        "# Assuming `chunks` is the list of text chunks\n",
        "metadata = {'source': 'KR-reasoning-LLM-framework.pdf'}\n",
        "documents = convert_chunks_to_documents(chunks, metadata)\n",
        "\n",
        "# Print the resulting Document objects\n",
        "for doc in documents:\n",
        "    print(doc)\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYRH2Yqef7rS"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-pinecone pinecone-notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGa1JlyfD3qJ",
        "outputId": "d9694cda-58b4-4c89-b014-ec8b36716198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating index askadoucment and embeddings ...index created....\n",
            "Ok\n"
          ]
        }
      ],
      "source": [
        "index_name= \"askadoucment\"\n",
        "vector_store = insert_or_fetch_embeddings(index_name, documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv6zVwLBHbtm"
      },
      "source": [
        "# **QA---------------------------**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwPiOJMNHbgn"
      },
      "outputs": [],
      "source": [
        "def ask_and_get_answer(vector_store, q):\n",
        "  from langchain.chains import RetrievalQA\n",
        "  from langchain_openai import ChatOpenAI\n",
        "\n",
        "  llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1, api_key=openai_api_key)\n",
        "\n",
        "  retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 12})\n",
        "\n",
        "  print(retriever)\n",
        "\n",
        "  chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "  print(chain)\n",
        "\n",
        "  answer = chain.invoke(q)\n",
        "  return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeOupvL-D6JY",
        "outputId": "fb92fd13-8724-41dd-fcb3-e599c8fa4ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tags=['PineconeVectorStore', 'OpenAIEmbeddings'] vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x7d33936ecf70> search_kwargs={'k': 12}\n",
            "verbose=False combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d33936eeec0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d33841cec50>, root_client=<openai.OpenAI object at 0x7d339138ec20>, root_async_client=<openai.AsyncOpenAI object at 0x7d33936ecf10>, temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x7d33936ecf70>, search_kwargs={'k': 12})\n",
            "{'query': 'Who are the authors on the K level LLM reasoning paper?', 'result': \"I'm not sure about the specific authors of the K level LLM reasoning paper.\"}\n"
          ]
        }
      ],
      "source": [
        "q = \"Who are the authors on the K level LLM reasoning paper?\"\n",
        "answer = ask_and_get_answer(vector_store, q)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMAt3JVUCz69",
        "outputId": "7e8522e9-35d8-405b-c5f5-27a095ebdae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/microsoft-game-theory-alympics - .pdf\n",
            "Loading /content/microsoft-game-theory-alympics.pdf\n",
            "Length of data 17\n",
            "Length of data 269\n"
          ]
        }
      ],
      "source": [
        "data = load_document('/content/microsoft-game-theory-alympics.pdf')\n",
        "print(f'Length of data {len(data)}')\n",
        "chunks = chunk_data(data)\n",
        "print(f'Length of data {len(chunks)}')\n",
        "\n",
        "metadata = {'source': 'alympics'}\n",
        "documents = convert_chunks_to_documents(chunks, metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joS3OeZT-N74",
        "outputId": "0f149d87-05c0-450e-89a4-27271f2537ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting all indexes ... \n",
            "Ok\n"
          ]
        }
      ],
      "source": [
        "delete_pinecone_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LfPcboN-L6d",
        "outputId": "62f5849a-770f-46d6-d396-c56027bdf7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating index askadoucment and embeddings ...index created....\n",
            "Ok\n"
          ]
        }
      ],
      "source": [
        "vector_store = insert_or_fetch_embeddings(index_name, documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmw87rlk-Ge4",
        "outputId": "2af5c7d3-868d-4943-9be3-edcf4becab30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tags=['PineconeVectorStore', 'OpenAIEmbeddings'] vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x7d337d825570> search_kwargs={'k': 12}\n",
            "verbose=False combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d3391379450>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d3392e8f460>, root_client=<openai.OpenAI object at 0x7d33841a69e0>, root_async_client=<openai.AsyncOpenAI object at 0x7d3391379420>, temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x7d337d825570>, search_kwargs={'k': 12})\n",
            "{'query': 'Who are the authors on the alympics microsoft paper: Exploring Strategic Decision-Making with AI Agents Shaoguang Mao?', 'result': 'I\\'m sorry, but I don\\'t have access to specific information about the authors on the paper \"Exploring Strategic Decision-Making with AI Agents\" by Shaoguang Mao.'}\n"
          ]
        }
      ],
      "source": [
        "q = \"Who are the authors on the alympics microsoft paper: Exploring Strategic Decision-Making with AI Agents Shaoguang Mao?\"\n",
        "answer = ask_and_get_answer(vector_store, q)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UzZxgKouxvH",
        "outputId": "2e949038-5c3d-4ee6-dfe8-132eee83c60a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tags=['PineconeVectorStore', 'OpenAIEmbeddings'] vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x7d337d825570> search_kwargs={'k': 12}\n",
            "verbose=False combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d33936cb3a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d33936ecf70>, root_client=<openai.OpenAI object at 0x7d33936efa60>, root_async_client=<openai.AsyncOpenAI object at 0x7d33936c9930>, temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x7d337d825570>, search_kwargs={'k': 12})\n",
            "{'query': 'What is the point of the alympics microsoft paper: Exploring Strategic Decision-Making with AI Agents Shaoguang Mao', 'result': 'I don\\'t have specific information on the content or purpose of the paper \"Exploring Strategic Decision-Making with AI Agents\" by Shaoguang Mao. It seems to be related to strategic decision-making with AI agents in the context of the Olympics. For specific details or insights, it would be best to read the paper or consult resources related to the topic.'}\n"
          ]
        }
      ],
      "source": [
        "q = \"What is the point of the alympics microsoft paper: Exploring Strategic Decision-Making with AI Agents Shaoguang Mao\"\n",
        "answer = ask_and_get_answer(vector_store, q)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kescEL0tWM1"
      },
      "source": [
        "# **CHROMA-----------------------**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeH66fl5mg1N",
        "outputId": "fe9c3730-5b0f-4301-974e-8e6e91fe24ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.5.23)\n",
            "Requirement already satisfied: fastapi<1,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.115.6)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.3.29)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (1.26.4)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.10.4)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.7.6)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.34.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.7.5)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.20.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.20.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.69.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (4.2.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.15.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (31.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (5.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.10.13)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.41.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.9)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (1.0.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (24.12.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.66.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.29.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (14.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.4.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pKqxYXitZ2d"
      },
      "outputs": [],
      "source": [
        "def create_embeddings_chroma(documents, persist_directory='./chroma_db'):\n",
        "  from langchain.vectorstores import Chroma\n",
        "  from langchain_openai import OpenAIEmbeddings\n",
        "  from uuid import uuid4\n",
        "\n",
        "  embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=512, api_key=openai_api_key)\n",
        "  vector_store = Chroma(\n",
        "    collection_name=\"ask_doc_collection\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=persist_directory,\n",
        "  )\n",
        "\n",
        "  uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "  vector_store.add_documents(documents=documents, ids=uuids)\n",
        "\n",
        "  return vector_store\n",
        "\n",
        "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
        "  from langchain.vectorstores import Chroma\n",
        "  from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "  embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=512, api_key=openai_api_key)\n",
        "  vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "  return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFQpNRqMtZ0F",
        "outputId": "1207ccb7-0126-4d9c-c7e4-5133f7ee1623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Tree-of-Through-Princeton-Deepmind-LLMs - .pdf\n",
            "Loading /content/Tree-of-Through-Princeton-Deepmind-LLMs.pdf\n",
            "Length of data 14\n",
            "Length of data 287\n"
          ]
        }
      ],
      "source": [
        "data = load_document('/content/Tree-of-Through-Princeton-Deepmind-LLMs.pdf')\n",
        "print(f'Length of data {len(data)}')\n",
        "chunks = chunk_data(data)\n",
        "print(f'Length of data {len(chunks)}')\n",
        "\n",
        "metadata = {'source': 'alympics'}\n",
        "documents = convert_chunks_to_documents(chunks, metadata)\n",
        "vector_store_chroma = create_embeddings_chroma(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjwwH-nT-nVZ",
        "outputId": "6fa5e7f6-613a-45c3-bf99-020a66206e67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7d339311fac0> search_kwargs={'k': 12}\n",
            "verbose=False combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d337d88dff0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d339137abf0>, root_client=<openai.OpenAI object at 0x7d337d88fa00>, root_async_client=<openai.AsyncOpenAI object at 0x7d337d88e170>, temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7d339311fac0>, search_kwargs={'k': 12})\n",
            "{'query': 'what is tree of thought from princeton and who published it?', 'result': 'The Tree of Thought is a framework for deliberate problem-solving with large language models proposed by Shunyu Yao from Princeton University, Dian Yu, Jeffrey Zhao, Izhak Shafran, and Yuan Cao from Google DeepMind, and Thomas L. Griffiths from Princeton University. This framework generalizes over the popular \"Chain of Thought\" approach to prompting language models and enables exploration over coherent units of text (\"thoughts\") that serve as intermediate steps.'}\n"
          ]
        }
      ],
      "source": [
        "q = \"what is tree of thought from princeton and who published it?\"\n",
        "answer = ask_and_get_answer(vector_store_chroma, q)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TK9D0miCSTW",
        "outputId": "e58496b2-21ca-4c0f-efb0-8f9f04284385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7d339311fac0> search_kwargs={'k': 12}\n",
            "verbose=False combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d338419ab90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d338419bca0>, root_client=<openai.OpenAI object at 0x7d3393ec19c0>, root_async_client=<openai.AsyncOpenAI object at 0x7d338419ac80>, temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7d339311fac0>, search_kwargs={'k': 12})\n",
            "{'query': 'Was google involved with tree of thought LLM research?', 'result': 'Yes, Google was involved in the Tree of Thoughts (ToT) research on deliberate problem-solving with Large Language Models. Dian Yu, Jeffrey Zhao, Izhak Shafran, and Yuan Cao from Google DeepMind were part of the research team.'}\n"
          ]
        }
      ],
      "source": [
        "db = load_embeddings_chroma('./chroma_db')\n",
        "q = \"Was google involved with tree of thought LLM research?\"\n",
        "answer = ask_and_get_answer(vector_store_chroma, q)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEvPNdRou7eB"
      },
      "source": [
        "# **ADDING MEMORY------------------**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-ooxnrvtZxm"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains\n",
        "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history\n",
        "\n",
        "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.2, api_key=openai_api_key)\n",
        "\n",
        "# Configure vector store to act as a retriever (finding similar items, returning top 12)\n",
        "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 12})\n",
        "\n",
        "\n",
        "# Create a memory buffer to track the conversation\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "crc = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,  # Link the ChatGPT LLM\n",
        "    retriever=retriever,  # Link the vector store based retriever\n",
        "    memory=memory,  # Link the conversation memory\n",
        "    chain_type='stuff',  # Specify the chain type\n",
        "    verbose=False  # Set to True to enable verbose logging for debugging\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deTHnogxtZwG"
      },
      "outputs": [],
      "source": [
        "# create a function to ask questions\n",
        "def ask_question(q, chain):\n",
        "    result = chain.invoke({'question': q})\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_q_j1HTTSsR",
        "outputId": "5bb3b7c4-1ca5-4656-e47b-2c1ba93e0f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/KR-reasoning-LLM-framework - .pdf\n",
            "Loading /content/KR-reasoning-LLM-framework.pdf\n",
            "Length of data 24\n",
            "Length of data 415\n"
          ]
        }
      ],
      "source": [
        "data = load_document('/content/KR-reasoning-LLM-framework.pdf')\n",
        "print(f'Length of data {len(data)}')\n",
        "chunks = chunk_data(data)\n",
        "print(f'Length of data {len(chunks)}')\n",
        "\n",
        "metadata = {'source': 'alympics'}\n",
        "documents = convert_chunks_to_documents(chunks, metadata)\n",
        "vector_store_chroma = create_embeddings_chroma(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "eIBDsIBvT2J8",
        "outputId": "9447837d-2008-46d6-a75a-ca15e6f21487"
      },
      "outputs": [
        {
          "ename": "UnauthorizedException",
          "evalue": "(401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'Date': 'Mon, 13 Jan 2025 03:09:22 GMT', 'Content-Type': 'text/plain', 'Content-Length': '12', 'Connection': 'keep-alive', 'x-pinecone-auth-rejected-reason': 'Malformed domain', 'www-authenticate': 'Malformed domain', 'server': 'envoy'})\nHTTP response body: Unauthorized\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnauthorizedException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-906130db84f5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'How many ways did KR reasoning LLM framwork paper evaluate their system?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-964f5b4db21c>\u001b[0m in \u001b[0;36mask_question\u001b[0;34m(q, chain)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a function to ask questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             outputs = (\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    157\u001b[0m         )\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\u001b[0m in \u001b[0;36m_get_docs\u001b[0;34m(self, question, inputs, run_manager)\u001b[0m\n\u001b[1;32m    394\u001b[0m     ) -> List[Document]:\n\u001b[1;32m    395\u001b[0m         \u001b[0;34m\"\"\"Get docs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         docs = self.retriever.invoke(\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_retriever_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             run_manager.on_retriever_end(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_other_args\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 result = self._get_relevant_documents(\n\u001b[0m\u001b[1;32m    260\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36m_get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity_score_threshold\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             docs_and_similarities = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query, k, filter, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[0;32m--> 380\u001b[0;31m         docs_and_scores = self.similarity_search_with_score(\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search_with_score\u001b[0;34m(self, query, k, filter, namespace)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         return self.similarity_search_by_vector_with_score(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search_by_vector_with_score\u001b[0;34m(self, embedding, k, filter, namespace)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         results = self._index.query(\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/utils/error_handling.py\u001b[0m in \u001b[0;36minner_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/data/index.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \"\"\"\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         response = self._query(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/data/index.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m         )\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         response = self._vector_api.query(\n\u001b[0m\u001b[1;32m    500\u001b[0m             QueryRequest(\n\u001b[1;32m    501\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \"\"\"\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/data/api/data_plane_api.py\u001b[0m in \u001b[0;36m__query\u001b[0;34m(self, query_request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_host_index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_host_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query_request\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         self.query = _Endpoint(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mcall_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         return self.api_client.call_api(\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"endpoint_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_method\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, async_threadpool_executor, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masync_req\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             return self.__call_api(\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mresource_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPineconeApiException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# perform request and return response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             response_data = self.request(\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[1;32m    517\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             return self.rest_client.POST(\n\u001b[0m\u001b[1;32m    519\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     ):\n\u001b[0;32m--> 345\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorizedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnauthorizedException\u001b[0m: (401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'Date': 'Mon, 13 Jan 2025 03:09:22 GMT', 'Content-Type': 'text/plain', 'Content-Length': '12', 'Connection': 'keep-alive', 'x-pinecone-auth-rejected-reason': 'Malformed domain', 'www-authenticate': 'Malformed domain', 'server': 'envoy'})\nHTTP response body: Unauthorized\n"
          ]
        }
      ],
      "source": [
        "q = 'How many ways did KR reasoning LLM framwork paper evaluate their system?'\n",
        "result = ask_question(q, crc)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0z9Bkm6Utue",
        "outputId": "0e8fa295-bb0a-4511-ddbc-1b9bf484d092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The KR reasoning LLM framework paper evaluated their system in five ways. They assessed \"Information Utilization (IU)\", \"Logical Reasoning (LR)\", \"Strategic Effectiveness (SE)\", \"Adaptability and Strategic Evolution (AD)\", and \"Long-term Planning (LP)\" on a scale from 1 to 5.\n"
          ]
        }
      ],
      "source": [
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "IX_iGWi7UCgH",
        "outputId": "efd4dc52-d381-480f-f8fb-60ccce6c3eb4"
      },
      "outputs": [
        {
          "ename": "UnauthorizedException",
          "evalue": "(401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'Date': 'Mon, 13 Jan 2025 03:08:59 GMT', 'Content-Type': 'text/plain', 'Content-Length': '12', 'Connection': 'keep-alive', 'x-pinecone-auth-rejected-reason': 'Malformed domain', 'www-authenticate': 'Malformed domain', 'server': 'envoy'})\nHTTP response body: Unauthorized\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnauthorizedException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-20a25e572307>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'could you tell me the total number of those eval strategies, take your best guess, and then divide it by 2?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-964f5b4db21c>\u001b[0m in \u001b[0;36mask_question\u001b[0;34m(q, chain)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a function to ask questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             outputs = (\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    157\u001b[0m         )\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\u001b[0m in \u001b[0;36m_get_docs\u001b[0;34m(self, question, inputs, run_manager)\u001b[0m\n\u001b[1;32m    394\u001b[0m     ) -> List[Document]:\n\u001b[1;32m    395\u001b[0m         \u001b[0;34m\"\"\"Get docs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         docs = self.retriever.invoke(\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_retriever_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             run_manager.on_retriever_end(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_other_args\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 result = self._get_relevant_documents(\n\u001b[0m\u001b[1;32m    260\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36m_get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity_score_threshold\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             docs_and_similarities = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query, k, filter, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[0;32m--> 380\u001b[0;31m         docs_and_scores = self.similarity_search_with_score(\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search_with_score\u001b[0;34m(self, query, k, filter, namespace)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         return self.similarity_search_by_vector_with_score(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search_by_vector_with_score\u001b[0;34m(self, embedding, k, filter, namespace)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         results = self._index.query(\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/utils/error_handling.py\u001b[0m in \u001b[0;36minner_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/data/index.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \"\"\"\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         response = self._query(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/data/index.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m         )\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         response = self._vector_api.query(\n\u001b[0m\u001b[1;32m    500\u001b[0m             QueryRequest(\n\u001b[1;32m    501\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \"\"\"\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/data/api/data_plane_api.py\u001b[0m in \u001b[0;36m__query\u001b[0;34m(self, query_request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_host_index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_host_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query_request\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         self.query = _Endpoint(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mcall_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         return self.api_client.call_api(\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"endpoint_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_method\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, async_threadpool_executor, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masync_req\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             return self.__call_api(\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mresource_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPineconeApiException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# perform request and return response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             response_data = self.request(\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[1;32m    517\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             return self.rest_client.POST(\n\u001b[0m\u001b[1;32m    519\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     ):\n\u001b[0;32m--> 345\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorizedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnauthorizedException\u001b[0m: (401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'Date': 'Mon, 13 Jan 2025 03:08:59 GMT', 'Content-Type': 'text/plain', 'Content-Length': '12', 'Connection': 'keep-alive', 'x-pinecone-auth-rejected-reason': 'Malformed domain', 'www-authenticate': 'Malformed domain', 'server': 'envoy'})\nHTTP response body: Unauthorized\n"
          ]
        }
      ],
      "source": [
        "q = 'could you tell me the total number of those eval strategies, take your best guess, and then divide it by 2?'\n",
        "result = ask_question(q, crc)\n",
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB8-GJeJUvx2",
        "outputId": "496d143d-e72d-4cc1-b87c-081e4bd96394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total number of evaluation strategies used in the KR reasoning LLM framework paper is not explicitly mentioned in the provided context. Therefore, I do not know the exact number of evaluation strategies used in that specific paper. Consequently, I cannot provide the result of dividing that number by 2.\n"
          ]
        }
      ],
      "source": [
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "2ye6wLcwVAP7",
        "outputId": "d053d04d-09af-4534-95a1-ddb843409307"
      },
      "outputs": [
        {
          "ename": "UnauthorizedException",
          "evalue": "(401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'Date': 'Mon, 13 Jan 2025 03:08:38 GMT', 'Content-Type': 'text/plain', 'Content-Length': '12', 'Connection': 'keep-alive', 'x-pinecone-auth-rejected-reason': 'Malformed domain', 'www-authenticate': 'Malformed domain', 'server': 'envoy'})\nHTTP response body: Unauthorized\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnauthorizedException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-69a6fe3fadb2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'divide that number by 5?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-964f5b4db21c>\u001b[0m in \u001b[0;36mask_question\u001b[0;34m(q, chain)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a function to ask questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             outputs = (\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    157\u001b[0m         )\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\u001b[0m in \u001b[0;36m_get_docs\u001b[0;34m(self, question, inputs, run_manager)\u001b[0m\n\u001b[1;32m    394\u001b[0m     ) -> List[Document]:\n\u001b[1;32m    395\u001b[0m         \u001b[0;34m\"\"\"Get docs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         docs = self.retriever.invoke(\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_retriever_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             run_manager.on_retriever_end(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_other_args\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 result = self._get_relevant_documents(\n\u001b[0m\u001b[1;32m    260\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36m_get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity_score_threshold\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             docs_and_similarities = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query, k, filter, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[0;32m--> 380\u001b[0;31m         docs_and_scores = self.similarity_search_with_score(\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search_with_score\u001b[0;34m(self, query, k, filter, namespace)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         return self.similarity_search_by_vector_with_score(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_pinecone/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search_by_vector_with_score\u001b[0;34m(self, embedding, k, filter, namespace)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         results = self._index.query(\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/utils/error_handling.py\u001b[0m in \u001b[0;36minner_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/data/index.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \"\"\"\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         response = self._query(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/data/index.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m         )\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         response = self._vector_api.query(\n\u001b[0m\u001b[1;32m    500\u001b[0m             QueryRequest(\n\u001b[1;32m    501\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \"\"\"\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/data/api/data_plane_api.py\u001b[0m in \u001b[0;36m__query\u001b[0;34m(self, query_request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_host_index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_host_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query_request\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         self.query = _Endpoint(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mcall_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         return self.api_client.call_api(\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"endpoint_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_method\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, async_threadpool_executor, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masync_req\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             return self.__call_api(\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mresource_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPineconeApiException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# perform request and return response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             response_data = self.request(\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[1;32m    517\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             return self.rest_client.POST(\n\u001b[0m\u001b[1;32m    519\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     ):\n\u001b[0;32m--> 345\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/core/openapi/shared/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorizedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnauthorizedException\u001b[0m: (401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'Date': 'Mon, 13 Jan 2025 03:08:38 GMT', 'Content-Type': 'text/plain', 'Content-Length': '12', 'Connection': 'keep-alive', 'x-pinecone-auth-rejected-reason': 'Malformed domain', 'www-authenticate': 'Malformed domain', 'server': 'envoy'})\nHTTP response body: Unauthorized\n"
          ]
        }
      ],
      "source": [
        "q = 'divide that number by 5?'\n",
        "result = ask_question(q, crc)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKkoQfg0VGSX",
        "outputId": "60d743e0-bec2-4f80-c166-859a8846053d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To find the result of dividing the total number of evaluation strategies used in the KR reasoning LLM framework paper by 5, we need to know the specific number of evaluation strategies mentioned in the text. Unfortunately, the text provided does not contain a specific number for the total evaluation strategies used in the KR reasoning LLM framework paper. Therefore, without that specific number, I cannot calculate the result of dividing it by 5.\n"
          ]
        }
      ],
      "source": [
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXRDBGYWVvbJ",
        "outputId": "c1491a26-ac82-4674-8bb5-4e6d6273f511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Who wrote KAN by MIT and CALTECH?' additional_kwargs={} response_metadata={}\n",
            "content=\"I don't have information about who wrote KAN by MIT and CALTECH.\" additional_kwargs={} response_metadata={}\n",
            "content='How many ways did KR reasoning LLM framwork paper evaluate their system?' additional_kwargs={} response_metadata={}\n",
            "content='The KR reasoning LLM framework paper evaluated their system in six ways. These include \"Information Utilization (IU)\", \"Logical Reasoning (LR)\", \"Strategic Effectiveness (SE)\", \"Adaptability and Strategic Evolution (AD)\", \"Long-term Planning (LP)\", and \"Identity Alignment (IA)\".' additional_kwargs={} response_metadata={}\n",
            "content='could you tell me the total number of those eval strategies?' additional_kwargs={} response_metadata={}\n",
            "content='The total number of evaluation strategies used in the KR reasoning LLM framework paper is five. These evaluation strategies are \"Information Utilization (IU)\", \"Logical Reasoning (LR)\", \"Strategic Effectiveness (SE)\", \"Adaptability and Strategic Evolution (AD)\", and \"Long-term Planning (LP)\".' additional_kwargs={} response_metadata={}\n",
            "content='How many ways did KR reasoning LLM framwork paper evaluate their system?' additional_kwargs={} response_metadata={}\n",
            "content='In the KR reasoning LLM framework paper, five evaluation strategies were used. They included \"Information Utilization (IU)\", \"Logical Reasoning (LR)\", \"Strategic Effectiveness (SE)\", \"Adaptability and Strategic Evolution (AD)\", and \"Long-term Planning (LP)\".' additional_kwargs={} response_metadata={}\n",
            "content='could you tell me the total number of those eval strategies?' additional_kwargs={} response_metadata={}\n",
            "content='The total number of evaluation strategies used in the KR reasoning LLM framework paper is five. These evaluation strategies include \"Information Utilization (IU)\", \"Logical Reasoning (LR)\", \"Strategic Effectiveness (SE)\", \"Adaptability and Strategic Evolution (AD)\", and \"Long-term Planning (LP)\".' additional_kwargs={} response_metadata={}\n",
            "content='divide that number by 5?' additional_kwargs={} response_metadata={}\n",
            "content='To find the result of dividing the total number of evaluation strategies used in the KR reasoning LLM framework paper by 5, we need to know the specific number of evaluation strategies mentioned in the text. Unfortunately, the text provided does not contain a specific number for the total evaluation strategies used in the KR reasoning LLM framework paper. Therefore, without that specific number, I cannot calculate the result of dividing it by 5.' additional_kwargs={} response_metadata={}\n",
            "content='who did KAN?' additional_kwargs={} response_metadata={}\n",
            "content=\"I don't have information about who wrote KAN by MIT and CALTECH.\" additional_kwargs={} response_metadata={}\n",
            "content='what is kan about?' additional_kwargs={} response_metadata={}\n",
            "content='I don\\'t have enough information to determine what \"KAN\" refers to in this context.' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "for item in result['chat_history']:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x4VSwknVZYG"
      },
      "source": [
        "# **LOOP for qa--------------------------------**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "aGDNsf8MVY5W",
        "outputId": "be0743b5-78f3-41f8-f591-988620d38e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your question: who did KAN?\n",
            "I don't have information about who wrote KAN by MIT and CALTECH.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question: what is kan about?\n",
            "I don't have enough information to determine what \"KAN\" refers to in this context.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-674ef3594a6d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Your question: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'exit quit bye'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Bye bye!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    q = input('Your question: ')\n",
        "    if q.lower() in 'exit quit bye':\n",
        "        print('Bye bye!')\n",
        "        break\n",
        "    result = ask_question(q, crc)\n",
        "    print(result['answer'])\n",
        "    print('-' * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb9c5WurVHDY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
